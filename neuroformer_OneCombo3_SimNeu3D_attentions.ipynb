{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running_jupyter: True\n",
      "Running in Jupyter\n",
      "CONTRASTIUVEEEEEEE False\n",
      "VISUAL: True\n",
      "PAST_STATE: True\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "\n",
    "import sys\n",
    "import glob\n",
    "from pathlib import Path, PurePath\n",
    "path = Path.cwd()\n",
    "parent_path = path.parents[1]\n",
    "sys.path.append(str(PurePath(parent_path, 'neuroformer')))\n",
    "sys.path.append('neuroformer')\n",
    "sys.path.append('.')\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import math\n",
    "\n",
    "from neuroformer.model_neuroformer import GPT, GPTConfig\n",
    "from neuroformer.trainer import Trainer, TrainerConfig\n",
    "from neuroformer.utils import set_seed, update_object, check_common_attrs, running_jupyter\n",
    "from neuroformer.visualize import set_plot_params\n",
    "from neuroformer.SpikeVidUtils import make_intervals, round_n, tokenizer, SpikeTimeVidData2\n",
    "import gdown\n",
    "\n",
    "parent_path = os.path.dirname(os.path.dirname(os.getcwd())) + \"/\"\n",
    "\n",
    "import argparse\n",
    "from neuroformer.SpikeVidUtils import round_n\n",
    "\n",
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument(\"--infer\", action=\"store_true\", help=\"Inference mode\")\n",
    "    parser.add_argument(\"--train\", action=\"store_true\", default=False, help=\"Train mode\")\n",
    "    parser.add_argument(\"--dist\", action=\"store_true\", default=False, help=\"Distributed mode\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=25, help=\"Random seed\")\n",
    "    parser.add_argument(\"--resume\", type=str, default=None, help=\"Resume from checkpoint\")\n",
    "    parser.add_argument(\"--rand_perm\", action=\"store_true\", default=False, help=\"Randomly permute the ID column\")\n",
    "    parser.add_argument(\"--mconf\", type=str, default=None, help=\"Path to model config file\")\n",
    "    parser.add_argument(\"--eos_loss\", action=\"store_true\", default=False, help=\"Use EOS loss\")\n",
    "    parser.add_argument(\"--no_eos_dt\", action=\"store_true\", default=False, help=\"No EOS dt token\")\n",
    "    parser.add_argument(\"--downstream\", action=\"store_true\", default=False, help=\"Downstream task\")\n",
    "    parser.add_argument(\"--freeze_model\", action=\"store_true\", default=False, help=\"Freeze model\")\n",
    "    parser.add_argument(\"--title\", type=str, default=None)\n",
    "    parser.add_argument(\"--dataset\", type=str, default=\"Combo3_SimNeu3D\")\n",
    "    parser.add_argument(\"--behavior\", action=\"store_true\", default=False, help=\"Behavior task\")\n",
    "    parser.add_argument(\"--pred_behavior\", action=\"store_true\", default=False, help=\"Predict behavior\")\n",
    "    parser.add_argument(\"--past_state\", action=\"store_true\", default=False, help=\"Input past state\")\n",
    "    parser.add_argument(\"--visual\", action=\"store_true\", default=False, help=\"Visualize\")\n",
    "    parser.add_argument(\"--contrastive\", action=\"store_true\", default=False, help=\"Contrastive\")\n",
    "    parser.add_argument(\"--clip_loss\", action=\"store_true\", default=False, help=\"Clip loss\")\n",
    "    parser.add_argument(\"--clip_vars\", nargs=\"+\", default=['id','frames'], help=\"Clip variables\")\n",
    "    parser.add_argument(\"--class_weights\", action=\"store_true\", default=False, help=\"Class weights\")\n",
    "    parser.add_argument(\"--resample\", action=\"store_true\", default=False, help=\"Resample\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "print(f\"running_jupyter: {running_jupyter()}\")\n",
    "# if running_jupyter(): # or __name__ == \"__main__\":\n",
    "print(\"Running in Jupyter\")\n",
    "INFERENCE = True\n",
    "DIST = False\n",
    "SEED = 25\n",
    "DOWNSTREAM = False\n",
    "TITLE = None\n",
    "RESUME = \"./models/tensorboard/Combo3_SimNeu3D/first/f_window_0.5/sparse_f:None_id:None/w:0.1_wp:0.1/Cont:False['id', 'frames']_window:0.1_f_window:1_df:0.01_blocksize:20_conv_True_shuffle:True_batch:512_sparse_(None_None)_blocksz40_pos_emb:False_temp_emb:True_drop:0.35_dt:True_2.0_12_max0.01_(4, 0, 4)_4_256.pt\"\n",
    "RAND_PERM = False\n",
    "MCONF = None\n",
    "EOS_LOSS = False\n",
    "NO_EOS_DT = False\n",
    "FREEZE_MODEL = False\n",
    "TITLE = None\n",
    "DATASET = \"Combo3_SimNeu3D\"\n",
    "BEHAVIOR = False\n",
    "PREDICT_BEHAVIOR = False\n",
    "VISUAL = True\n",
    "PAST_STATE = True\n",
    "CONTRASTIVE = False\n",
    "CLIP_LOSS = True\n",
    "CLIP_VARS = ['id','frames']\n",
    "CLASS_WEIGHTS = False\n",
    "RESAMPLE_DATA = False\n",
    "# else:\n",
    "#     print(\"Running in terminal\")\n",
    "#     args = parse_args()\n",
    "#     INFERENCE = not args.train\n",
    "#     DIST = args.dist\n",
    "#     SEED = args.seed\n",
    "#     DOWNSTREAM = args.downstream\n",
    "#     TITLE = args.title\n",
    "#     RESUME = args.resume\n",
    "#     RAND_PERM = args.rand_perm\n",
    "#     MCONF = args.mconf\n",
    "#     EOS_LOSS = args.eos_loss\n",
    "#     NO_EOS_DT = args.no_eos_dt\n",
    "#     FREEZE_MODEL = args.freeze_model\n",
    "#     DATASET = args.dataset\n",
    "#     BEHAVIOR = args.behavior\n",
    "#     PREDICT_BEHAVIOR = args.pred_behavior\n",
    "#     VISUAL = args.visual\n",
    "#     PAST_STATE = args.past_state\n",
    "#     CONTRASTIVE = args.contrastive\n",
    "#     CLIP_LOSS = args.clip_loss\n",
    "#     CLIP_VARS = args.clip_vars\n",
    "#     CLASS_WEIGHTS = args.class_weights\n",
    "#     RESAMPLE_DATA = args.resample\n",
    "\n",
    "# SET SEED - VERY IMPORTANT\n",
    "set_seed(SEED)\n",
    "\n",
    "print(f\"CONTRASTIUVEEEEEEE {CONTRASTIVE}\")\n",
    "print(f\"VISUAL: {VISUAL}\")\n",
    "print(f\"PAST_STATE: {PAST_STATE}\")\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIDEO STACK SHAPE: torch.Size([1, 1280, 1, 64, 112])\n",
      "STIMULUS SHAPE: torch.Size([1280, 64, 112])\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "\n",
    "-- DATA --\n",
    "neuroformer/data/OneCombo3_V1AL/\n",
    "df = response\n",
    "video_stack = stimulus\n",
    "DOWNLOAD DATA URL = https://drive.google.com/drive/folders/1jNvA4f-epdpRmeG9s2E-2Sfo-pwYbjeY?usp=sharing\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from neuroformer.prepare_data import DataLinks\n",
    "\n",
    "ds = DATASET\n",
    "\n",
    "DATA_POINTERS = getattr(DataLinks, ds)\n",
    "DATA_DIR = DATA_POINTERS['DIRECTORY']\n",
    "RESPONSE_PATH = DATA_POINTERS['RESPONSE_PATH']\n",
    "STIMULUS_PATH = DATA_POINTERS['STIMULUS_PATH']\n",
    "data_dir = f\"./data/{ds}/\"\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    print(\"Downloading data...\")\n",
    "    import gdown\n",
    "    url = DATA_POINTERS['url']\n",
    "    gdown.download_folder(id=url, quiet=False, use_cookies=False, output=DATA_POINTERS['DIRECTORY'])\n",
    "\n",
    "\n",
    "df = pd.read_csv(RESPONSE_PATH)\n",
    "video_stack = torch.load(STIMULUS_PATH)\n",
    "stimulus = video_stack[0, :, 0]\n",
    "\n",
    "print(f\"VIDEO STACK SHAPE: {video_stack.shape}\")\n",
    "print(f\"STIMULUS SHAPE: {stimulus.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common attributes: {}\n"
     ]
    }
   ],
   "source": [
    "# load config files\n",
    "import yaml\n",
    "\n",
    "# base_path = \"configs/visnav/predict_behavior\"\n",
    "if MCONF is not None:\n",
    "    base_path = os.path.dirname(MCONF)\n",
    "elif RESUME is not None:\n",
    "    base_path = os.path.dirname(RESUME)\n",
    "else:\n",
    "    base_path = \"./configs/Combo3_V1AL/kernel_size/wave_emb/01second-noselfatt/01second-noselfatt_small/\"\n",
    "    \n",
    "\n",
    "with open(os.path.join(base_path, 'mconf.yaml'), 'r') as stream:\n",
    "    mconf = yaml.full_load(stream)\n",
    "\n",
    "with open(os.path.join(base_path, 'tconf.yaml'), 'r') as stream:\n",
    "    tconf = yaml.full_load(stream)\n",
    "\n",
    "with open(os.path.join(base_path, 'dconf.yaml'), 'r') as stream:\n",
    "    dconf = yaml.full_load(stream)\n",
    "\n",
    "import omegaconf\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# open yaml as omegacong\n",
    "mconf = OmegaConf.create(mconf)\n",
    "tconf = OmegaConf.create(tconf)\n",
    "dconf = OmegaConf.create(dconf)\n",
    "\n",
    "# set attrs that are not equal\n",
    "common_attrs = check_common_attrs(mconf, tconf, dconf)\n",
    "print(f\"Common attributes: {common_attrs}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if INFERENCE or mconf:\n",
    "    frame_window = mconf.frame_window\n",
    "    window = mconf.window\n",
    "    window_prev = mconf.window_prev\n",
    "    window_behavior = mconf.window_behavior if hasattr(mconf, 'window_behavior') else None\n",
    "    dt = mconf.dt\n",
    "    dt_frames = mconf.dt_frames if hasattr(mconf, 'dt_frames') else 0.05\n",
    "    dt_vars = mconf.dt_vars if hasattr(mconf, 'dt_vars') else 0.05\n",
    "    dt_speed = mconf.dt_speed if hasattr(mconf, 'dt_speed') else 0.2\n",
    "    intervals = None\n",
    "else:\n",
    "    frame_window = 0.5\n",
    "    window = 0.05\n",
    "    window_prev = None\n",
    "    window_behavior = window\n",
    "    dt = 0.01\n",
    "    dt_frames = 0.05\n",
    "    dt_vars = 0.05\n",
    "    dt_speed = 0.2\n",
    "    intervals = None\n",
    "\n",
    "# randomly permute 'id' column\n",
    "if RAND_PERM:\n",
    "    df['ID'] = df['ID'].sample(frac=1, random_state=25).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## choose modalities ##\n",
    "\n",
    "# behavior\n",
    "behavior = BEHAVIOR\n",
    "behavior_vars = ['eyerad', 'phi', 'speed', 'th']\n",
    "# behavior_vars = ['speed']\n",
    "n_behavior = len(behavior_vars)\n",
    "predict_behavior = PREDICT_BEHAVIOR\n",
    "# stimulus\n",
    "visual_stim = VISUAL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroformer.SpikeVidUtils import trial_df, get_df_visnav, make_intervals\n",
    "\n",
    "\n",
    "if behavior is True:\n",
    "    behavior = pd.DataFrame({k: data[k] for k in behavior_vars + ['t']})\n",
    "    # rename t to time\n",
    "    behavior = behavior.rename(columns={'t': 'Time'}) if behavior is not None else None\n",
    "    behavior['Interval'] = make_intervals(behavior, window)\n",
    "    behavior['Interval_2'] = make_intervals(behavior, window_prev)\n",
    "\n",
    "    # prepare speed variables\n",
    "    behavior['speed'] = behavior['speed'].apply(lambda x: round_n(x, dt_speed))\n",
    "    dt_range_speed = behavior['speed'].min(), behavior['speed'].max()\n",
    "    dt_range_speed = np.arange(dt_range_speed[0], dt_range_speed[1] + dt_speed, dt_speed)\n",
    "    n_behavior = len(dt_range_speed)\n",
    "\n",
    "    stoi_speed = { round_n(ch, dt_speed):i for i,ch in enumerate(dt_range_speed) }\n",
    "    itos_speed = { i:round_n(ch, dt_speed) for i,ch in enumerate(dt_range_speed) }\n",
    "    assert (window_behavior) % dt_vars < 1e-5, \"window + window_prev must be divisible by dt_vars\"\n",
    "    samples_per_behavior = int((window + window_prev) // dt_vars)\n",
    "    behavior_block_size = int((window + window_prev) // dt_vars) * (len(behavior.columns) - 1)\n",
    "else:\n",
    "    behavior = None\n",
    "    behavior_vars = None\n",
    "    behavior_block_size = 0\n",
    "    samples_per_behavior = 0\n",
    "    stoi_speed = None\n",
    "    itos_speed = None\n",
    "    dt_range_speed = None\n",
    "    n_behavior = None\n",
    "\n",
    "if predict_behavior:\n",
    "    loss_bprop = ['behavior']\n",
    "else:\n",
    "    loss_bprop = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Interval'] = make_intervals(df, window)\n",
    "df['real_interval'] = make_intervals(df, 0.05)\n",
    "df['Interval_2'] = make_intervals(df, window_prev)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# randomly permute 'id' column\n",
    "if RAND_PERM:\n",
    "    print('// randomly permuting ID column //')\n",
    "    df['ID'] = df['ID'].sample(frac=1, random_state=25).reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1030997/883616637.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  frame_feats = torch.tensor(stimulus, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "## resnet3d feats\n",
    "n_frames = round(frame_window * 1/dt_frames)\n",
    "# kernel_size = (n_frames, 4, 4)\n",
    "kernel_size = [n_frames, 8, 8]\n",
    "stride_size = [n_frames, 4, 4]\n",
    "padding_size = 0\n",
    "n_embd = 256\n",
    "n_embd_frames = 64\n",
    "frame_feats = stimulus\n",
    "frame_block_size = 0\n",
    "frame_feats = torch.tensor(stimulus, dtype=torch.float32)\n",
    "conv_layer = True\n",
    "\n",
    "prev_id_block_size = 20\n",
    "id_block_size = 20   #\n",
    "block_size = frame_block_size + id_block_size + prev_id_block_size\n",
    "frame_memory = frame_window // dt_frames\n",
    "window = window\n",
    "\n",
    "neurons = sorted(list(set(df['ID'])))\n",
    "max_window = max(window, window_prev)\n",
    "stoi, itos, stoi_dt, itos_dt = tokenizer(neurons, max_window, dt, NO_EOS_DT)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%\n",
    "# var_group = 'Interval'\n",
    "# int_trials = df.groupby([var_group, 'Trial']).size()\n",
    "# print(int_trials.mean())\n",
    "# # df.groupby(['Interval', 'Trial']).agg(['nunique'])\n",
    "# n_unique = len(df.groupby([var_group, 'Trial']).size())\n",
    "# df.groupby([var_group, 'Trial']).size().nlargest(int(0.1 * n_unique))\n",
    "# # df.groupby(['Interval_2', 'Trial']).size().mean()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "r_split = 0.8\n",
    "all_trials = sorted(df['Trial'].unique())\n",
    "train_trials = random.sample(all_trials, int(len(all_trials) * r_split))\n",
    "\n",
    "train_data = df[df['Trial'].isin(train_trials)]\n",
    "test_data = df[~df['Trial'].isin(train_trials)]\n",
    "\n",
    "# r_split_ft = np.arange(0, 1, 0.25)\n",
    "r_split_ft = 0.1\n",
    "finetune_trials = train_trials[:int(len(train_trials) * r_split_ft)]\n",
    "finetune_data = df[df['Trial'].isin(finetune_trials)]\n",
    "\n",
    "n = []\n",
    "n_trial = [2, 8, 14, 19]\n",
    "for n_stim in range(df['Trial'].max() // 20):\n",
    "    # n_trial = [2, 4, 6, 8, 10, 12, 14, 18]\n",
    "    for n_t in n_trial:\n",
    "        trial = (n_stim + 1) * 20 - (n_t)\n",
    "        n.append(trial)\n",
    "train_data = df[~df['Trial'].isin(n)].reset_index(drop=True)\n",
    "test_data = df[df['Trial'].isin(n)].reset_index(drop=True)\n",
    "small_data = df[df['Trial'].isin([5])].reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLASS_WEIGHTS:\n",
    "    class_weights = {}\n",
    "    class_weights['id'] = torch.ones(len(stoi.keys()), dtype=torch.float32)\n",
    "    class_weights['id'][stoi['PAD']] = 0\n",
    "    class_weights['id'][stoi['EOS']] = 1 / 10\n",
    "    class_weights['dt'] = torch.ones(len(stoi_dt.keys()), dtype=torch.float32)\n",
    "    class_weights['dt'][stoi_dt['PAD']] = 0\n",
    "else:\n",
    "    class_weights = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population Size:  363\n",
      "ID Population Size:  363\n",
      "DT Population Size:  13\n",
      "Min Interval: 0.2\n",
      "train: 462075, test: 112454\n"
     ]
    }
   ],
   "source": [
    "from neuroformer.SpikeVidUtils import SpikeTimeVidData2\n",
    "\n",
    "train_dataset = SpikeTimeVidData2(train_data, None, block_size, id_block_size, frame_block_size, prev_id_block_size, \n",
    "                                  window, dt, frame_memory, stoi, itos, neurons, stoi_dt, itos_dt, frame_feats,\n",
    "                                  pred=False, window_prev=window_prev, frame_window=frame_window,\n",
    "                                  dt_frames=dt_frames, intervals=None, dataset=DATASET,\n",
    "                                  behavior=behavior, behavior_vars=behavior_vars, dt_vars=dt_vars,\n",
    "                                  behavior_block_size=behavior_block_size, samples_per_behavior=samples_per_behavior,\n",
    "                                  window_behavior=window_behavior, predict_behavior=predict_behavior,\n",
    "                                  stoi_speed=stoi_speed, itos_speed=itos_speed, dt_speed=dt_speed, labels=True,\n",
    "                                  resample_data=RESAMPLE_DATA)\n",
    "\n",
    "# update_object(train_dataset, dconf)\n",
    "train_dataset = train_dataset.copy(train_data)\n",
    "test_dataset = train_dataset.copy(test_data, resample_data=False)\n",
    "finetune_dataset = train_dataset.copy(finetune_data, resample_data=False)\n",
    "    \n",
    "print(f'train: {len(train_dataset)}, test: {len(test_dataset)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/22/2023 10:44:59 - INFO - neuroformer.model_neuroformer -   number of parameters: 1.316634e+07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 28\n"
     ]
    }
   ],
   "source": [
    "\n",
    "layers = (mconf.n_state_layers, mconf.n_state_history_layers, mconf.n_stimulus_layers)   \n",
    "max_epochs = 1000\n",
    "batch_size = round((32 * 16))\n",
    "shuffle = True\n",
    "\n",
    "model_conf = GPTConfig(train_dataset.population_size, block_size,    # frame_block_size\n",
    "                        id_vocab_size=train_dataset.id_population_size,\n",
    "                        frame_block_size=frame_block_size,\n",
    "                        id_block_size=id_block_size,  # frame_block_size\n",
    "                        prev_id_block_size=prev_id_block_size,\n",
    "                        behavior_block_size=behavior_block_size,\n",
    "                        sparse_mask=False, p_sparse=None, \n",
    "                        sparse_topk_frame=None, sparse_topk_id=None, sparse_topk_prev_id=None,\n",
    "                        n_dt=len(stoi_dt.keys()),\n",
    "                        pretrain=False,\n",
    "                        n_state_layers=4, n_state_history_layers=0,\n",
    "                        n_stimulus_layers=4, self_att_layers=0,\n",
    "                        n_behavior_layers=0, predict_behavior=predict_behavior, n_behavior=n_behavior,\n",
    "                        n_head=4, n_embd=n_embd, \n",
    "                        contrastive=mconf.contrastive, clip_emb=1024, clip_temp=mconf.clip_temp,\n",
    "                        conv_layer=conv_layer, kernel_size=kernel_size, stride_size=stride_size, padding_size=padding_size,\n",
    "                        temp_emb=mconf.temp_emb, pos_emb=False,\n",
    "                        id_drop=0.35, im_drop=0.35, b_drop=0.45,\n",
    "                        window=window, window_prev=window_prev, frame_window=frame_window, dt=dt,\n",
    "                        neurons=neurons, stoi_dt=stoi_dt, itos_dt=itos_dt, n_embd_frames=n_embd_frames,\n",
    "                        ignore_index_id=stoi['PAD'], ignore_index_dt=stoi_dt['PAD'],\n",
    "                        eos_loss=EOS_LOSS,\n",
    "                        class_weights=class_weights, contrastive_vars=CLIP_VARS)  # 0.35\n",
    "\n",
    "if INFERENCE or MCONF is not None:\n",
    "    update_object(model_conf, mconf)\n",
    "\n",
    "if not INFERENCE:\n",
    "    if PAST_STATE is False:\n",
    "        print(f\"// -- No past state, layers=0 -- //\")\n",
    "        model_conf.n_state_history_layers = 0\n",
    "\n",
    "    if CONTRASTIVE or CLIP_LOSS is True:\n",
    "        print(f\"// -- contrastive objective clip{CLIP_LOSS} -- //\")\n",
    "        model_conf.contrastive = True\n",
    "        model_conf.clip_loss = CLIP_LOSS\n",
    "    else:\n",
    "        print(f\"// -- no contrastive objective -- //\")\n",
    "        model_conf.contrastive = False\n",
    "    if VISUAL is False:\n",
    "        print(f\"// -- No visual, layers=0 -- //\")\n",
    "        model_conf.n_stimulus_layers = 0\n",
    "\n",
    "model = GPT(model_conf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(train_dataset, batch_size=2, shuffle=False, num_workers=4, pin_memory=True)\n",
    "iterable = iter(loader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1280, 64, 112])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.frame_feats.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_prev torch.Size([2, 20])\n",
      "dt_prev torch.Size([2, 20])\n",
      "pad_prev torch.Size([2])\n",
      "id torch.Size([2, 20])\n",
      "dt torch.Size([2, 20])\n",
      "pad torch.Size([2])\n",
      "interval torch.Size([2])\n",
      "trial torch.Size([2])\n",
      "frames torch.Size([2, 1, 20, 64, 112])\n",
      "f_idx torch.Size([2, 2])\n",
      "cid torch.Size([2, 2])\n",
      "pid torch.Size([2, 2])\n",
      "y: id, torch.Size([2, 20])\n",
      "y: dt, torch.Size([2, 20])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iterable)\n",
    "# print(x['behavior'].shape, x['behavior_dt'].shape)\n",
    "for k in x.keys():\n",
    "    print(k, x[k].shape)\n",
    "for k in y.keys():\n",
    "    print(f\"y: {k}, {y[k].shape}\")\n",
    "\n",
    "\n",
    "# epoch250_rand{RAND_PERM}_downstream:{DOWNSTREAM}\n",
    "# title =  f'3/4prop_{CLASS_WEIGHTS}/past_state_{PAST_STATE}_visual{VISUAL}_contrastive_{CONTRASTIVE}_clip_loss{CLIP_LOSS}t{mconf.clip_temp}_freeze_{FREEZE_MODEL}_class_weights{CLASS_WEIGHTS}/randperm_{RAND_PERM}/Big_fixed_noself-att'\n",
    "# title = f'ablations_2/{SEED}/RESUME{RESUME != None}_paststate{PAST_STATE}_visual{VISUAL}_contrastive{model_conf.contrastive}'\n",
    "# model_path = f\"\"\"./models/tensorboard/{DATASET}/ablations_small/{title}_2/sparse_f:{mconf.sparse_topk_frame}_id:{mconf.sparse_topk_id}/w:{mconf.window}_wp:{mconf.window_prev}/Cont:{mconf.contrastive}_window:{mconf.window}_f_window:{mconf.frame_window}_df:{mconf.dt}_blocksize:{mconf.id_block_size}_conv_{mconf.conv_layer}_shuffle:{shuffle}_batch:{batch_size}_sparse_({mconf.sparse_topk_frame}_{mconf.sparse_topk_id})_blocksz{block_size}_pos_emb:{mconf.pos_emb}_temp_emb:{mconf.temp_emb}_drop:{mconf.id_drop}_dt:{shuffle}_2.0_{max(stoi_dt.values())}_max{dt}_{layers}_{mconf.n_head}_{mconf.n_embd}.pt\"\"\"\n",
    "model_path = f\"\"\"./models/tensorboard/{DATASET}/{TITLE}/sparse_f:{mconf.sparse_topk_frame}_id:{mconf.sparse_topk_id}/w:{mconf.window}_wp:{mconf.window_prev}/Cont:{mconf.contrastive}{CLIP_VARS}_window:{mconf.window}_f_window:{mconf.frame_window}_df:{mconf.dt}_blocksize:{mconf.id_block_size}_conv_{mconf.conv_layer}_shuffle:{shuffle}_batch:{batch_size}_sparse_({mconf.sparse_topk_frame}_{mconf.sparse_topk_id})_blocksz{block_size}_pos_emb:{mconf.pos_emb}_temp_emb:{mconf.temp_emb}_drop:{mconf.id_drop}_dt:{shuffle}_2.0_{max(stoi_dt.values())}_max{dt}_{layers}_{mconf.n_head}_{mconf.n_embd}.pt\"\"\"\n",
    "# model_path = RESUME\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_PATH ---->: ./models/tensorboard/Combo3_SimNeu3D/None/sparse_f:None_id:None/w:0.1_wp:0.1/Cont:False['id', 'frames']_window:0.1_f_window:1_df:0.01_blocksize:20_conv_True_shuffle:True_batch:512_sparse_(None_None)_blocksz40_pos_emb:False_temp_emb:True_drop:0.35_dt:True_2.0_12_max0.01_(4, 0, 4)_4_256.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if DOWNSTREAM:\n",
    "    print(f\"// Downstream Objective: {DOWNSTREAM} //\")\n",
    "    \"\"\"\n",
    "    image indexes\n",
    "\n",
    "    (140, 260)\n",
    "    (339, 424)\n",
    "    (500, 620)\n",
    "    (680, 840) \n",
    "    (960, 1050)\n",
    "\n",
    "    \"\"\"\n",
    "    import tifffile\n",
    "    from neuroformer.SpikeVidUtils import get_interval_idx\n",
    "    from neuroformer.modules import ClassifierWrapper\n",
    "\n",
    "    stim2_path = \"./data/Combo3_V1AL/stimuli/Combined Stimuli 3-Movie2.tif\"\n",
    "    stimulus_2 = tifffile.imread(stim2_path)\n",
    "\n",
    "    stim3_path = \"./data/Combo3_V1AL/stimuli/Combined Stimuli 3-Movie3.tif\"\n",
    "    stimulus_3 = tifffile.imread(stim3_path)\n",
    "\n",
    "    mouse_indexes = [(140, 260), (339, 424), (500, 620), (680, 840), (960, 1050)]\n",
    "    stimulus_mice = np.concatenate([stimulus_2[i[0]:i[1]] for i in mouse_indexes])\n",
    "    stimulus_control = np.concatenate([stimulus_3[i[0]:i[1]] for i in mouse_indexes])\n",
    "    assert stimulus_mice.shape == stimulus_control.shape, \"stimulus shapes must be equal\"\n",
    "\n",
    "    control_labels = np.zeros(stimulus_control.shape[0])\n",
    "    mice_labels = np.ones(stimulus_mice.shape[0])\n",
    "    stimulus_task = np.concatenate([stimulus_control, stimulus_mice])\n",
    "    labels_task = np.concatenate([control_labels, mice_labels])\n",
    "    mouse_indexes_downsampled = [tuple(map(lambda x: x // 3, i)) for i in mouse_indexes]\n",
    "    stim_2_ds = stimulus[1]\n",
    "    stim_2_ds_mice = np.concatenate([stim_2_ds[i[0]:i[1]] for i in mouse_indexes_downsampled])\n",
    "    mouse_indexes_intervals = [tuple(map(lambda x: get_interval_idx(x, 0.05), i)) for i in mouse_indexes_downsampled]\n",
    "    intervals_cls = np.concatenate([np.arange(i[0], i[1], window) for i in mouse_indexes_intervals])\n",
    "    # don't use same intervals for training and testing (because images will be the same)\n",
    "    train_interval_cls = np.random.choice(intervals_cls, size=int(len(intervals_cls) * 0.8), replace=False)\n",
    "    test_interval_cls = np.setdiff1d(intervals_cls, train_interval_cls)\n",
    "    train_trial_cls = train_data[train_data['Trial'] > 20]['Trial'].unique()\n",
    "    test_trial_cls = test_data[test_data['Trial'] > 20]['Trial'].unique()\n",
    "    train_interval_trial_cls = np.array(np.meshgrid(train_interval_cls, train_trial_cls)).T.reshape(-1, 2)\n",
    "    test_interval_trial_cls = np.array(np.meshgrid(test_interval_cls, test_trial_cls)).T.reshape(-1, 2)\n",
    "    train_dataset = train_dataset.copy(train_data, t=train_interval_trial_cls)\n",
    "    test_dataset = test_dataset.copy(test_data, t=test_interval_trial_cls)\n",
    "\n",
    "\n",
    "print(f\"MODEL_PATH ---->: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./models/tensorboard/Combo3_SimNeu3D/first/f_window_0.5/sparse_f:None_id:None/w:0.1_wp:0.1/Cont:False['id', 'frames']_window:0.1_f_window:1_df:0.01_blocksize:20_conv_True_shuffle:True_batch:512_sparse_(None_None)_blocksz40_pos_emb:False_temp_emb:True_drop:0.35_dt:True_2.0_12_max0.01_(4, 0, 4)_4_256.pt\n"
     ]
    }
   ],
   "source": [
    "tconf = TrainerConfig(max_epochs=max_epochs, batch_size=batch_size, learning_rate=1e-4, \n",
    "                    num_workers=4, lr_decay=True, patience=3, warmup_tokens=8e4, \n",
    "                    decay_weights=True, weight_decay=0.1, shuffle=shuffle,\n",
    "                    final_tokens=len(train_dataset)*(id_block_size) * (max_epochs),\n",
    "                    clip_norm=1.0, grad_norm_clip=1.0,\n",
    "                    dataset='Combo3_V1AL', mode='predict',\n",
    "                    block_size=train_dataset.block_size,\n",
    "                    id_block_size=train_dataset.id_block_size,\n",
    "                    show_grads=False, plot_raster=False,\n",
    "                    ckpt_path=model_path, no_pbar=False, \n",
    "                    dist=DIST, save_every=20, loss_bprop=loss_bprop)\n",
    "\n",
    "if not INFERENCE:\n",
    "    if DOWNSTREAM:\n",
    "        mconf.__setattr__('freeze_model', FREEZE_MODEL)\n",
    "        tconf.__setattr__('warmup_tokens', 100)\n",
    "        N_CLASSES = 2\n",
    "        classifier = ClassifierWrapper(model, mconf, N_CLASSES)\n",
    "        train_model = classifier\n",
    "\n",
    "    else:\n",
    "        train_model = model\n",
    "    trainer = Trainer(train_model, train_dataset, test_dataset, tconf, model_conf)\n",
    "    trainer.train()\n",
    "else:\n",
    "    if RESUME is not None:\n",
    "        model_path = RESUME\n",
    "    else:\n",
    "        model_path = glob.glob(os.path.join(base_path, '**.pt'), recursive=True)[0]\n",
    "    print(f\"Loading model from {model_path}\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# # model.train()\n",
    "# for i in range(1):\n",
    "#     x, y = next(iterable)\n",
    "#     x_1 = x['id'][0].detach().cpu().numpy()\n",
    "#     x_pad = x['pad']\n",
    "#     x_1 = x['id'][:len(x_1) - int(x_pad[0])]\n",
    "#     preds, features, loss = model(x, y)\n",
    "\n",
    "#     step_choices = random.sample(range(len(x_1)), min(5, len(x_1)))\n",
    "#     fig, ax = plt.subplots(1, len(step_choices), figsize=(50, 5))\n",
    "#     for step in step_choices:\n",
    "#         step_preds = preds['id'][0, step].detach().cpu().numpy()\n",
    "#         x_axis = np.arange(0, len(step_preds))\n",
    "#         ax_step = ax[step]\n",
    "#         ax_step.scatter(x_axis, step_preds)\n",
    "#         ax_step.set_title(f\"\"\"{step}\"\"\", fontsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_prev torch.Size([2, 20])\n",
      "dt_prev torch.Size([2, 20])\n",
      "pad_prev torch.Size([2])\n",
      "id torch.Size([2, 20])\n",
      "dt torch.Size([2, 20])\n",
      "pad torch.Size([2])\n",
      "interval torch.Size([2])\n",
      "trial torch.Size([2])\n",
      "frames torch.Size([2, 1, 20, 64, 112])\n",
      "f_idx torch.Size([2, 2])\n",
      "cid torch.Size([2, 2])\n",
      "pid torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iterable)\n",
    "for k in x.keys():\n",
    "    print(k, x[k].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, features, loss = model(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_prev torch.Size([2, 20, 256])\n",
      "id torch.Size([2, 20, 256])\n",
      "frames torch.Size([2, 406, 256])\n",
      "raw_frames torch.Size([2, 1, 15, 27, 256])\n",
      "last_layer torch.Size([2, 20, 256])\n"
     ]
    }
   ],
   "source": [
    "for key in features.keys():\n",
    "    print(key, features[key].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroformer.attentionVis import reshape_features\n",
    "\n",
    "frame_att = reshape_features(features, kernel_size, frame_window, dt_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 15, 27, 256])\n",
      "torch.Size([2, 406, 256])\n",
      "(2, 15, 27, 256)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 15, 27, 256])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = features['raw_frames']\n",
    "feats_frames = features['frames']\n",
    "print(feats.shape)\n",
    "print(feats_frames.shape)\n",
    "\n",
    "feats_frames = feats_frames[:, 1:].detach().cpu().numpy()\n",
    "feats_frames = feats_frames.reshape(-1, 15, 27, 256)\n",
    "print(feats_frames.shape)\n",
    "\n",
    "feats_un = torch.tensor(feats_frames)\n",
    "\n",
    "torch.stack([feats_un, feats_un], dim=1).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_attention_hooks(model):\n",
    "    features = {}\n",
    "\n",
    "    # helper function for hook\n",
    "    def get_features(name):\n",
    "        def hook(model, input, output):\n",
    "            features[name] = output.detach().cpu()\n",
    "        return hook\n",
    "\n",
    "    grads = {}\n",
    "    def get_grads(name):\n",
    "        def hook(model, input, output):\n",
    "            grads[name] = output.detach().cpu()\n",
    "        return hook\n",
    "\n",
    "    \"\"\"\n",
    "    register forward hooks for all multimodal transformer layers\n",
    "    so that the features are saved after every forward pass\n",
    "    \"\"\"\n",
    "\n",
    "    for n, mod in enumerate(model.neural_visual_transformer.neural_state_blocks):\n",
    "        mod.register_forward_hook(get_features(f'neural_state_block_{n}'))\n",
    "\n",
    "    for n, mod in enumerate(model.neural_visual_transformer.neural_state_history_blocks):\n",
    "        mod.register_forward_hook(get_features(f'neural_state_history_block_{n}'))\n",
    "\n",
    "    for n, mod in enumerate(model.neural_visual_transformer.neural_state_history_self_attention):\n",
    "        mod.register_forward_hook(get_features(f'neural_state_history_self_attention_{n}'))\n",
    "\n",
    "    for n, mod in enumerate(model.neural_visual_transformer.neural_state_stimulus_blocks):\n",
    "        mod.register_forward_hook(get_features(f'neural_state_stimulus_block_{n}'))\n",
    "\n",
    "    for n, mod in enumerate(model.neural_visual_transformer.neural_state_stimulus_blocks):\n",
    "        mod.attn.attn_drop.register_full_backward_hook(get_grads(f'neural_state_stimulus_block_{n}'))\n",
    "\n",
    "def get_atts(model):\n",
    "    attentions = {}\n",
    "\n",
    "    for n, mod in enumerate(model.neural_visual_transformer.neural_state_blocks):\n",
    "        attentions[f'neural_state_block_{n}'] = mod.attn.att.detach().cpu()\n",
    "\n",
    "    for n, mod in enumerate(model.neural_visual_transformer.neural_state_history_blocks):\n",
    "        attentions[f'neural_state_history_block_{n}'] = mod.attn.att.detach().cpu()\n",
    "\n",
    "    for n, mod in enumerate(model.neural_visual_transformer.neural_state_stimulus_blocks):\n",
    "        attentions[f'neural_stimulus_block_{n}'] = mod.attn.att.detach().cpu()\n",
    "    \n",
    "    return attentions\n",
    "\n",
    "def get_grads(model):\n",
    "    grads = {}\n",
    "    \n",
    "    for n, mod in enumerate(model.neural_visual_transformer.neural_state_stimulus_blocks):\n",
    "        grads[f'neural_stimulus_block_{n}'] = mod.attn.att.detach().cpu()    \n",
    "    return grads\n",
    "\n",
    "def gradcam(atts, grads):\n",
    "    common_keys = set(atts.keys()).intersection(set(grads.keys()))\n",
    "    for key in common_keys:\n",
    "        atts[key] = atts[key] * grads[key]\n",
    "    return atts\n",
    "\n",
    "def accum_atts(att_dict, key=None):\n",
    "    if key is None:\n",
    "        att_keys = att_dict.keys()\n",
    "    else:\n",
    "        att_keys = [k for k in att_dict.keys() if key in k]\n",
    "    atts = []\n",
    "    for k in att_keys:\n",
    "        att = att_dict[k]\n",
    "        att = att.sum(-3).detach().cpu()\n",
    "        reshape_c = att.shape[-1] // stimulus.shape[0]\n",
    "        assert att.shape[-1] % stimulus.shape[0] == 0, \"Attention shape does not match stimulus shape\"\n",
    "        att = att.view(att.shape[0], att.shape[-2], reshape_c, att.shape[-1] // reshape_c)\n",
    "        att = att.sum(-2)\n",
    "        atts.append(att)\n",
    "    return torch.stack(atts)\n",
    "\n",
    "def accum_atts_visual(att_dict, key='neural_stimulus_block'):\n",
    "    \"\"\"\n",
    "    Accumulates attention weights for visual attention\n",
    "    \"\"\"\n",
    "    if key is None:\n",
    "        att_keys = att_dict.keys()\n",
    "    else:\n",
    "        att_keys = [k for k in att_dict.keys() if key in k]\n",
    "    atts = []\n",
    "    for k in att_keys:\n",
    "        att = att_dict[k]\n",
    "        # reshaped_att = reshape_features(att, kernel_size, frame_window, dt_frames)\n",
    "        atts.append(att)\n",
    "    # (B, n_head * layers, ID, Frame)\n",
    "    return torch.stack(atts, dim=1) # .min(1)[0].mean(1) # (B, head, layer, ID, Frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_attention_hooks(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions = get_atts(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "att = accum_atts_visual(attentions, key='neural_stimulus_block')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att.shape[1:-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 20, 406])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att[:, -1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 20, 406])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions['neural_stimulus_block_0'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [05:45<00:00,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved attention matrix to ./models/tensorboard/Combo3_SimNeu3D/first/f_window_0.5/sparse_f:None_id:None/w:0.1_wp:0.1/attentions/1_att_matrix_gradcam_True.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import collections\n",
    "from neuroformer.utils import all_device\n",
    "\n",
    "# sample some random trials\n",
    "att_trials = np.random.choice(train_data['Trial'].unique(), size=50, replace=False)\n",
    "# att_df = train_data[train_data['Trial'].isin(att_trials)]\n",
    "att_df = train_data\n",
    "att_dataset = train_dataset.copy(att_df)\n",
    "loader = DataLoader(att_dataset, batch_size=32 * 8)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "model.to(device)\n",
    "model.zero_grad()\n",
    "pbar = tqdm(loader)\n",
    "\n",
    "# get the shapes of the attention matrices\n",
    "x, y = next(iter(loader))\n",
    "x, y = all_device((x, y), device)\n",
    "_, _, _, = model(x, y)\n",
    "attentions = get_atts(model)\n",
    "neural_stimulus_atts = attentions['neural_stimulus_block_0']\n",
    "len_stimulus_block = neural_stimulus_atts.shape[-1]\n",
    "\n",
    "model.eval()\n",
    "# att_matrix = np.zeros((len(stoi.keys()), len_stimulus_block))\n",
    "att_matrix = None\n",
    "att_dict = collections.defaultdict(list)\n",
    "grad_att_dict = collections.defaultdict(list)\n",
    "\n",
    "grad_cond = True\n",
    "for x, y in pbar:\n",
    "    model.zero_grad()\n",
    "    x, y = all_device((x, y), device)\n",
    "    model.to(device)\n",
    "    with torch.set_grad_enabled(grad_cond):\n",
    "        _, _, _, = model(x, y)\n",
    "    # model.cpu()\n",
    "    x, y = all_device((x, y), \"cpu\")\n",
    "    attentions = get_atts(model)\n",
    "    if grad_cond:\n",
    "        gradients = get_grads(model)\n",
    "        grad_attentions = gradcam(attentions, gradients)\n",
    "        grad_att = accum_atts_visual(attentions, key='neural_stimulus_block')\n",
    "\n",
    "    att = accum_atts_visual(attentions, key='neural_stimulus_block')\n",
    "    att_dims = att.shape\n",
    "\n",
    "    # resize the attention matrix\n",
    "    if att_matrix is None:\n",
    "        if len(att_dims) > 2:\n",
    "            dims = (len(stoi.keys()), *att_dims[1:-2], att_dims[-1])\n",
    "            att_matrix = np.zeros(dims)\n",
    "        else:\n",
    "            att_matrix = np.zeros((len(stoi.keys()), len_stimulus_block))\n",
    "    \n",
    "    if len(att.size()) > 2:\n",
    "        # flatten batch\n",
    "        att_reshape = (att.shape[0] * att.shape[-2], *att_dims[1:-2], att_dims[-1])\n",
    "        att = att.view(att_reshape)\n",
    "\n",
    "    x_id = x['id'].flatten()\n",
    "    y_id = y['id'].flatten()\n",
    "    eos_idx = (x_id == stoi['EOS']).nonzero()\n",
    "    index_ranges = [1, eos_idx[0]]\n",
    "    x_id = x_id[index_ranges[0]:index_ranges[1]]\n",
    "    y_id = y_id[index_ranges[0]-1:index_ranges[1]-1]\n",
    "    att = att[index_ranges[0]:index_ranges[1]]\n",
    "    # convert from token to ID\n",
    "    neurons = [int(itos[int(n)]) if not isinstance(itos[int(n)], str) else 1002 for n in x_id]\n",
    "    if len(att) > 1:\n",
    "        for n, neuron in enumerate(neurons):\n",
    "            att_matrix[neuron] += np.array(att[n])\n",
    "            att_dict[neuron].append(att[n])\n",
    "            grad_att_dict[neuron].append(att[n])\n",
    "    # clear gpu memory\n",
    "    del x, y, attentions, att, x_id, y_id, eos_idx, index_ranges, neurons\n",
    "    if grad_cond:\n",
    "        del gradients\n",
    "    model.zero_grad()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def dict_to_tensor(dict_):\n",
    "    \"\"\"\n",
    "    ::parameter:: dict: a dict of lists\n",
    "    ::return:: concatenate lists into tensors\n",
    "    \"\"\"\n",
    "    for key, value in dict_.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            dict_[key] = value\n",
    "        elif isinstance(value, list):\n",
    "            dict_[key] = torch.stack(dict_[key])\n",
    "    return dict_\n",
    "        \n",
    "att_dict_ = dict_to_tensor(att_dict)\n",
    "grad_att_dict_ = dict_to_tensor(att_dict)\n",
    "\n",
    "# save att_matrix\n",
    "att_path = os.path.join(base_path, \"attentions\")\n",
    "if not os.path.exists(att_path):\n",
    "    os.mkdir(att_path)\n",
    "n_files = len(glob.glob(os.path.join(att_path, \"*.npy\")))\n",
    "\n",
    "save_path = os.path.join(att_path, f\"{n_files}_att_matrix_gradcam_{grad_cond}.npy\")\n",
    "save_path_att_dict = os.path.join(att_path, f\"{n_files}_att_dict.pt\")\n",
    "save_path_grad_att_dict = os.path.join(att_path, f\"{n_files}_grad_att_dict.pt\")\n",
    "np.save(save_path, att_matrix)\n",
    "torch.save(att_dict_, save_path_att_dict)\n",
    "torch.save(grad_att_dict_, save_path_grad_att_dict)\n",
    "print(f\"Saved attention matrix to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_matrix_pth = \"./models/tensorboard/Combo3_SimNeu3D/first/f_window_0.5/sparse_f:None_id:None/w:0.1_wp:0.1/attentions/0_att_matrix_gradcam_True.npy\"\n",
    "# att_matrix = np.load(att_matrix_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# att_matrix_pth = \"./models/tensorboard/Combo3_SimNeu3D/first/f_window_0.5/sparse_f:None_id:None/w:0.1_wp:0.1/attentions/0_att_matrix_gradcam_True.npy\"\n",
    "# att_matrix = np.load(att_matrix_pth)\n",
    "\n",
    "# load csv file as numpy array\n",
    "centroids = torch.tensor(np.array(pd.read_csv(os.path.join(DATA_DIR, 'simNeu_3D_Combo4_1000Rep_centroids.csv'))), )\n",
    "centroids = torch.flip(centroids, dims=[1])\n",
    "\n",
    "INTERPOLATE = False\n",
    "\n",
    "print(f\"max_height: {centroids[:, 0].max()}\") \n",
    "print(f\"max width: {centroids[:, 1].max()}\")\n",
    "\n",
    "print(f\"frame feats: {frame_feats.shape}\")\n",
    "print(f\"raw_frame feats: {features['raw_frames'].shape}\")\n",
    "\n",
    "att_matrix_ = att_matrix # .min(1).min(1)\n",
    "att_matrix_.shape\n",
    "h, w = features['raw_frames'].shape[-3], features['raw_frames'].shape[-2] \n",
    "H, W = 192, 320 # math.ceil(centroids[:, 0].max()), math.ceil(centroids[:, 1].max())\n",
    "\n",
    "att_matrix_h_w = torch.tensor(att_matrix_[:, 1:].reshape(-1, h, w))[:centroids.shape[0]]\n",
    "\n",
    "outpatt_matrix_H_W = att_matrix_h_w\n",
    "# # convolve with mean filter\n",
    "# outpatt_matrix_H_W = F.avg_pool2d(outpatt_matrix_H_W.unsqueeze(1), kernel_size=2, stride=1).squeeze(1)\n",
    "# # second convolve with mean filter\n",
    "# outpatt_matrix_H_W = F.avg_pool2d(outpatt_matrix_H_W.unsqueeze(1), kernel_size=2, stride=1).squeeze(1)\n",
    "# # third convolve with mean filter\n",
    "# outpatt_matrix_H_W = F.avg_pool2d(outpatt_matrix_H_W.unsqueeze(1), kernel_size=2, stride=1).squeeze(1)\n",
    "\n",
    "if INTERPOLATE:\n",
    "    # Interpolate to new dimensions\n",
    "    print(f\"interpolating to {H}x{W}\")\n",
    "    outpatt_matrix_H_W = F.interpolate(att_matrix_h_w.unsqueeze(1), size=(H, W), mode='bilinear', align_corners=False).squeeze(1)\n",
    "else:\n",
    "    # resize centroids\"\n",
    "    print(f\"resizing centroids\")\n",
    "    centroids = centroids * torch.tensor([h/H, w/W]).unsqueeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"centroids: {centroids.shape}\")\n",
    "print(f\"max_centroids, h: {centroids[:, 0].max()}, w: {centroids[:, 1].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save_dir = \"./data/Combo3_SimNeu3D/centroids\"\n",
    "\n",
    "# # # # save att_matrix\n",
    "# # # if not os.path.exists(save_dir):\n",
    "# # #     os.mkdir(save_dir)\n",
    "# # # n_files = len(glob.glob(os.path.join(save_dir, \"*.npy\")))\n",
    "# # # save_path = os.path.join(save_dir, f\"{n_files}_outpatt_matrix_H_W.npy\")\n",
    "# # # np.save(save_path, outpatt_matrix_H_W)\n",
    "\n",
    "# # load att_matrix\n",
    "# att_path = \"./data/Combo3_SimNeu3D/centroids/0_outpatt_matrix_H_W.npy\"\n",
    "# outpatt_matrix_H_W = torch.tensor(np.load(att_path))\n",
    "\n",
    "# if not INTERPOLATE:\n",
    "#     outpatt_matrix_H_W = F.interpolate(outpatt_matrix_H_W.unsqueeze(1), size=(h, w), mode='bilinear', align_corners=False).squeeze(1)\n",
    "#     # outpatt_matrix_H_W = outpatt_matrix_H_W / torch.tensor([h/H]).unsqueeze(0).unsqueeze(-1)\n",
    "#     # outpatt_matrix_H_W = outpatt_matrix_H_W / torch.tensor([w/W]).unsqueeze(0).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_w = outpatt_matrix_H_W.mean((1), keepdim=True)\n",
    "# mean_h = outpatt_matrix_H_W.mean((2), keepdim=True)\n",
    "# outpatt_matrix_H_W = outpatt_matrix_H_W - mean_w - mean_h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the indices of max values in H and W dimensions\n",
    "max_values, max_indices = torch.max(outpatt_matrix_H_W.view(outpatt_matrix_H_W.size(0), -1), dim=1)\n",
    "\n",
    "# Convert the linear indices to H, W coordinates\n",
    "max_h = max_indices // outpatt_matrix_H_W.size(2)\n",
    "max_w = max_indices % outpatt_matrix_H_W.size(2)\n",
    "\n",
    "# Combine the coordinates into an (N, 2) tensor\n",
    "max_coordinates = torch.stack([max_h, max_w], dim=1)\n",
    "# keep only real neurons and discard PAD/EOS/SOS\n",
    "# Now output_tensor has shape (N, H, W) -> (5, 4, 6) in this example\n",
    "\n",
    "# calculate correlation between centroids and max coordinates\n",
    "corr_h = np.corrcoef(centroids[:, 0], max_coordinates[:, 0])\n",
    "corr_w = np.corrcoef(centroids[:, 1], max_coordinates[:, 1])\n",
    "\n",
    "print(f\"corr_h: {corr_h[0][1]}\")\n",
    "print(f\"corr_w: {corr_w[0][1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def plot_true_centroids_on_predicted_maps(true_centroids, predicted_maps, n_examples=None, columns=4):\n",
    "    \"\"\"\n",
    "    Plots 2D predicted maps with true centroids in a grid layout.\n",
    "    \n",
    "    :param true_centroids: Tensor of shape (N, 2) representing the true centroids (H, W)\n",
    "    :param predicted_maps: Tensor of shape (N, H, W) representing the predicted maps\n",
    "    :param columns: Number of columns in the grid layout\n",
    "    \"\"\"\n",
    "    if not isinstance(true_centroids, torch.Tensor) or not isinstance(predicted_maps, torch.Tensor):\n",
    "        raise ValueError(\"Input should be PyTorch tensors\")\n",
    "    \n",
    "    if n_examples is not None:\n",
    "        # Select a random subset of examples\n",
    "        indices = np.random.choice(true_centroids.shape[0], n_examples, replace=False)\n",
    "        true_centroids = true_centroids[indices]\n",
    "        predicted_maps = predicted_maps[indices]\n",
    "    else:\n",
    "        indices = np.arange(true_centroids.shape[0])\n",
    "    \n",
    "    # Get the shape of the tensors\n",
    "    N, H, W = predicted_maps.shape\n",
    "    \n",
    "    # Ensure the true centroids have the compatible shape\n",
    "    if true_centroids.shape[0] != N or true_centroids.shape[1] != 2:\n",
    "        raise ValueError(\"True centroids must have shape (N, 2)\")\n",
    "    \n",
    "    # Calculate the number of rows needed for the grid\n",
    "    rows = (N + columns - 1) // columns\n",
    "    \n",
    "    # Set up the figure and axes\n",
    "    fig, axes = plt.subplots(rows, columns, figsize=(4 * columns, 4 * rows))\n",
    "    \n",
    "    # Flatten the axes to make it easier to iterate through\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot each map with centroids\n",
    "    for i, n in enumerate(indices):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot predicted map\n",
    "        ax.imshow(predicted_maps[i].detach().cpu().numpy(), cmap='viridis', extent=[0, W, H, 0])\n",
    "        \n",
    "        # Plot true centroid\n",
    "        centroid_h, centroid_w = true_centroids[i].detach().cpu().numpy()\n",
    "        ax.scatter(centroid_w, centroid_h, c='red', marker='x', label='True Centroid')\n",
    "        \n",
    "        # Set the aspect ratio to be equal to ensure that the map is square\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "        \n",
    "        # Additional settings\n",
    "        # ax.axis('off')\n",
    "        ax.set_title(f'Map {n}')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(N, rows * columns):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "true_centroids = torch.tensor([[2, 3], [1, 4], [2, 2], [3, 1], [1, 5]]) # Example true centroids tensor\n",
    "predicted_maps = torch.randn(5, 4, 6) # Example predicted maps tensor\n",
    "plot_true_centroids_on_predicted_maps(centroids, outpatt_matrix_H_W, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids[77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_coordinates[77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
