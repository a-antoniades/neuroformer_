{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/neuroformer/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/neuroformer/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libjpeg.so.8: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import collections\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "import glob\n",
    "from pathlib import Path, PurePath\n",
    "path = Path.cwd()\n",
    "parent_path = path.parents[1]\n",
    "sys.path.append(str(PurePath(parent_path, 'neuroformer')))\n",
    "sys.path.append('neuroformer')\n",
    "sys.path.append('.')\n",
    "sys.path.append('../')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from neuroformer.trainer import Trainer, TrainerConfig\n",
    "from neuroformer.utils import set_seed\n",
    "\n",
    "\n",
    "from scipy import io as scipyio\n",
    "from scipy.special import softmax\n",
    "import skimage\n",
    "import skvideo.io\n",
    "from neuroformer.utils import print_full\n",
    "from scipy.ndimage import gaussian_filter, uniform_filter\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from neuroformer.visualize import *\n",
    "set_plot_params()\n",
    "parent_path = os.path.dirname(os.path.dirname(os.getcwd())) + \"/\"\n",
    "\n",
    "\n",
    "from neuroformer.model_neuroformer import GPT, GPTConfig, neuralGPTConfig\n",
    "from neuroformer.trainer import Trainer, TrainerConfig\n",
    "\n",
    "\n",
    "import json\n",
    "# for i in {1..10}; do python3 -m gather_atts.py; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroformer.prepare_data import load_LRN\n",
    "\n",
    "df, stimulus = load_LRN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import set_seed\n",
    "n_seed = 25\n",
    "set_seed(n_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config files\n",
    "\n",
    "import yaml\n",
    "\n",
    "base_path = \"./models/tensorboard/LRN/channel/2_window:0.5_prev:19.5/sparse_f:None_id:None/w:0.5_wp:19.5\"\n",
    "\n",
    "with open(os.path.join(base_path, 'mconf.yaml'), 'r') as stream:\n",
    "    mconf = yaml.full_load(stream)\n",
    "\n",
    "with open(os.path.join(base_path, 'tconf.yaml'), 'r') as stream:\n",
    "    tconf = yaml.full_load(stream)\n",
    "\n",
    "with open(os.path.join(base_path, 'dconf.yaml'), 'r') as stream:\n",
    "    dconf = yaml.full_load(stream)\n",
    "\n",
    "import omegaconf\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# open yaml as omegacong\n",
    "mconf = OmegaConf.create(mconf)\n",
    "tconf = OmegaConf.create(tconf)\n",
    "dconf = OmegaConf.create(dconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(parent_path + \"code/data/OneCombo3/Combo3_all_stim.csv\")\n",
    "w_mult = 3\n",
    "frame_window = dconf.frame_window\n",
    "window = dconf.window\n",
    "window_prev = dconf.window_prev\n",
    "dt = dconf.dt\n",
    "dt_frames = dconf.dt_frames\n",
    "# p_window = window / (window + window_prev)\n",
    "# intervals = np.load(os.path.join(base_path, \"intervals.npy\"))\n",
    "intervals = None\n",
    "\n",
    "\n",
    "from SpikeVidUtils import make_intervals\n",
    "\n",
    "df['real_interval'] = make_intervals(df, dt)\n",
    "df['Interval'] = make_intervals(df, window)\n",
    "df['Interval_2'] = make_intervals(df, window_prev)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# n_dt = sorted((df['Interval_dt'].unique()).round(2)) \n",
    "max_window = max(window, window_prev)\n",
    "dt_range = math.ceil(max_window / dt) + 1  # add first / last interval for SOS / EOS'\n",
    "n_dt = [round(dt * n, 2) for n in range(dt_range)] + ['EOS'] + ['PAD']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.68422611538335\n"
     ]
    }
   ],
   "source": [
    "int_trials = df.groupby(['Interval', 'Trial']).size()\n",
    "print(int_trials.mean())\n",
    "# df.groupby(['Interval', 'Trial']).agg(['nunique'])model_path\n",
    "# var_group = 'Interval_2'\n",
    "# n_unique = len(df.groupby([var_group, 'Trial']).size())\n",
    "# df.groupby([var_group, 'Trial']).size().nlargest(int(0.2 * n_unique))\n",
    "# df.groupby(['Interval_2', 'Trial']).size().mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SpikeVidUtils import SpikeTimeVidData2\n",
    "\n",
    "## resnet3d feats\n",
    "n_embd = mconf.n_embd\n",
    "frame_feats = torch.tensor(stimulus, dtype=torch.float32).transpose(1, 0)\n",
    "frame_block_size = mconf.frame_block_size  # math.ceil(frame_feats.shape[-1] * frame_window)\n",
    "n_embd_frames = mconf.n_embd_frames\n",
    "\n",
    "prev_id_block_size = mconf.prev_id_block_size    # math.ceil(frame_block_size * (1 - p_window))\n",
    "id_block_size = mconf.id_block_size           # math.ceil(frame_block_size * p_window)\n",
    "block_size = frame_block_size + id_block_size + prev_id_block_size # frame_block_size * 2  # small window for faster training\n",
    "frame_memory = dconf.frame_memory   # how many frames back does model see\n",
    "\n",
    "neurons = sorted(list(set(df['ID'])))\n",
    "id_stoi = { ch:i for i,ch in enumerate(neurons) }\n",
    "id_itos = { i:ch for i,ch in enumerate(neurons) }\n",
    "\n",
    "# translate neural embeddings to separate them from ID embeddings\n",
    "neurons = sorted(list(set(df['ID'].unique())))\n",
    "trial_tokens = [f\"Trial {n}\" for n in df['Trial'].unique()]\n",
    "feat_encodings = neurons + ['SOS'] + ['EOS'] + ['PAD']  # + pixels \n",
    "stoi = { ch:i for i,ch in enumerate(feat_encodings) }\n",
    "itos = { i:ch for i,ch in enumerate(feat_encodings) }\n",
    "stoi_dt = { ch:i for i,ch in enumerate(n_dt) }\n",
    "itos_dt = { i:ch for i,ch in enumerate(n_dt) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_split = 0.8\n",
    "train_trials = sorted(df['Trial'].unique())[:int(len(df['Trial'].unique()) * r_split)]\n",
    "train_data = df[df['Trial'].isin(train_trials)]\n",
    "test_data = df[~df['Trial'].isin(train_trials)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 705418 Neurons: 1003\n",
      "id block size: 150\n",
      "frames: 200, id: 150\n",
      "Length: 177367 Neurons: 1003\n",
      "id block size: 150\n",
      "frames: 200, id: 150\n",
      "train: 58888, test: 14725\n"
     ]
    }
   ],
   "source": [
    "from SpikeVidUtils import SpikeTimeVidData2\n",
    "\n",
    "# train_dat1aset = spikeTimeData(spikes, block_size, dt, stoi, itos)\n",
    "\n",
    "\n",
    "train_dataset = SpikeTimeVidData2(train_data, None, block_size, id_block_size, frame_block_size, prev_id_block_size, \n",
    "                                  window, dt, frame_memory, stoi, itos, neurons, stoi_dt, itos_dt, frame_feats,\n",
    "                                  pred=False, window_prev=window_prev, frame_window=frame_window,\n",
    "                                  dt_frames=dt_frames, intervals=intervals)\n",
    "test_dataset = SpikeTimeVidData2(test_data, None, block_size, id_block_size, frame_block_size, prev_id_block_size, \n",
    "                                 window, dt, frame_memory, stoi, itos, neurons, stoi_dt, itos_dt, frame_feats, \n",
    "                                 pred=False, window_prev=window_prev, frame_window=frame_window,\n",
    "                                 dt_frames=dt_frames, intervals=intervals)\n",
    "\n",
    "print(f'train: {len(train_dataset)}, test: {len(test_dataset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import get_class_weights\n",
    "# class_weights = get_class_weights(train_dataset, stoi, stoi_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/01/2023 23:28:33 - INFO - model_neuroformer_LRN -   number of parameters: 2.603624e+07\n"
     ]
    }
   ],
   "source": [
    "from model_neuroformer_LRN import GPT, GPTConfig\n",
    "# initialize config class and model (holds hyperparameters)\n",
    "   \n",
    "conv_layer = False\n",
    "model_conf = GPTConfig(train_dataset.population_size, block_size,    # frame_block_size\n",
    "                        id_vocab_size=train_dataset.id_population_size,\n",
    "                        frame_block_size=frame_block_size,\n",
    "                        id_block_size=id_block_size,  # frame_block_size\n",
    "                        prev_id_block_size=prev_id_block_size,\n",
    "                        sparse_mask=False, p_sparse=0.25, \n",
    "                        sparse_topk_frame=None, sparse_topk_id=None, sparse_topk_prev_id=None,\n",
    "                        n_dt=len(n_dt),\n",
    "                        data_size=train_dataset.size,\n",
    "                        class_weights=None,\n",
    "                        pretrain=False,\n",
    "                        n_state_layers=mconf.n_state_layers, n_state_history_layers=mconf.n_state_history_layers,\n",
    "                        n_stimulus_layers=mconf.n_stimulus_layers, self_att_layers=mconf.self_att_layers,\n",
    "                        n_head=mconf.n_head, n_embd=mconf.n_embd, \n",
    "                        contrastive=True, clip_emb=1024, clip_temp=0.5,\n",
    "                        temp_emb=True, pos_emb=False,\n",
    "                        id_drop=0.35, im_drop=0.35,\n",
    "                        window=window, window_prev=window_prev, frame_window=frame_window, dt=dt,\n",
    "                        n_embd_frames=n_embd_frames, dataset=None,\n",
    "                        ignore_index_id=stoi['PAD'], ignore_index_dt=stoi_dt['PAD'])  # 0.35\n",
    "\n",
    "for k, v in model_conf.__dict__.items():\n",
    "    if not hasattr(mconf, k):\n",
    "        print(f\"k: {k}, v: {v}\")\n",
    "        setattr(mconf, k, v)\n",
    "\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = (mconf.n_state_layers, mconf.n_state_history_layers, mconf.n_stimulus_layers)\n",
    "max_epochs = 300\n",
    "batch_size = round((14))\n",
    "shuffle = True\n",
    "\n",
    "weighted = True if mconf.class_weights is not None else False\n",
    "title =  f'window:{window}_prev:{window_prev}_smooth'\n",
    "model_path = f\"\"\"./models/tensorboard/LRN/ignore_index/2_{title}/sparse_f:{mconf.sparse_topk_frame}_id:{mconf.sparse_topk_id}/w:{window}_wp:{window_prev}/{6}_Cont:{mconf.contrastive}_window:{window}_f_window:{frame_window}_df:{dt}_blocksize:{id_block_size}_conv_{conv_layer}_shuffle:{shuffle}_batch:{batch_size}_sparse_({mconf.sparse_topk_frame}_{mconf.sparse_topk_id})_blocksz{block_size}_pos_emb:{mconf.pos_emb}_temp_emb:{mconf.temp_emb}_drop:{mconf.id_drop}_dt:{shuffle}_2.0_{max(stoi_dt.values())}_max{dt}_{layers}_{mconf.n_head}_{mconf.n_embd}.pt\"\"\"\n",
    "\n",
    "# model_path = \"/data5/antonis/neuroformer/models/tensorboard/LRN/channel/window:0.5_prev:19.5_smooth/sparse_f:None_id:None/w:0.5_wp:19.5/6_Cont:True_window:0.5_f_window:20_df:0.1_blocksize:150_conv_False_shuffle:True_batch:12_sparse_(None_None)_blocksz1150_pos_emb:False_temp_emb:True_drop:0.35_dt:True_2.0_197_max0.1_(8, 8, 8)_8_256.pt\"\n",
    "# if os.path.exists(model_path):\n",
    "#     print(f\"Loading model from {model_path}\")\n",
    "#     model.load_state_dict(torch.load(model_path))\n",
    "# else:\n",
    "#     print(f\"Model not found at {model_path}\")\n",
    "#     raise FileNotFoundError\n",
    "\n",
    "tconf = TrainerConfig(max_epochs=max_epochs, batch_size=batch_size, learning_rate=1e-4, \n",
    "                    num_workers=4, lr_decay=False, patience=3, warmup_tokens=8e7, \n",
    "                    decay_weights=True, weight_decay=0.2, shuffle=shuffle,\n",
    "                    final_tokens=len(train_dataset)*(id_block_size) * (max_epochs),\n",
    "                    clip_norm=1.0, grad_norm_clip=1.0,\n",
    "                    dataset='higher_order', mode='predict',\n",
    "                    block_size=train_dataset.block_size,\n",
    "                    id_block_size=train_dataset.id_block_size,\n",
    "                    show_grads=False, plot_raster=False,\n",
    "                    ckpt_path=model_path, no_pbar=False, \n",
    "                    dist=True, save_every=1000)\n",
    "\n",
    "# trainer = Trainer(model, train_dataset, test_dataset, tconf, mconf)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(train_dataset, batch_size=1, shuffle=shuffle, num_workers=4, pin_memory=True)\n",
    "iterable = iter(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from 6_Cont:False_window:0.5_f_window:20_df:0.1_blocksize:150_conv_False_shuffle:True_batch:32_sparse_(None_None)_blocksz1150_pos_emb:False_temp_emb:True_drop:0.35_dt:True_2.0_197_max0.1_(8, 8, 8)_8_256.pt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "RUN SIMULATION\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from utils import *\n",
    "from IPython.utils import io\n",
    "# top_p=0.25, top_p_t=0.9, temp=2.\n",
    "\n",
    "model_weights = glob.glob(os.path.join(base_path, '**/**.pt'), recursive=True)\n",
    "model_weights = sorted(model_weights, key=os.path.getmtime, reverse=True)\n",
    "assert len(model_weights) > 0, \"No model weights found\"\n",
    "\n",
    "\n",
    "if model_path in model_weights:\n",
    "    load_weights = model_path\n",
    "else:\n",
    "    print(f'Loading weights from {os.path.basename(model_weights[0])}')\n",
    "    load_weights = model_weights[0]\n",
    "\n",
    "model.load_state_dict(torch.load(load_weights, map_location=torch.device('cpu')))\n",
    "\n",
    "trials = test_data['Trial'].unique()[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, features, loss = model(x, y)\n",
    "# model.neural_visual_transformer.neural_state_blocks[0].attn.att.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "\n",
    "# helper function for hook\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        features[name] = output.detach().cpu()\n",
    "    return hook\n",
    "\n",
    "grads = {}\n",
    "def get_grads(name):\n",
    "    def hook(model, input, output):\n",
    "        grads[name] = output.detach().cpu()\n",
    "    return hook\n",
    "\n",
    "\"\"\"\n",
    "register forward hooks for all multimodal transformer layers\n",
    "so that the features are saved after every forward pass\n",
    "\"\"\"\n",
    "\n",
    "for n, mod in enumerate(model.neural_visual_transformer.neural_state_blocks):\n",
    "    mod.register_forward_hook(get_features(f'neural_state_block_{n}'))\n",
    "\n",
    "for n, mod in enumerate(model.neural_visual_transformer.neural_state_history_blocks):\n",
    "    mod.register_forward_hook(get_features(f'neural_state_history_block_{n}'))\n",
    "\n",
    "for n, mod in enumerate(model.neural_visual_transformer.neural_state_history_self_attention):\n",
    "    mod.register_forward_hook(get_features(f'neural_state_history_self_attention_{n}'))\n",
    "\n",
    "for n, mod in enumerate(model.neural_visual_transformer.neural_state_stimulus_blocks):\n",
    "    mod.register_forward_hook(get_features(f'neural_state_stimulus_block_{n}'))\n",
    "\n",
    "for n, mod in enumerate(model.neural_visual_transformer.neural_state_stimulus_blocks):\n",
    "    mod.attn.attn_drop.register_full_backward_hook(get_grads(f'neural_state_stimulus_block_{n}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "do a forward pass and save the features\n",
    "\"\"\"\n",
    "\n",
    "x, y  = next(iterable)\n",
    "\n",
    "preds = []\n",
    "feats = []\n",
    "\n",
    "features = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits, feats, loss = model(x, y)\n",
    "    for n, mod in enumerate(model.neural_visual_transformer.neural_state_blocks):\n",
    "        preds.append(features[f'neural_state_block_{n}'])\n",
    "    for n, mod in enumerate(model.neural_visual_transformer.neural_state_history_blocks):\n",
    "        preds.append(features[f'neural_state_history_block_{n}'])\n",
    "    for n, mod in enumerate(model.neural_visual_transformer.neural_state_history_self_attention):\n",
    "        preds.append(features[f'neural_state_history_self_attention_{n}'])\n",
    "    for n, mod in enumerate(model.neural_visual_transformer.neural_state_stimulus_blocks):\n",
    "        preds.append(features[f'neural_state_stimulus_block_{n}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_atts(name):\n",
    "    def hook(model, input, output):\n",
    "        attentions[name] = output\n",
    "    return hook\n",
    "\n",
    "\n",
    "def get_atts(model):\n",
    "    attentions = {}\n",
    "\n",
    "    for n, mod in enumerate(model.neural_visual_transformer.neural_state_blocks):\n",
    "        attentions[f'neural_state_block_{n}'] = mod.attn.att.detach().cpu()\n",
    "\n",
    "    for n, mod in enumerate(model.neural_visual_transformer.neural_state_history_blocks):\n",
    "        attentions[f'neural_state_history_block_{n}'] = mod.attn.att.detach().cpu()\n",
    "\n",
    "    for n, mod in enumerate(model.neural_visual_transformer.neural_state_stimulus_blocks):\n",
    "        attentions[f'neural_stimulus_block_{n}'] = mod.attn.att.detach().cpu()\n",
    "    \n",
    "    return attentions\n",
    "\n",
    "def get_grads(model):\n",
    "    grads = {}\n",
    "    \n",
    "    for n, mod in enumerate(model.neural_visual_transformer.neural_state_stimulus_blocks):\n",
    "        grads[f'neural_stimulus_block_{n}'] = mod.attn.att.detach().cpu()    \n",
    "    return grads\n",
    "\n",
    "def gradcam(atts, grads):\n",
    "    common_keys = set(atts.keys()).intersection(set(grads.keys()))\n",
    "    for key in common_keys:\n",
    "        atts[key] = atts[key] * grads[key]\n",
    "    return atts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dropout(p=0.2, inplace=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.neural_visual_transformer.neural_state_stimulus_blocks[0].attn.attn_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accum_atts(att_dict, key=None):\n",
    "    if key is None:\n",
    "        att_keys = att_dict.keys()\n",
    "    else:\n",
    "        att_keys = [k for k in att_dict.keys() if key in k]\n",
    "    atts = []\n",
    "    for k in att_keys:\n",
    "        att = att_dict[k]\n",
    "        att = att.sum(-3).detach().cpu()\n",
    "        reshape_c = att.shape[-1] // stimulus.shape[0]\n",
    "        assert att.shape[-1] % stimulus.shape[0] == 0, \"Attention shape does not match stimulus shape\"\n",
    "        att = att.view(att.shape[0], att.shape[-2], reshape_c, att.shape[-1] // reshape_c)\n",
    "        att = att.sum(-2)\n",
    "        atts.append(att)\n",
    "    return torch.stack(atts)\n",
    "\n",
    "def reshape_attentions(att_vis):\n",
    "    n_id_block, n_vis_block = att_vis.shape[-2], att_vis.shape[-1]\n",
    "    att_vis = att_vis.view(n_id_block, n_vis_block)\n",
    "    reshape_c = att_vis.shape[-1] // stimulus.shape[0]\n",
    "    assert att_vis.shape[-1] % stimulus.shape[0] == 0, \"Attention shape does not match stimulus shape\"\n",
    "    att_vis = att_vis.view(att_vis.shape[0], reshape_c, att_vis.shape[1] // reshape_c)\n",
    "    att_vis = att_vis.sum(-2)\n",
    "    return att_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_device(data, device):\n",
    "    device = torch.device(device)\n",
    "    if isinstance(data, dict):\n",
    "        return {k: all_device(v, device) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [all_device(v, device) for v in data]\n",
    "    elif isinstance(data, tuple):\n",
    "        return tuple(all_device(v, device) for v in data)\n",
    "    else:\n",
    "        return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 881109 Neurons: 1003\n",
      "id block size: 150\n",
      "frames: 200, id: 150\n"
     ]
    }
   ],
   "source": [
    "att_matrix = np.zeros((len(stoi.keys()), 1000))\n",
    "neurons = sorted(list(set(df['ID'].unique())))\n",
    "\n",
    "att_data = df[df['Trial'].isin([i for i in range(0, 500)])]\n",
    "att_dataset = SpikeTimeVidData2(att_data, None, block_size, id_block_size, frame_block_size, prev_id_block_size, \n",
    "                                  window, dt, frame_memory, stoi, itos, neurons, stoi_dt, itos_dt, frame_feats,\n",
    "                                  pred=False, window_prev=window_prev, frame_window=frame_window,\n",
    "                                  dt_frames=dt_frames, intervals=intervals)\n",
    "loader = DataLoader(att_dataset, batch_size=3, shuffle=False, num_workers=1)\n",
    "# model = model.to(\"cuda\")\n",
    "model.load_state_dict(torch.load(load_weights, map_location=torch.device('cpu')))\n",
    "model = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "model.to(device)\n",
    "model.zero_grad()\n",
    "pbar = tqdm(loader)\n",
    "\n",
    "model.eval()\n",
    "att_matrix = np.zeros((1000, 1000))\n",
    "# att_matrix = np.load(\"./models/tensorboard/LRN/channel/2_window:0.5_prev:19.5/sparse_f:None_id:None/w:0.5_wp:19.5/attentions/1_False_att_matrix_gradcam.npy\")\n",
    "\n",
    "grad_cond = False\n",
    "for x, y in pbar:\n",
    "    model.zero_grad()\n",
    "    x, y = all_device((x, y), device)\n",
    "    model.to(device)\n",
    "    with torch.set_grad_enabled(grad_cond):\n",
    "        _, _, _, = model(x, y)\n",
    "    # model.cpu()\n",
    "    x, y = all_device((x, y), \"cpu\")\n",
    "    attentions = get_atts(model)\n",
    "    if grad_cond:\n",
    "        gradients = get_grads(model)\n",
    "        attentions = gradcam(attentions, gradients)\n",
    "    # x, y = all_device((x, y), \"cpu\")\n",
    "    att = accum_atts(attentions, key='neural_stimulus_block').sum(0)\n",
    "    if len(att.size()) > 2:\n",
    "        # flatten batch\n",
    "        att = att.view(-1, 1000)\n",
    "    x_id = x['id'].flatten()\n",
    "    y_id = y['id'].flatten()\n",
    "    eos_idx = (x_id == stoi['EOS']).nonzero()\n",
    "    index_ranges = [1, eos_idx[0]]\n",
    "    x_id = x_id[index_ranges[0]:index_ranges[1]]\n",
    "    y_id = y_id[index_ranges[0]-1:index_ranges[1]-1]\n",
    "    att = att[index_ranges[0]:index_ranges[1]]\n",
    "    neurons = [int(itos[int(n)]) if not isinstance(itos[int(n)], str) else 1002 for n in x_id]\n",
    "    if len(att) > 1:\n",
    "        for n, neuron in enumerate(neurons):\n",
    "            att_matrix[neuron] += np.array(att[n])\n",
    "    # clear gpu memory\n",
    "    del x, y, attentions, att, x_id, y_id, eos_idx, index_ranges, neurons\n",
    "    if grad_cond:\n",
    "        del gradients\n",
    "    model.zero_grad()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# save att_matrix\n",
    "att_path = os.path.join(base_path, \"attentions\")\n",
    "if not os.path.exists(att_path):\n",
    "    os.mkdir(att_path)\n",
    "n_files = len(glob.glob(os.path.join(att_path, \"*.npy\")))\n",
    "\n",
    "save_path = os.path.join(att_path, f\"{n_files}_att_matrix_gradcam_{grad_cond}.npy\")\n",
    "np.save(save_path, att_matrix)\n",
    "print(f\"Saved attention matrix to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(att_dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "iterable = iter(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "x, y = next(iterable)\n",
    "_, _, _, = model(x, y)\n",
    "attentions = get_atts(model)\n",
    "att = accum_atts(attentions, key='neural_stimulus_block')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradcam(atts, grads):\n",
    "    common_keys = set(atts.keys()).intersection(set(grads.keys()))\n",
    "    for key in common_keys:\n",
    "        atts[key] = atts[key] * grads[key].clamp(min=0)\n",
    "    return atts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer = f\"neural_stimulus_block_{n}\".format(n=mconf.n_stimulus_layers-1)\n",
    "attentions[last_layer].min(dim=1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_vis = accum_atts(attentions, key='neural_stimulus_block').mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150, 1000])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_vis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 1000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions[last_layer].min(dim=1)[0][0].numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [04:54<?, ?it/s]\n",
      "/tmp/ipykernel_157776/163276799.py:48: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     24\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m---> 25\u001b[0m     _, _, _, \u001b[39m=\u001b[39m model(x, y)\n\u001b[1;32m     26\u001b[0m attentions \u001b[39m=\u001b[39m get_atts(model)\n\u001b[1;32m     27\u001b[0m gradients \u001b[39m=\u001b[39m get_grads(model)\n",
      "File \u001b[0;32m/opt/conda/envs/neuroformer/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/neuroformer/neuroformer/model_neuroformer_LRN.py:1043\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, x, targets)\u001b[0m\n\u001b[1;32m   1041\u001b[0m features, pad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_features(x)\n\u001b[1;32m   1042\u001b[0m \u001b[39m# features['id'] =\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mneural_visual_transformer(features)\n\u001b[1;32m   1044\u001b[0m id_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_id(x)\n\u001b[1;32m   1045\u001b[0m dt_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dt(x)    \u001b[39m# (B, T_id, 1)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/neuroformer/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/neuroformer/neuroformer/model_neuroformer_LRN.py:833\u001b[0m, in \u001b[0;36mMultimodalTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    831\u001b[0m     x \u001b[39m=\u001b[39m mod(x, stimulus, stimulus)\n\u001b[1;32m    832\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneural_state_blocks:\n\u001b[0;32m--> 833\u001b[0m     x \u001b[39m=\u001b[39m mod(x, x, x, mask)\n\u001b[1;32m    834\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneural_state_history_blocks:\n\u001b[1;32m    835\u001b[0m     x \u001b[39m=\u001b[39m mod(x, neural_history, neural_history)\n",
      "File \u001b[0;32m/opt/conda/envs/neuroformer/lib/python3.9/site-packages/torch/nn/modules/module.py:1212\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1209\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1210\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> 1212\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/neuroformer/neuroformer/model_neuroformer_LRN.py:654\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, q, k, v, mask, pad, dtx)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[39m# x = self.ln1(x + self.attn(x, k, v, mask, pad))\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[39m# x = self.ln2(x + self.mlp(x))\u001b[39;00m\n\u001b[1;32m    653\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln1(x), k, v, mask, pad)\n\u001b[0;32m--> 654\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln2(x))\n\u001b[1;32m    655\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_f(x)\n",
      "File \u001b[0;32m/opt/conda/envs/neuroformer/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/neuroformer/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/neuroformer/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/neuroformer/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from visualize import set_plot_white\n",
    "set_plot_white()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "\n",
    "loader = DataLoader(att_dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "iterable = iter(loader)\n",
    "\n",
    "last_layer = f\"neural_stimulus_block_{n}\".format(n=mconf.n_stimulus_layers-1)\n",
    "n_neurons = 10\n",
    "ncols = 5\n",
    "nrows = n_neurons // ncols\n",
    "\n",
    "plt.figure(figsize=(40, (20) * (n_neurons // 20)))\n",
    "\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "n_idx = 3\n",
    "counter = 0\n",
    "pbar = tqdm(range(n_neurons))\n",
    "while pbar:\n",
    "    x, y = next(iterable)\n",
    "    model.zero_grad()\n",
    "    with torch.set_grad_enabled(True):\n",
    "        _, _, _, = model(x, y)\n",
    "    attentions = get_atts(model)\n",
    "    gradients = get_grads(model)\n",
    "    attentions = gradcam(attentions, gradients)\n",
    "\n",
    "    att_vis = attentions[last_layer].min(dim=1)[0][0].numpy()\n",
    "    \n",
    "    # # x, y = all_device((x, y), \"cpu\")\n",
    "    # att_vis = accum_atts(attentions, key='neural_stimulus_block').view(-1, 1000)\n",
    "    # # att_id, att_vis_grad = interpret(x, y, model)\n",
    "    # # att_vis_grad = reshape_attentions(att_vis)\n",
    "\n",
    "    # for n_idx in range(att_vis.shape[0]):\n",
    "        # n_idx = 1\n",
    "    try:\n",
    "        neuron_x = int(itos[int(x['id'].flatten()[n_idx])])\n",
    "        neuron_y = int(itos[int(y['id'].flatten()[n_idx])])\n",
    "        counter += 1\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    lw = 5\n",
    "    fs = 15\n",
    "    plt.figure()\n",
    "    plt.grid()\n",
    "    plt.title(f\"x: {neuron_x}, y: {neuron_y}\", fontsize=20)\n",
    "    plt.plot(att_vis[n_idx])\n",
    "    plt.axvline(x=neuron_x, color='b', label='x', linewidth=lw)\n",
    "    plt.axvline(x=neuron_y, color='g', label='y', linewidth=lw)\n",
    "    save_dir = os.path.join(base_path, \"gradcam\")\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    n_plots = glob.glob(os.path.join(save_dir, \"**/**.png\"))\n",
    "    # plt.savefig(os.path.join(save_dir, f\"{len(n_plots)}.png\"))\n",
    "    # plt.plot(att_vis_grad[n_idx], color='purple', linestyle='--', label='grad')\n",
    "    # plt.xlabel(\"Channel\", fontsize=fs)\n",
    "    # plt.ylabel(\"Attention\", fontsize=fs)\n",
    "    plt.close()\n",
    "    if counter == 0:\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attentionVis import interpret\n",
    "\n",
    "id_x = x['id'].flatten()[n_idx]\n",
    "neuron_x = int(itos[int(id_x)])\n",
    "\n",
    "R_id, R_id_vis = interpret(x, y, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_atts(R_id_vis, key='neural_stimulus_block')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_attentions(att_vis):\n",
    "    n_id_block, n_vis_block = att_vis.shape[-2], att_vis.shape[-1]\n",
    "    att_vis = att_vis.view(n_id_block, n_vis_block)\n",
    "    reshape_c = att_vis.shape[-1] // stimulus.shape[0]\n",
    "    assert att_vis.shape[-1] % stimulus.shape[0] == 0, \"Attention shape does not match stimulus shape\"\n",
    "    att_vis = att_vis.view(att_vis.shape[0], reshape_c, att_vis.shape[1] // reshape_c)\n",
    "    att_vis = att_vis.sum(-2)\n",
    "    return att_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_attentions(R_id_vis).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_vis_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4878f327e989b61de6701446a3bf7b6f9ae7705c9c90fa2b4cdf5489a55bcfeb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
