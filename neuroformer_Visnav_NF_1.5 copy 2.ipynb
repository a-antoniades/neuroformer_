{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter\n",
      "CONTRASTIUVEEEEEEE False\n",
      "VISUAL: True\n",
      "PAST_STATE: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import sys\n",
    "import glob\n",
    "from pathlib import Path, PurePath\n",
    "path = Path.cwd()\n",
    "parent_path = path.parents[1]\n",
    "sys.path.append(str(PurePath(parent_path, 'neuroformer')))\n",
    "sys.path.append('neuroformer')\n",
    "sys.path.append('.')\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import math\n",
    "\n",
    "from neuroformer.model_neuroformer import GPT, GPTConfig\n",
    "from neuroformer.trainer import Trainer, TrainerConfig\n",
    "from neuroformer.utils_2 import (set_seed, update_object, \n",
    "                                 check_common_attrs, running_jupyter, \n",
    "                                 all_device, load_config, update_config, \n",
    "                                 dict_to_object, object_to_dict, recursive_print)\n",
    "from neuroformer.visualize import set_plot_params\n",
    "from neuroformer.SpikeVidUtils import make_intervals, round_n, SpikeTimeVidData2\n",
    "import gdown\n",
    "\n",
    "parent_path = os.path.dirname(os.path.dirname(os.getcwd())) + \"/\"\n",
    "\n",
    "import argparse\n",
    "from neuroformer.SpikeVidUtils import round_n\n",
    "\n",
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument(\"--infer\", action=\"store_true\", help=\"Inference mode\")\n",
    "    parser.add_argument(\"--train\", action=\"store_true\", default=False, help=\"Train mode\")\n",
    "    parser.add_argument(\"--dist\", action=\"store_true\", default=False, help=\"Distributed mode\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=25, help=\"Random seed\")\n",
    "    parser.add_argument(\"--resume\", type=str, default=None, help=\"Resume from checkpoint\")\n",
    "    parser.add_argument(\"--rand_perm\", action=\"store_true\", default=False, help=\"Randomly permute the ID column\")\n",
    "    parser.add_argument(\"--mconf\", type=str, default=None, help=\"Path to model config file\")\n",
    "    parser.add_argument(\"--eos_loss\", action=\"store_true\", default=False, help=\"Use EOS loss\")\n",
    "    parser.add_argument(\"--no_eos_dt\", action=\"store_true\", default=False, help=\"No EOS dt token\")\n",
    "    parser.add_argument(\"--downstream\", action=\"store_true\", default=False, help=\"Downstream task\")\n",
    "    parser.add_argument(\"--freeze_model\", action=\"store_true\", default=False, help=\"Freeze model\")\n",
    "    parser.add_argument(\"--title\", type=str, default=None)\n",
    "    parser.add_argument(\"--dataset\", type=str, default=\"Distance-Coding\")\n",
    "    parser.add_argument(\"--behavior\", action=\"store_true\", default=False, help=\"Behavior task\")\n",
    "    parser.add_argument(\"--pred_behavior\", action=\"store_true\", default=False, help=\"Predict behavior\")\n",
    "    parser.add_argument(\"--past_state\", action=\"store_true\", default=False, help=\"Input past state\")\n",
    "    parser.add_argument(\"--visual\", action=\"store_true\", default=False, help=\"Visualize\")\n",
    "    parser.add_argument(\"--contrastive\", action=\"store_true\", default=False, help=\"Contrastive\")\n",
    "    parser.add_argument(\"--clip_loss\", action=\"store_true\", default=False, help=\"Clip loss\")\n",
    "    parser.add_argument(\"--clip_vars\", nargs=\"+\", default=['id','frames'], help=\"Clip variables\")\n",
    "    parser.add_argument(\"--class_weights\", action=\"store_true\", default=False, help=\"Class weights\")\n",
    "    parser.add_argument(\"--resample\", action=\"store_true\", default=False, help=\"Resample\")\n",
    "    parser.add_argument(\"--loss_bprop\", type=str, default=None, help=\"Loss type to backpropagate\")\n",
    "    parser.add_argument(\"--config\", type=str, default=None, help=\"Config file\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "if running_jupyter(): # or __name__ == \"__main__\":\n",
    "    print(\"Running in Jupyter\")\n",
    "    INFERENCE = False\n",
    "    DIST = False\n",
    "    SEED = 69\n",
    "    DOWNSTREAM = False\n",
    "    TITLE = None\n",
    "    RESUME = None\n",
    "    RAND_PERM = False\n",
    "    MCONF = None\n",
    "    EOS_LOSS = False\n",
    "    NO_EOS_DT = False\n",
    "    FREEZE_MODEL = False\n",
    "    TITLE = None\n",
    "    DATASET = \"lateral\"\n",
    "    BEHAVIOR = False\n",
    "    PREDICT_BEHAVIOR = False\n",
    "    VISUAL = True\n",
    "    PAST_STATE = True\n",
    "    CONTRASTIVE = False\n",
    "    CLIP_LOSS = True\n",
    "    CLIP_VARS = ['id','frames']\n",
    "    CLASS_WEIGHTS = False\n",
    "    RESAMPLE_DATA = False\n",
    "    LOSS_BPROP = None\n",
    "    CONFIG = None\n",
    "else:\n",
    "    print(\"Running in terminal\")\n",
    "    args = parse_args()\n",
    "    INFERENCE = not args.train\n",
    "    DIST = args.dist\n",
    "    SEED = args.seed\n",
    "    DOWNSTREAM = args.downstream\n",
    "    TITLE = args.title\n",
    "    RESUME = args.resume\n",
    "    RAND_PERM = args.rand_perm\n",
    "    MCONF = args.mconf\n",
    "    EOS_LOSS = args.eos_loss\n",
    "    NO_EOS_DT = args.no_eos_dt\n",
    "    FREEZE_MODEL = args.freeze_model\n",
    "    DATASET = args.dataset\n",
    "    BEHAVIOR = args.behavior\n",
    "    PREDICT_BEHAVIOR = args.pred_behavior\n",
    "    VISUAL = args.visual\n",
    "    PAST_STATE = args.past_state\n",
    "    CONTRASTIVE = args.contrastive\n",
    "    CLIP_LOSS = args.clip_loss\n",
    "    CLIP_VARS = args.clip_vars\n",
    "    CLASS_WEIGHTS = args.class_weights\n",
    "    RESAMPLE_DATA = args.resample\n",
    "    LOSS_BPROP = args.loss_bprop\n",
    "    CONFIG = args.config\n",
    "\n",
    "# SET SEED - VERY IMPORTANT\n",
    "set_seed(SEED)\n",
    "\n",
    "print(f\"CONTRASTIUVEEEEEEE {CONTRASTIVE}\")\n",
    "print(f\"VISUAL: {VISUAL}\")\n",
    "print(f\"PAST_STATE: {PAST_STATE}\")\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spikes: (2023, 150578), speed: (30117,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "\n",
    "-- DATA --\n",
    "neuroformer/data/OneCombo3_V1AL/\n",
    "df = response\n",
    "video_stack = stimulus\n",
    "DOWNLOAD DATA URL = https://drive.google.com/drive/folders/1jNvA4f-epdpRmeG9s2E-2Sfo-pwYbjeY?usp=sharing\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from neuroformer.prepare_data import DataLinks\n",
    "from neuroformer.DataUtils import round_n\n",
    "\n",
    "\n",
    "if DATASET in [\"first\", \"visnav\"]:\n",
    "    data_path = \"./data/VisNav_VR_Expt\"\n",
    "elif DATASET == \"medial\":\n",
    "    data_path = \"./data/VisNav_VR_Expt/MedialVRDataset/\"\n",
    "elif DATASET == \"lateral\":\n",
    "    data_path = \"./data/VisNav_VR_Expt/LateralVRDataset\"\n",
    "\n",
    "spikes_path = f\"{data_path}/NF_1.5/spikerates_dt_0.01.npy\"\n",
    "speed_path = f\"{data_path}/NF_1.5/behavior_speed_dt_0.05.npy\"\n",
    "stim_path = f\"{data_path}/NF_1.5/stimulus.npy\"\n",
    "\n",
    "spikes = np.load(spikes_path)\n",
    "speed = np.round(np.load(speed_path), 3)\n",
    "stimulus = np.load(stim_path)\n",
    "\n",
    "frame_feats = None\n",
    "print(f\"spikes: {spikes.shape}, speed: {speed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function\n",
    "if CONFIG is None:\n",
    "    # config_path = \"./configs/NF_1.5/mconf.yaml\"\n",
    "    # config_path = \"./configs/NF_1.5/VisNav_VR_Expt/gru2_only/mconf.yaml\"\n",
    "    config_path = \"./configs/NF_1.5/VisNav_VR_Expt/mlp_only/mconf.yaml\"\n",
    "    # config_path = \"./configs/NF_1.5/VisNav_VR_Expt/gru2_only_cls/mconf.yaml\"\n",
    "else:\n",
    "    config_path = CONFIG\n",
    "config = load_config(config_path)  # replace 'config.yaml' with your file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = config.window.curr\n",
    "window_prev = config.window.prev\n",
    "dt = config.resolution.dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intervals.shape: (30116,)\n",
      "distance.shape: (30116,)\n",
      "{'tokens': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]}\n",
      "ID vocab size: 2026\n",
      "{'tokens': [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.0, 1.01, 1.02, 1.03, 1.04, 1.05, 1.06, 1.07, 1.08, 1.09, 1.1, 1.11, 1.12, 1.13, 1.14, 1.15, 1.16, 1.17, 1.18, 1.19, 1.2, 1.21, 1.22, 1.23, 1.24, 1.25, 1.26, 1.27, 1.28, 1.29, 1.3, 1.31, 1.32, 1.33, 1.34, 1.35, 1.36, 1.37, 1.38, 1.39, 1.4, 1.41, 1.42, 1.43, 1.44, 1.45, 1.46, 1.47, 1.48, 1.49, 1.5, 1.51, 1.52, 1.53, 1.54, 1.55, 1.56, 1.57, 1.58, 1.59, 1.6, 1.61, 1.62, 1.63, 1.64, 1.65, 1.66, 1.67, 1.68, 1.69, 1.7, 1.71, 1.72, 1.73, 1.74, 1.75, 1.76, 1.77, 1.78, 1.79, 1.8, 1.81, 1.82, 1.83, 1.84, 1.85, 1.86, 1.87, 1.88, 1.89, 1.9, 1.91, 1.92, 1.93, 1.94, 1.95, 1.96, 1.97, 1.98, 1.99, 2.0, 2.01, 2.02, 2.03, 2.04, 2.05, 2.06, 2.07, 2.08, 2.09, 2.1, 2.11, 2.12, 2.13, 2.14, 2.15, 2.16, 2.17, 2.18, 2.19, 2.2, 2.21, 2.22, 2.23, 2.24, 2.25, 2.26, 2.27, 2.28, 2.29, 2.3, 2.31, 2.32, 2.33, 2.34, 2.35, 2.36, 2.37, 2.38, 2.39, 2.4, 2.41, 2.42, 2.43, 2.44, 2.45, 2.46, 2.47, 2.48, 2.49, 2.5, 2.51, 2.52, 2.53, 2.54, 2.55, 2.56, 2.57, 2.58, 2.59, 2.6, 2.61, 2.62, 2.63, 2.64, 2.65, 2.66, 2.67, 2.68, 2.69, 2.7, 2.71, 2.72, 2.73, 2.74, 2.75, 2.76, 2.77, 2.78, 2.79, 2.8, 2.81, 2.82, 2.83, 2.84, 2.85, 2.86, 2.87, 2.88, 2.89, 2.9, 2.91, 2.92, 2.93, 2.94, 2.95, 2.96, 2.97, 2.98, 2.99, 3.0, 3.01, 3.02, 3.03, 3.04, 3.05, 3.06, 3.07, 3.08, 3.09, 3.1, 3.11, 3.12, 3.13, 3.14, 3.15, 3.16, 3.17, 3.18, 3.19, 3.2, 3.21, 3.22, 3.23, 3.24, 3.25, 3.26, 3.27, 3.28, 3.29, 3.3, 3.31, 3.32, 3.33, 3.34, 3.35, 3.36, 3.37, 3.38, 3.39, 3.4, 3.41, 3.42, 3.43, 3.44, 3.45, 3.46, 3.47, 3.48, 3.49, 3.5, 3.51, 3.52, 3.53, 3.54, 3.55, 3.56, 3.57, 3.58, 3.59, 3.6, 3.61, 3.62, 3.63, 3.64, 3.65, 3.66, 3.67, 3.68, 3.69, 3.7, 3.71, 3.72, 3.73, 3.74, 3.75, 3.76, 3.77, 3.78, 3.79, 3.8, 3.81, 3.82, 3.83, 3.84, 3.85, 3.86, 3.87, 3.88, 3.89, 3.9, 3.91, 3.92, 3.93, 3.94, 3.95, 3.96, 3.97, 3.98, 3.99, 4.0, 4.01, 4.02, 4.03, 4.04, 4.05, 4.06, 4.07, 4.08, 4.09, 4.1, 4.11, 4.12, 4.13, 4.14, 4.15, 4.16, 4.17, 4.18, 4.19, 4.2, 4.21, 4.22, 4.23, 4.24, 4.25, 4.26, 4.27, 4.28, 4.29, 4.3, 4.31, 4.32, 4.33, 4.34, 4.35, 4.36, 4.37, 4.38, 4.39, 4.4, 4.41, 4.42, 4.43, 4.44, 4.45, 4.46, 4.47, 4.48, 4.49, 4.5, 4.51, 4.52, 4.53, 4.54, 4.55, 4.56, 4.57, 4.58, 4.59, 4.6, 4.61, 4.62, 4.63, 4.64, 4.65, 4.66, 4.67, 4.68, 4.69, 4.7, 4.71, 4.72, 4.73, 4.74, 4.75, 4.76, 4.77, 4.78, 4.79, 4.8, 4.81, 4.82, 4.83, 4.84, 4.85, 4.86, 4.87, 4.88, 4.89, 4.9, 4.91, 4.92, 4.93, 4.94, 4.95, 4.96, 4.97, 4.98, 4.99, 5.0, 5.01, 5.02, 5.03, 5.04, 5.05, 5.06, 5.07, 5.08, 5.09, 5.1, 5.11, 5.12, 5.13, 5.14, 5.15, 5.16, 5.17, 5.18, 5.19, 5.2, 5.21, 5.22, 5.23, 5.24, 5.25, 5.26, 5.27, 5.28, 5.29, 5.3, 5.31, 5.32, 5.33, 5.34, 5.35, 5.36, 5.37, 5.38, 5.39, 5.4, 5.41, 5.42, 5.43, 5.44, 5.45, 5.46, 5.47, 5.48, 5.49, 5.5, 5.51, 5.52, 5.53, 5.54, 5.55, 5.56, 5.57, 5.58, 5.59, 5.6, 5.61, 5.62, 5.63, 5.64, 5.65, 5.66, 5.67, 5.68, 5.69, 5.7, 5.71, 5.72, 5.73, 5.74, 5.75, 5.76, 5.77, 5.78, 5.79, 5.8, 5.81, 5.82, 5.83, 5.84, 5.85, 5.86, 5.87, 5.88, 5.89, 5.9, 5.91, 5.92, 5.93, 5.94, 5.95, 5.96, 5.97, 5.98, 5.99, 6.0, 6.01, 6.02, 6.03, 6.04, 6.05, 6.06, 6.07, 6.08, 6.09, 6.1, 6.11, 6.12, 6.13, 6.14, 6.15, 6.16, 6.17, 6.18, 6.19, 6.2, 6.21, 6.22, 6.23, 6.24, 6.25, 6.26, 6.27, 6.28, 6.29, 6.3, 6.31, 6.32, 6.33, 6.34, 6.35, 6.36, 6.37, 6.38, 6.39, 6.4, 6.41, 6.42, 6.43, 6.44, 6.45, 6.46, 6.47, 6.48, 6.49, 6.5, 6.51, 6.52, 6.53, 6.54, 6.55, 6.56, 6.57, 6.58, 6.59, 6.6, 6.61, 6.62, 6.63, 6.64, 6.65, 6.66, 6.67, 6.68, 6.69, 6.7, 6.71, 6.72, 6.73, 6.74, 6.75, 6.76, 6.77, 6.78, 6.79, 6.8, 6.81, 6.82, 6.83, 6.84, 6.85, 6.86, 6.87, 6.88, 6.89, 6.9, 6.91, 6.92, 6.93, 6.94, 6.95, 6.96, 6.97, 6.98, 6.99, 7.0, 7.01, 7.02, 7.03, 7.04, 7.05, 7.06, 7.07, 7.08, 7.09, 7.1, 7.11, 7.12, 7.13, 7.14, 7.15, 7.16, 7.17, 7.18, 7.19, 7.2, 7.21, 7.22, 7.23, 7.24, 7.25, 7.26, 7.27, 7.28, 7.29, 7.3, 7.31, 7.32, 7.33, 7.34, 7.35, 7.36, 7.37, 7.38, 7.39, 7.4, 7.41, 7.42, 7.43, 7.44, 7.45, 7.46, 7.47, 7.48, 7.49, 7.5, 7.51, 7.52, 7.53, 7.54, 7.55, 7.56, 7.57, 7.58, 7.59, 7.6, 7.61, 7.62, 7.63, 7.64, 7.65, 7.66, 7.67, 7.68, 7.69, 7.7, 7.71, 7.72, 7.73, 7.74, 7.75, 7.76, 7.77, 7.78, 7.79, 7.8, 7.81, 7.82, 7.83, 7.84, 7.85, 7.86, 7.87, 7.88, 7.89, 7.9, 7.91, 7.92, 7.93, 7.94, 7.95, 7.96, 7.97, 7.98, 7.99, 8.0, 8.01, 8.02, 8.03, 8.04, 8.05, 8.06, 8.07, 8.08, 8.09, 8.1, 8.11, 8.12, 8.13, 8.14, 8.15, 8.16, 8.17, 8.18, 8.19, 8.2, 8.21, 8.22, 8.23, 8.24, 8.25, 8.26, 8.27, 8.28, 8.29, 8.3, 8.31, 8.32, 8.33, 8.34, 8.35, 8.36, 8.37, 8.38, 8.39, 8.4, 8.41, 8.42, 8.43, 8.44, 8.45, 8.46, 8.47, 8.48, 8.49, 8.5, 8.51, 8.52, 8.53, 8.54, 8.55, 8.56, 8.57, 8.58, 8.59, 8.6, 8.61, 8.62, 8.63, 8.64, 8.65, 8.66, 8.67, 8.68, 8.69, 8.7, 8.71, 8.72, 8.73, 8.74, 8.75, 8.76, 8.77, 8.78, 8.79, 8.8, 8.81, 8.82, 8.83, 8.84, 8.85, 8.86, 8.87, 8.88, 8.89, 8.9, 8.91, 8.92, 8.93, 8.94, 8.95, 8.96, 8.97, 8.98, 8.99, 9.0, 9.01, 9.02, 9.03, 9.04, 9.05, 9.06, 9.07, 9.08, 9.09, 9.1, 9.11, 9.12, 9.13, 9.14, 9.15, 9.16, 9.17, 9.18, 9.19, 9.2, 9.21, 9.22, 9.23, 9.24, 9.25, 9.26, 9.27, 9.28, 9.29, 9.3, 9.31, 9.32, 9.33, 9.34, 9.35, 9.36, 9.37, 9.38, 9.39, 9.4, 9.41, 9.42, 9.43, 9.44, 9.45, 9.46, 9.47, 9.48, 9.49, 9.5, 9.51, 9.52, 9.53, 9.54, 9.55, 9.56, 9.57, 9.58, 9.59, 9.6, 9.61, 9.62, 9.63, 9.64, 9.65, 9.66, 9.67, 9.68, 9.69, 9.7, 9.71, 9.72, 9.73, 9.74, 9.75, 9.76, 9.77, 9.78, 9.79, 9.8, 9.81, 9.82, 9.83, 9.84, 9.85, 9.86, 9.87, 9.88, 9.89, 9.9, 9.91, 9.92, 9.93, 9.94, 9.95, 9.96, 9.97, 9.98, 9.99, 10.0, 10.01, 10.02, 10.03, 10.04, 10.05, 10.06, 10.07, 10.08, 10.09, 10.1, 10.11, 10.12, 10.13, 10.14, 10.15, 10.16, 10.17, 10.18, 10.19, 10.2, 10.21, 10.22, 10.23, 10.24, 10.25, 10.26, 10.27, 10.28, 10.29, 10.3, 10.31, 10.32, 10.33, 10.34, 10.35, 10.36, 10.37, 10.38, 10.39, 10.4, 10.41, 10.42, 10.43, 10.44, 10.45, 10.46, 10.47, 10.48, 10.49, 10.5, 10.51, 10.52, 10.53, 10.54, 10.55, 10.56, 10.57, 10.58, 10.59, 10.6, 10.61, 10.62, 10.63, 10.64, 10.65, 10.66, 10.67, 10.68, 10.69, 10.7, 10.71, 10.72, 10.73, 10.74, 10.75, 10.76, 10.77, 10.78, 10.79, 10.8, 10.81, 10.82, 10.83, 10.84, 10.85, 10.86, 10.87, 10.88, 10.89, 10.9, 10.91, 10.92, 10.93, 10.94, 10.95, 10.96, 10.97, 10.98, 10.99, 11.0, 11.01, 11.02, 11.03, 11.04, 11.05, 11.06, 11.07, 11.08, 11.09, 11.1, 11.11, 11.12, 11.13, 11.14, 11.15, 11.16, 11.17, 11.18, 11.19, 11.2, 11.21, 11.22, 11.23, 11.24, 11.25, 11.26, 11.27, 11.28, 11.29, 11.3, 11.31, 11.32, 11.33, 11.34, 11.35, 11.36, 11.37, 11.38, 11.39, 11.4, 11.41, 11.42, 11.43, 11.44, 11.45, 11.46, 11.47, 11.48, 11.49, 11.5, 11.51, 11.52, 11.53, 11.54, 11.55, 11.56, 11.57, 11.58, 11.59, 11.6, 11.61, 11.62, 11.63, 11.64, 11.65, 11.66, 11.67, 11.68, 11.69, 11.7, 11.71, 11.72, 11.73, 11.74, 11.75, 11.76, 11.77, 11.78, 11.79, 11.8, 11.81, 11.82, 11.83, 11.84, 11.85, 11.86, 11.87, 11.88, 11.89, 11.9, 11.91, 11.92, 11.93, 11.94, 11.95, 11.96, 11.97, 11.98, 11.99, 12.0, 12.01, 12.02, 12.03, 12.04, 12.05, 12.06, 12.07, 12.08, 12.09, 12.1, 12.11, 12.12, 12.13, 12.14, 12.15, 12.16, 12.17, 12.18, 12.19, 12.2, 12.21, 12.22, 12.23, 12.24, 12.25, 12.26, 12.27, 12.28, 12.29, 12.3, 12.31, 12.32, 12.33, 12.34, 12.35, 12.36, 12.37, 12.38, 12.39, 12.4, 12.41, 12.42, 12.43, 12.44, 12.45, 12.46, 12.47, 12.48, 12.49, 12.5, 12.51, 12.52, 12.53, 12.54, 12.55, 12.56, 12.57, 12.58, 12.59, 12.6, 12.61, 12.62, 12.63, 12.64, 12.65, 12.66, 12.67, 12.68, 12.69, 12.7, 12.71, 12.72, 12.73, 12.74, 12.75, 12.76, 12.77, 12.78, 12.79, 12.8, 12.81, 12.82, 12.83, 12.84, 12.85, 12.86, 12.87, 12.88, 12.89, 12.9, 12.91, 12.92, 12.93, 12.94, 12.95, 12.96, 12.97, 12.98, 12.99, 13.0, 13.01, 13.02, 13.03, 13.04, 13.05, 13.06, 13.07, 13.08, 13.09, 13.1, 13.11, 13.12, 13.13, 13.14, 13.15, 13.16, 13.17, 13.18, 13.19, 13.2, 13.21, 13.22, 13.23, 13.24, 13.25, 13.26, 13.27, 13.28, 13.29, 13.3, 13.31, 13.32, 13.33, 13.34, 13.35, 13.36, 13.37, 13.38, 13.39, 13.4, 13.41, 13.42, 13.43, 13.44, 13.45, 13.46, 13.47, 13.48, 13.49, 13.5, 13.51, 13.52, 13.53, 13.54, 13.55, 13.56, 13.57, 13.58, 13.59, 13.6, 13.61, 13.62, 13.63, 13.64, 13.65, 13.66, 13.67, 13.68, 13.69, 13.7, 13.71, 13.72, 13.73, 13.74, 13.75, 13.76, 13.77, 13.78, 13.79, 13.8, 13.81, 13.82, 13.83, 13.84, 13.85, 13.86, 13.87, 13.88, 13.89, 13.9, 13.91, 13.92, 13.93, 13.94, 13.95, 13.96, 13.97, 13.98, 13.99, 14.0, 14.01, 14.02, 14.03, 14.04, 14.05, 14.06, 14.07, 14.08, 14.09, 14.1, 14.11, 14.12, 14.13, 14.14, 14.15, 14.16, 14.17, 14.18, 14.19, 14.2, 14.21, 14.22, 14.23, 14.24, 14.25, 14.26, 14.27, 14.28, 14.29, 14.3, 14.31, 14.32, 14.33, 14.34, 14.35, 14.36, 14.37, 14.38, 14.39, 14.4, 14.41, 14.42, 14.43, 14.44, 14.45, 14.46, 14.47, 14.48, 14.49, 14.5, 14.51, 14.52, 14.53, 14.54, 14.55, 14.56, 14.57, 14.58, 14.59, 14.6, 14.61, 14.62, 14.63, 14.64, 14.65, 14.66, 14.67, 14.68, 14.69, 14.7, 14.71, 14.72, 14.73, 14.74, 14.75, 14.76, 14.77, 14.78, 14.79, 14.8, 14.81, 14.82, 14.83, 14.84, 14.85, 14.86, 14.87, 14.88, 14.89, 14.9, 14.91, 14.92, 14.93, 14.94, 14.95, 14.96, 14.97, 14.98, 14.99, 15.0, 15.01, 15.02, 15.03, 15.04, 15.05, 15.06, 15.07, 15.08, 15.09, 15.1, 15.11, 15.12, 15.13, 15.14, 15.15, 15.16, 15.17, 15.18, 15.19, 15.2, 15.21, 15.22, 15.23, 15.24, 15.25, 15.26, 15.27, 15.28, 15.29, 15.3, 15.31, 15.32, 15.33, 15.34, 15.35, 15.36, 15.37, 15.38, 15.39, 15.4, 15.41, 15.42, 15.43, 15.44, 15.45, 15.46, 15.47, 15.48, 15.49, 15.5, 15.51, 15.52, 15.53, 15.54, 15.55, 15.56, 15.57, 15.58, 15.59, 15.6, 15.61, 15.62, 15.63, 15.64, 15.65, 15.66, 15.67, 15.68, 15.69, 15.7, 15.71, 15.72, 15.73, 15.74, 15.75, 15.76, 15.77, 15.78, 15.79, 15.8, 15.81, 15.82, 15.83, 15.84, 15.85, 15.86, 15.87, 15.88, 15.89, 15.9, 15.91, 15.92, 15.93, 15.94, 15.95, 15.96, 15.97, 15.98, 15.99, 16.0, 16.01, 16.02, 16.03, 16.04, 16.05, 16.06, 16.07, 16.08, 16.09, 16.1, 16.11, 16.12, 16.13, 16.14, 16.15, 16.16, 16.17, 16.18, 16.19, 16.2, 16.21, 16.22, 16.23, 16.24, 16.25, 16.26, 16.27, 16.28, 16.29, 16.3, 16.31, 16.32, 16.33, 16.34, 16.35, 16.36, 16.37, 16.38, 16.39, 16.4, 16.41, 16.42, 16.43, 16.44, 16.45, 16.46, 16.47, 16.48, 16.49, 16.5, 16.51, 16.52, 16.53, 16.54, 16.55, 16.56, 16.57, 16.58, 16.59, 16.6, 16.61, 16.62, 16.63, 16.64, 16.65, 16.66, 16.67, 16.68, 16.69, 16.7, 16.71, 16.72, 16.73, 16.74, 16.75, 16.76, 16.77, 16.78, 16.79, 16.8, 16.81, 16.82, 16.83, 16.84, 16.85, 16.86, 16.87, 16.88, 16.89, 16.9, 16.91, 16.92, 16.93, 16.94, 16.95, 16.96, 16.97, 16.98, 16.99, 17.0, 17.01, 17.02, 17.03, 17.04, 17.05, 17.06, 17.07, 17.08, 17.09, 17.1, 17.11, 17.12, 17.13, 17.14, 17.15, 17.16, 17.17, 17.18, 17.19, 17.2, 17.21, 17.22, 17.23, 17.24, 17.25, 17.26, 17.27, 17.28, 17.29, 17.3, 17.31, 17.32, 17.33, 17.34, 17.35, 17.36, 17.37, 17.38, 17.39, 17.4, 17.41, 17.42, 17.43, 17.44, 17.45, 17.46, 17.47, 17.48, 17.49, 17.5, 17.51, 17.52, 17.53, 17.54, 17.55, 17.56, 17.57, 17.58, 17.59, 17.6, 17.61, 17.62, 17.63, 17.64, 17.65, 17.66, 17.67, 17.68, 17.69, 17.7, 17.71, 17.72, 17.73, 17.74, 17.75, 17.76, 17.77, 17.78, 17.79, 17.8, 17.81, 17.82, 17.83, 17.84, 17.85, 17.86, 17.87, 17.88, 17.89, 17.9, 17.91, 17.92, 17.93, 17.94, 17.95, 17.96, 17.97, 17.98, 17.99, 18.0, 18.01, 18.02, 18.03, 18.04, 18.05, 18.06, 18.07, 18.08, 18.09, 18.1, 18.11, 18.12, 18.13, 18.14, 18.15, 18.16, 18.17, 18.18, 18.19, 18.2, 18.21, 18.22, 18.23, 18.24, 18.25, 18.26, 18.27, 18.28, 18.29, 18.3, 18.31, 18.32, 18.33, 18.34, 18.35, 18.36, 18.37, 18.38, 18.39, 18.4, 18.41, 18.42, 18.43, 18.44, 18.45, 18.46, 18.47, 18.48, 18.49, 18.5, 18.51, 18.52, 18.53, 18.54, 18.55, 18.56, 18.57, 18.58, 18.59, 18.6, 18.61, 18.62, 18.63, 18.64, 18.65, 18.66, 18.67, 18.68, 18.69, 18.7, 18.71, 18.72, 18.73, 18.74, 18.75, 18.76, 18.77, 18.78, 18.79, 18.8, 18.81, 18.82, 18.83, 18.84, 18.85, 18.86, 18.87, 18.88, 18.89, 18.9, 18.91, 18.92, 18.93, 18.94, 18.95, 18.96, 18.97, 18.98, 18.99, 19.0, 19.01, 19.02, 19.03, 19.04, 19.05, 19.06, 19.07, 19.08, 19.09, 19.1, 19.11, 19.12, 19.13, 19.14, 19.15, 19.16, 19.17, 19.18, 19.19, 19.2, 19.21, 19.22, 19.23, 19.24, 19.25, 19.26, 19.27, 19.28, 19.29, 19.3, 19.31, 19.32, 19.33, 19.34, 19.35, 19.36, 19.37, 19.38, 19.39, 19.4, 19.41, 19.42, 19.43, 19.44, 19.45, 19.46, 19.47, 19.48, 19.49, 19.5, 19.51, 19.52, 19.53, 19.54, 19.55, 19.56, 19.57, 19.58, 19.59, 19.6, 19.61, 19.62, 19.63, 19.64, 19.65, 19.66, 19.67, 19.68, 19.69, 19.7, 19.71, 19.72, 19.73, 19.74, 19.75, 19.76, 19.77, 19.78, 19.79, 19.8, 19.81, 19.82, 19.83, 19.84, 19.85, 19.86, 19.87, 19.88, 19.89, 19.9, 19.91, 19.92, 19.93, 19.94, 19.95, 19.96, 19.97, 19.98, 19.99, 20.0, 20.01, 20.02, 20.03, 20.04, 20.05, 20.06, 20.07, 20.08, 20.09, 20.1, 20.11, 20.12, 20.13, 20.14, 20.15, 20.16, 20.17, 20.18, 20.19, 20.2, 20.21, 20.22, 20.23, 20.24, 20.25, 20.26, 20.27, 20.28, 20.29, 20.3, 20.31, 20.32, 20.33, 20.34, 20.35, 20.36, 20.37, 20.38, 20.39, 20.4, 20.41, 20.42, 20.43, 20.44, 20.45, 20.46, 20.47, 20.48, 20.49, 20.5, 20.51, 20.52, 20.53, 20.54, 20.55, 20.56, 20.57, 20.58, 20.59, 20.6, 20.61, 20.62, 20.63, 20.64, 20.65, 20.66, 20.67, 20.68, 20.69, 20.7, 20.71, 20.72, 20.73, 20.74, 20.75, 20.76, 20.77, 20.78, 20.79, 20.8, 20.81, 20.82, 20.83, 20.84, 20.85, 20.86, 20.87, 20.88, 20.89, 20.9, 20.91, 20.92, 20.93, 20.94, 20.95, 20.96, 20.97, 20.98, 20.99, 21.0, 21.01, 21.02, 21.03, 21.04, 21.05, 21.06, 21.07, 21.08, 21.09, 21.1, 21.11, 21.12, 21.13, 21.14, 21.15, 21.16, 21.17, 21.18, 21.19, 21.2, 21.21, 21.22, 21.23, 21.24, 21.25, 21.26, 21.27, 21.28, 21.29, 21.3, 21.31, 21.32, 21.33, 21.34, 21.35, 21.36, 21.37, 21.38, 21.39, 21.4, 21.41, 21.42, 21.43, 21.44, 21.45, 21.46, 21.47, 21.48, 21.49, 21.5, 21.51, 21.52, 21.53, 21.54, 21.55, 21.56, 21.57, 21.58, 21.59, 21.6, 21.61, 21.62, 21.63, 21.64, 21.65, 21.66, 21.67, 21.68, 21.69, 21.7, 21.71, 21.72, 21.73, 21.74, 21.75, 21.76, 21.77, 21.78, 21.79, 21.8, 21.81, 21.82, 21.83, 21.84, 21.85, 21.86, 21.87, 21.88, 21.89, 21.9, 21.91, 21.92, 21.93, 21.94, 21.95, 21.96, 21.97, 21.98, 21.99, 22.0, 22.01, 22.02, 22.03, 22.04, 22.05, 22.06, 22.07, 22.08, 22.09, 22.1, 22.11, 22.12, 22.13, 22.14, 22.15, 22.16, 22.17, 22.18, 22.19, 22.2, 22.21, 22.22, 22.23, 22.24, 22.25, 22.26, 22.27, 22.28, 22.29, 22.3, 22.31, 22.32, 22.33, 22.34, 22.35, 22.36, 22.37, 22.38, 22.39, 22.4, 22.41, 22.42, 22.43, 22.44, 22.45, 22.46, 22.47, 22.48, 22.49, 22.5, 22.51, 22.52, 22.53, 22.54, 22.55, 22.56, 22.57, 22.58, 22.59, 22.6, 22.61, 22.62, 22.63, 22.64, 22.65, 22.66, 22.67, 22.68, 22.69, 22.7, 22.71, 22.72, 22.73, 22.74, 22.75, 22.76, 22.77, 22.78, 22.79, 22.8, 22.81, 22.82, 22.83, 22.84, 22.85, 22.86, 22.87, 22.88, 22.89, 22.9, 22.91, 22.92, 22.93, 22.94, 22.95, 22.96, 22.97, 22.98, 22.99, 23.0, 23.01, 23.02, 23.03, 23.04, 23.05, 23.06, 23.07, 23.08, 23.09, 23.1, 23.11, 23.12, 23.13, 23.14, 23.15, 23.16, 23.17, 23.18, 23.19, 23.2, 23.21, 23.22, 23.23, 23.24, 23.25, 23.26, 23.27, 23.28, 23.29, 23.3, 23.31, 23.32, 23.33, 23.34, 23.35, 23.36, 23.37, 23.38, 23.39, 23.4, 23.41, 23.42, 23.43, 23.44, 23.45, 23.46, 23.47, 23.48, 23.49, 23.5, 23.51, 23.52, 23.53, 23.54, 23.55, 23.56, 23.57, 23.58, 23.59, 23.6, 23.61, 23.62, 23.63, 23.64, 23.65, 23.66, 23.67, 23.68, 23.69, 23.7, 23.71, 23.72, 23.73, 23.74, 23.75, 23.76, 23.77, 23.78, 23.79, 23.8, 23.81, 23.82, 23.83, 23.84, 23.85, 23.86, 23.87, 23.88, 23.89, 23.9, 23.91, 23.92, 23.93, 23.94, 23.95, 23.96, 23.97, 23.98, 23.99, 24.0, 24.01, 24.02, 24.03, 24.04, 24.05, 24.06, 24.07, 24.08, 24.09, 24.1, 24.11, 24.12, 24.13, 24.14, 24.15, 24.16, 24.17, 24.18, 24.19, 24.2, 24.21, 24.22, 24.23, 24.24, 24.25, 24.26, 24.27, 24.28, 24.29, 24.3, 24.31, 24.32, 24.33, 24.34, 24.35, 24.36, 24.37, 24.38, 24.39, 24.4, 24.41, 24.42, 24.43, 24.44, 24.45, 24.46, 24.47, 24.48, 24.49, 24.5, 24.51, 24.52, 24.53, 24.54, 24.55, 24.56, 24.57, 24.58, 24.59, 24.6, 24.61, 24.62, 24.63, 24.64, 24.65, 24.66, 24.67, 24.68, 24.69, 24.7, 24.71, 24.72, 24.73, 24.74, 24.75, 24.76, 24.77, 24.78, 24.79, 24.8, 24.81, 24.82, 24.83, 24.84, 24.85, 24.86, 24.87, 24.88, 24.89, 24.9, 24.91, 24.92, 24.93, 24.94, 24.95, 24.96, 24.97, 24.98, 24.99, 25.0, 25.01, 25.02, 25.03, 25.04, 25.05, 25.06, 25.07, 25.08, 25.09, 25.1, 25.11, 25.12, 25.13, 25.14, 25.15, 25.16, 25.17, 25.18, 25.19, 25.2, 25.21, 25.22, 25.23, 25.24, 25.25, 25.26, 25.27, 25.28, 25.29, 25.3, 25.31, 25.32, 25.33, 25.34, 25.35, 25.36, 25.37, 25.38, 25.39, 25.4, 25.41, 25.42, 25.43, 25.44, 25.45, 25.46, 25.47, 25.48, 25.49, 25.5, 25.51, 25.52, 25.53, 25.54, 25.55, 25.56, 25.57, 25.58, 25.59, 25.6, 25.61, 25.62, 25.63, 25.64, 25.65, 25.66, 25.67, 25.68, 25.69, 25.7, 25.71, 25.72, 25.73, 25.74, 25.75, 25.76, 25.77, 25.78, 25.79, 25.8, 25.81, 25.82, 25.83, 25.84, 25.85, 25.86, 25.87, 25.88, 25.89, 25.9, 25.91, 25.92, 25.93, 25.94, 25.95, 25.96, 25.97, 25.98, 25.99], 'resolution': 0.01}\n",
      "dt vocab size: 2603\n",
      "{'tokens': [-1.326, -1.325, -1.324, -1.323, -1.322, -1.321, -1.32, -1.319, -1.318, -1.317, -1.316, -1.315, -1.314, -1.313, -1.312, -1.311, -1.31, -1.309, -1.308, -1.307, -1.306, -1.305, -1.304, -1.303, -1.302, -1.301, -1.3, -1.299, -1.298, -1.297, -1.296, -1.295, -1.294, -1.293, -1.292, -1.291, -1.29, -1.289, -1.288, -1.287, -1.286, -1.285, -1.284, -1.283, -1.282, -1.281, -1.28, -1.279, -1.278, -1.277, -1.276, -1.275, -1.274, -1.273, -1.272, -1.271, -1.27, -1.269, -1.268, -1.267, -1.266, -1.265, -1.264, -1.263, -1.262, -1.261, -1.26, -1.259, -1.258, -1.257, -1.256, -1.255, -1.254, -1.253, -1.252, -1.251, -1.25, -1.249, -1.248, -1.247, -1.246, -1.245, -1.244, -1.243, -1.242, -1.241, -1.24, -1.239, -1.238, -1.237, -1.236, -1.235, -1.234, -1.233, -1.232, -1.231, -1.23, -1.229, -1.228, -1.227, -1.226, -1.225, -1.224, -1.223, -1.222, -1.221, -1.22, -1.219, -1.218, -1.217, -1.216, -1.215, -1.214, -1.213, -1.212, -1.211, -1.21, -1.209, -1.208, -1.207, -1.206, -1.205, -1.204, -1.203, -1.202, -1.201, -1.2, -1.199, -1.198, -1.197, -1.196, -1.195, -1.194, -1.193, -1.192, -1.191, -1.19, -1.189, -1.188, -1.187, -1.186, -1.185, -1.184, -1.183, -1.182, -1.181, -1.18, -1.179, -1.178, -1.177, -1.176, -1.175, -1.174, -1.173, -1.172, -1.171, -1.17, -1.169, -1.168, -1.167, -1.166, -1.165, -1.164, -1.163, -1.162, -1.161, -1.16, -1.159, -1.158, -1.157, -1.156, -1.155, -1.154, -1.153, -1.152, -1.151, -1.15, -1.149, -1.148, -1.147, -1.146, -1.145, -1.144, -1.143, -1.142, -1.141, -1.14, -1.139, -1.138, -1.137, -1.136, -1.135, -1.134, -1.133, -1.132, -1.131, -1.13, -1.129, -1.128, -1.127, -1.126, -1.125, -1.124, -1.123, -1.122, -1.121, -1.12, -1.119, -1.118, -1.117, -1.116, -1.115, -1.114, -1.113, -1.112, -1.111, -1.11, -1.109, -1.108, -1.107, -1.106, -1.105, -1.104, -1.103, -1.102, -1.101, -1.1, -1.099, -1.098, -1.097, -1.096, -1.095, -1.094, -1.093, -1.092, -1.091, -1.09, -1.089, -1.088, -1.087, -1.086, -1.085, -1.084, -1.083, -1.082, -1.081, -1.08, -1.079, -1.078, -1.077, -1.076, -1.075, -1.074, -1.073, -1.072, -1.071, -1.07, -1.069, -1.068, -1.067, -1.066, -1.065, -1.064, -1.063, -1.062, -1.061, -1.06, -1.059, -1.058, -1.057, -1.056, -1.055, -1.054, -1.053, -1.052, -1.051, -1.05, -1.049, -1.048, -1.047, -1.046, -1.045, -1.044, -1.043, -1.042, -1.041, -1.04, -1.039, -1.038, -1.037, -1.036, -1.035, -1.034, -1.033, -1.032, -1.031, -1.03, -1.029, -1.028, -1.027, -1.026, -1.025, -1.024, -1.023, -1.022, -1.021, -1.02, -1.019, -1.018, -1.017, -1.016, -1.015, -1.014, -1.013, -1.012, -1.011, -1.01, -1.009, -1.008, -1.007, -1.006, -1.005, -1.004, -1.003, -1.002, -1.001, -1.0, -0.999, -0.998, -0.997, -0.996, -0.995, -0.994, -0.993, -0.992, -0.991, -0.99, -0.989, -0.988, -0.987, -0.986, -0.985, -0.984, -0.983, -0.982, -0.981, -0.98, -0.979, -0.978, -0.977, -0.976, -0.975, -0.974, -0.973, -0.972, -0.971, -0.97, -0.969, -0.968, -0.967, -0.966, -0.965, -0.964, -0.963, -0.962, -0.961, -0.96, -0.959, -0.958, -0.957, -0.956, -0.955, -0.954, -0.953, -0.952, -0.951, -0.95, -0.949, -0.948, -0.947, -0.946, -0.945, -0.944, -0.943, -0.942, -0.941, -0.94, -0.939, -0.938, -0.937, -0.936, -0.935, -0.934, -0.933, -0.932, -0.931, -0.93, -0.929, -0.928, -0.927, -0.926, -0.925, -0.924, -0.923, -0.922, -0.921, -0.92, -0.919, -0.918, -0.917, -0.916, -0.915, -0.914, -0.913, -0.912, -0.911, -0.91, -0.909, -0.908, -0.907, -0.906, -0.905, -0.904, -0.903, -0.902, -0.901, -0.9, -0.899, -0.898, -0.897, -0.896, -0.895, -0.894, -0.893, -0.892, -0.891, -0.89, -0.889, -0.888, -0.887, -0.886, -0.885, -0.884, -0.883, -0.882, -0.881, -0.88, -0.879, -0.878, -0.877, -0.876, -0.875, -0.874, -0.873, -0.872, -0.871, -0.87, -0.869, -0.868, -0.867, -0.866, -0.865, -0.864, -0.863, -0.862, -0.861, -0.86, -0.859, -0.858, -0.857, -0.856, -0.855, -0.854, -0.853, -0.852, -0.851, -0.85, -0.849, -0.848, -0.847, -0.846, -0.845, -0.844, -0.843, -0.842, -0.841, -0.84, -0.839, -0.838, -0.837, -0.836, -0.835, -0.834, -0.833, -0.832, -0.831, -0.83, -0.829, -0.828, -0.827, -0.826, -0.825, -0.824, -0.823, -0.822, -0.821, -0.82, -0.819, -0.818, -0.817, -0.816, -0.815, -0.814, -0.813, -0.812, -0.811, -0.81, -0.809, -0.808, -0.807, -0.806, -0.805, -0.804, -0.803, -0.802, -0.801, -0.8, -0.799, -0.798, -0.797, -0.796, -0.795, -0.794, -0.793, -0.792, -0.791, -0.79, -0.789, -0.788, -0.787, -0.786, -0.785, -0.784, -0.783, -0.782, -0.781, -0.78, -0.779, -0.778, -0.777, -0.776, -0.775, -0.774, -0.773, -0.772, -0.771, -0.77, -0.769, -0.768, -0.767, -0.766, -0.765, -0.764, -0.763, -0.762, -0.761, -0.76, -0.759, -0.758, -0.757, -0.756, -0.755, -0.754, -0.753, -0.752, -0.751, -0.75, -0.749, -0.748, -0.747, -0.746, -0.745, -0.744, -0.743, -0.742, -0.741, -0.74, -0.739, -0.738, -0.737, -0.736, -0.735, -0.734, -0.733, -0.732, -0.731, -0.73, -0.729, -0.728, -0.727, -0.726, -0.725, -0.724, -0.723, -0.722, -0.721, -0.72, -0.719, -0.718, -0.717, -0.716, -0.715, -0.714, -0.713, -0.712, -0.711, -0.71, -0.709, -0.708, -0.707, -0.706, -0.705, -0.704, -0.703, -0.702, -0.701, -0.7, -0.699, -0.698, -0.697, -0.696, -0.695, -0.694, -0.693, -0.692, -0.691, -0.69, -0.689, -0.688, -0.687, -0.686, -0.685, -0.684, -0.683, -0.682, -0.681, -0.68, -0.679, -0.678, -0.677, -0.676, -0.675, -0.674, -0.673, -0.672, -0.671, -0.67, -0.669, -0.668, -0.667, -0.666, -0.665, -0.664, -0.663, -0.662, -0.661, -0.66, -0.659, -0.658, -0.657, -0.656, -0.655, -0.654, -0.653, -0.652, -0.651, -0.65, -0.649, -0.648, -0.647, -0.646, -0.645, -0.644, -0.643, -0.642, -0.641, -0.64, -0.639, -0.638, -0.637, -0.636, -0.635, -0.634, -0.633, -0.632, -0.631, -0.63, -0.629, -0.628, -0.627, -0.626, -0.625, -0.624, -0.623, -0.622, -0.621, -0.62, -0.619, -0.618, -0.617, -0.616, -0.615, -0.614, -0.613, -0.612, -0.611, -0.61, -0.609, -0.608, -0.607, -0.606, -0.605, -0.604, -0.603, -0.602, -0.601, -0.6, -0.599, -0.598, -0.597, -0.596, -0.595, -0.594, -0.593, -0.592, -0.591, -0.59, -0.589, -0.588, -0.587, -0.586, -0.585, -0.584, -0.583, -0.582, -0.581, -0.58, -0.579, -0.578, -0.577, -0.576, -0.575, -0.574, -0.573, -0.572, -0.571, -0.57, -0.569, -0.568, -0.567, -0.566, -0.565, -0.564, -0.563, -0.562, -0.561, -0.56, -0.559, -0.558, -0.557, -0.556, -0.555, -0.554, -0.553, -0.552, -0.551, -0.55, -0.549, -0.548, -0.547, -0.546, -0.545, -0.544, -0.543, -0.542, -0.541, -0.54, -0.539, -0.538, -0.537, -0.536, -0.535, -0.534, -0.533, -0.532, -0.531, -0.53, -0.529, -0.528, -0.527, -0.526, -0.525, -0.524, -0.523, -0.522, -0.521, -0.519, -0.518, -0.517, -0.516, -0.515, -0.514, -0.513, -0.512, -0.511, -0.51, -0.509, -0.508, -0.507, -0.506, -0.505, -0.504, -0.503, -0.502, -0.501, -0.5, -0.499, -0.498, -0.497, -0.496, -0.495, -0.494, -0.493, -0.492, -0.491, -0.49, -0.489, -0.488, -0.487, -0.486, -0.485, -0.484, -0.483, -0.482, -0.481, -0.48, -0.479, -0.478, -0.477, -0.476, -0.475, -0.474, -0.473, -0.472, -0.471, -0.47, -0.469, -0.468, -0.467, -0.466, -0.465, -0.464, -0.463, -0.462, -0.461, -0.46, -0.459, -0.458, -0.457, -0.456, -0.455, -0.454, -0.453, -0.452, -0.451, -0.45, -0.449, -0.448, -0.447, -0.446, -0.445, -0.444, -0.443, -0.442, -0.441, -0.44, -0.439, -0.438, -0.437, -0.436, -0.435, -0.434, -0.433, -0.432, -0.431, -0.43, -0.429, -0.428, -0.427, -0.426, -0.425, -0.424, -0.423, -0.422, -0.421, -0.42, -0.419, -0.418, -0.417, -0.416, -0.415, -0.414, -0.413, -0.412, -0.411, -0.41, -0.409, -0.408, -0.407, -0.406, -0.405, -0.404, -0.403, -0.402, -0.401, -0.4, -0.399, -0.398, -0.397, -0.396, -0.395, -0.394, -0.393, -0.392, -0.391, -0.39, -0.389, -0.388, -0.387, -0.386, -0.385, -0.384, -0.383, -0.382, -0.381, -0.38, -0.379, -0.378, -0.377, -0.376, -0.375, -0.374, -0.373, -0.372, -0.371, -0.37, -0.369, -0.368, -0.367, -0.366, -0.365, -0.364, -0.363, -0.362, -0.361, -0.36, -0.359, -0.358, -0.357, -0.356, -0.355, -0.354, -0.353, -0.352, -0.351, -0.35, -0.349, -0.348, -0.347, -0.346, -0.345, -0.344, -0.343, -0.342, -0.341, -0.34, -0.339, -0.338, -0.337, -0.336, -0.335, -0.334, -0.333, -0.332, -0.331, -0.33, -0.329, -0.328, -0.327, -0.326, -0.325, -0.324, -0.323, -0.322, -0.321, -0.32, -0.319, -0.318, -0.317, -0.316, -0.315, -0.314, -0.313, -0.312, -0.311, -0.31, -0.309, -0.308, -0.307, -0.306, -0.305, -0.304, -0.303, -0.302, -0.301, -0.3, -0.299, -0.298, -0.297, -0.296, -0.295, -0.294, -0.293, -0.292, -0.291, -0.29, -0.289, -0.288, -0.287, -0.286, -0.285, -0.284, -0.283, -0.282, -0.281, -0.28, -0.279, -0.278, -0.277, -0.276, -0.275, -0.274, -0.273, -0.272, -0.271, -0.27, -0.269, -0.268, -0.267, -0.266, -0.265, -0.264, -0.263, -0.262, -0.261, -0.26, -0.259, -0.258, -0.257, -0.256, -0.255, -0.254, -0.253, -0.252, -0.251, -0.25, -0.249, -0.248, -0.247, -0.246, -0.245, -0.244, -0.243, -0.242, -0.241, -0.24, -0.239, -0.238, -0.237, -0.236, -0.235, -0.234, -0.233, -0.232, -0.231, -0.23, -0.229, -0.228, -0.227, -0.226, -0.225, -0.224, -0.223, -0.222, -0.221, -0.22, -0.219, -0.218, -0.217, -0.216, -0.215, -0.214, -0.213, -0.212, -0.211, -0.21, -0.209, -0.208, -0.207, -0.206, -0.205, -0.204, -0.203, -0.202, -0.201, -0.2, -0.199, -0.198, -0.197, -0.196, -0.195, -0.194, -0.193, -0.192, -0.191, -0.19, -0.189, -0.188, -0.187, -0.186, -0.185, -0.184, -0.183, -0.182, -0.181, -0.18, -0.179, -0.178, -0.177, -0.176, -0.175, -0.174, -0.173, -0.172, -0.171, -0.17, -0.169, -0.168, -0.167, -0.166, -0.165, -0.164, -0.163, -0.162, -0.161, -0.16, -0.159, -0.158, -0.157, -0.156, -0.155, -0.154, -0.153, -0.152, -0.151, -0.15, -0.149, -0.148, -0.147, -0.146, -0.145, -0.144, -0.143, -0.142, -0.141, -0.14, -0.139, -0.138, -0.137, -0.136, -0.135, -0.134, -0.133, -0.132, -0.131, -0.13, -0.129, -0.128, -0.127, -0.126, -0.125, -0.124, -0.123, -0.122, -0.121, -0.12, -0.119, -0.118, -0.117, -0.116, -0.115, -0.114, -0.113, -0.112, -0.111, -0.11, -0.109, -0.108, -0.107, -0.106, -0.105, -0.104, -0.103, -0.102, -0.101, -0.1, -0.099, -0.098, -0.097, -0.096, -0.095, -0.094, -0.093, -0.092, -0.091, -0.09, -0.089, -0.088, -0.087, -0.086, -0.085, -0.084, -0.083, -0.082, -0.081, -0.08, -0.079, -0.078, -0.077, -0.076, -0.075, -0.074, -0.073, -0.072, -0.071, -0.07, -0.069, -0.068, -0.067, -0.066, -0.065, -0.064, -0.063, -0.062, -0.061, -0.06, -0.059, -0.058, -0.057, -0.056, -0.055, -0.054, -0.053, -0.052, -0.051, -0.05, -0.049, -0.048, -0.047, -0.046, -0.045, -0.044, -0.043, -0.042, -0.041, -0.04, -0.039, -0.038, -0.037, -0.036, -0.035, -0.034, -0.033, -0.032, -0.031, -0.03, -0.029, -0.028, -0.027, -0.026, -0.025, -0.024, -0.023, -0.022, -0.021, -0.02, -0.019, -0.018, -0.017, -0.016, -0.015, -0.014, -0.013, -0.012, -0.011, -0.01, -0.009, -0.008, -0.007, -0.006, -0.005, -0.004, -0.003, -0.002, -0.001, -0.0, 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01, 0.011, 0.012, 0.013, 0.014, 0.015, 0.016, 0.017, 0.018, 0.019, 0.02, 0.021, 0.022, 0.023, 0.024, 0.025, 0.026, 0.027, 0.028, 0.029, 0.03, 0.031, 0.032, 0.033, 0.034, 0.035, 0.036, 0.037, 0.038, 0.039, 0.04, 0.041, 0.042, 0.043, 0.044, 0.045, 0.046, 0.047, 0.048, 0.049, 0.05, 0.051, 0.052, 0.053, 0.054, 0.055, 0.056, 0.057, 0.058, 0.059, 0.06, 0.061, 0.062, 0.063, 0.064, 0.065, 0.066, 0.067, 0.068, 0.069, 0.07, 0.071, 0.072, 0.073, 0.074, 0.075, 0.076, 0.077, 0.078, 0.079, 0.08, 0.081, 0.082, 0.083, 0.084, 0.085, 0.086, 0.087, 0.088, 0.089, 0.09, 0.091, 0.092, 0.093, 0.094, 0.095, 0.096, 0.097, 0.098, 0.099, 0.1, 0.101, 0.102, 0.103, 0.104, 0.105, 0.106, 0.107, 0.108, 0.109, 0.11, 0.111, 0.112, 0.113, 0.114, 0.115, 0.116, 0.117, 0.118, 0.119, 0.12, 0.121, 0.122, 0.123, 0.124, 0.125, 0.126, 0.127, 0.128, 0.129, 0.13, 0.131, 0.132, 0.133, 0.134, 0.135, 0.136, 0.137, 0.138, 0.139, 0.14, 0.141, 0.142, 0.143, 0.144, 0.145, 0.146, 0.147, 0.148, 0.149, 0.15, 0.151, 0.152, 0.153, 0.154, 0.155, 0.156, 0.157, 0.158, 0.159, 0.16, 0.161, 0.162, 0.163, 0.164, 0.165, 0.166, 0.167, 0.168, 0.169, 0.17, 0.171, 0.172, 0.173, 0.174, 0.175, 0.176, 0.177, 0.178, 0.179, 0.18, 0.181, 0.182, 0.183, 0.184, 0.185, 0.186, 0.187, 0.188, 0.189, 0.19, 0.191, 0.192, 0.193, 0.194, 0.195, 0.196, 0.197, 0.198, 0.199, 0.2, 0.201, 0.202, 0.203, 0.204, 0.205, 0.206, 0.207, 0.208, 0.209, 0.21, 0.211, 0.212, 0.213, 0.214, 0.215, 0.216, 0.217, 0.218, 0.219, 0.22, 0.221, 0.222, 0.223, 0.224, 0.225, 0.226, 0.227, 0.228, 0.229, 0.23, 0.231, 0.232, 0.233, 0.234, 0.235, 0.236, 0.237, 0.238, 0.239, 0.24, 0.241, 0.242, 0.243, 0.244, 0.245, 0.246, 0.247, 0.248, 0.249, 0.25, 0.251, 0.252, 0.253, 0.254, 0.255, 0.256, 0.257, 0.258, 0.259, 0.26, 0.261, 0.262, 0.263, 0.264, 0.265, 0.266, 0.267, 0.268, 0.269, 0.27, 0.271, 0.272, 0.273, 0.274, 0.275, 0.276, 0.277, 0.278, 0.279, 0.28, 0.281, 0.282, 0.283, 0.284, 0.285, 0.286, 0.287, 0.288, 0.289, 0.29, 0.291, 0.292, 0.293, 0.294, 0.295, 0.296, 0.297, 0.298, 0.299, 0.3, 0.301, 0.302, 0.303, 0.304, 0.305, 0.306, 0.307, 0.308, 0.309, 0.31, 0.311, 0.312, 0.313, 0.314, 0.315, 0.316, 0.317, 0.318, 0.319, 0.32, 0.321, 0.322, 0.323, 0.324, 0.325, 0.326, 0.327, 0.328, 0.329, 0.33, 0.331, 0.332, 0.333, 0.334, 0.335, 0.336, 0.337, 0.338, 0.339, 0.34, 0.341, 0.342, 0.343, 0.344, 0.345, 0.346, 0.347, 0.348, 0.349, 0.35, 0.351, 0.352, 0.353, 0.354, 0.355, 0.356, 0.357, 0.358, 0.359, 0.36, 0.361, 0.362, 0.363, 0.364, 0.365, 0.366, 0.367, 0.368, 0.369, 0.37, 0.371, 0.372, 0.373, 0.374, 0.375, 0.376, 0.377, 0.378, 0.379, 0.38, 0.381, 0.382, 0.383, 0.384, 0.385, 0.386, 0.387, 0.388, 0.389, 0.39, 0.391, 0.392, 0.393, 0.394, 0.395, 0.396, 0.397, 0.398, 0.399, 0.4, 0.401, 0.402, 0.403, 0.404, 0.405, 0.406, 0.407, 0.408, 0.409, 0.41, 0.411, 0.412, 0.413, 0.414, 0.415, 0.416, 0.417, 0.418, 0.419, 0.42, 0.421, 0.422, 0.423, 0.424, 0.425, 0.426, 0.427, 0.428, 0.429, 0.43, 0.431, 0.432, 0.433, 0.434, 0.435, 0.436, 0.437, 0.438, 0.439, 0.44, 0.441, 0.442, 0.443, 0.444, 0.445, 0.446, 0.447, 0.448, 0.449, 0.45, 0.451, 0.452, 0.453, 0.454, 0.455, 0.456, 0.457, 0.458, 0.459, 0.46, 0.461, 0.462, 0.463, 0.464, 0.465, 0.466, 0.467, 0.468, 0.469, 0.47, 0.471, 0.472, 0.473, 0.474, 0.475, 0.476, 0.477, 0.478, 0.479, 0.48, 0.481, 0.482, 0.483, 0.484, 0.485, 0.486, 0.487, 0.488, 0.489, 0.49, 0.491, 0.492, 0.493, 0.494, 0.495, 0.496, 0.497, 0.498, 0.499, 0.5, 0.501, 0.502, 0.503, 0.504, 0.505, 0.506, 0.507, 0.508, 0.509, 0.51, 0.511, 0.512, 0.513, 0.514, 0.515, 0.516, 0.517, 0.518, 0.519, 0.52, 0.521, 0.522, 0.523, 0.524, 0.525, 0.526, 0.527, 0.528, 0.529, 0.53, 0.531, 0.532, 0.533, 0.534, 0.535, 0.536, 0.537, 0.538, 0.539, 0.54, 0.541, 0.542, 0.543, 0.544, 0.545, 0.546, 0.547, 0.548, 0.549, 0.55, 0.551, 0.552, 0.553, 0.554, 0.555, 0.556, 0.557, 0.558, 0.559, 0.56, 0.561, 0.562, 0.563, 0.564, 0.565, 0.566, 0.567, 0.568, 0.569, 0.57, 0.571, 0.572, 0.573, 0.574, 0.575, 0.576, 0.577, 0.578, 0.579, 0.58, 0.581, 0.582, 0.583, 0.584, 0.585, 0.586, 0.587, 0.588, 0.589, 0.59, 0.591, 0.592, 0.593, 0.594, 0.595, 0.596, 0.597, 0.598, 0.599, 0.6, 0.601, 0.602, 0.603, 0.604, 0.605, 0.606, 0.607, 0.608, 0.609, 0.61, 0.611, 0.612, 0.613, 0.614, 0.615, 0.616, 0.617, 0.618, 0.619, 0.62, 0.621, 0.622, 0.623, 0.624, 0.625, 0.626, 0.627, 0.628, 0.629, 0.63, 0.631, 0.632, 0.633, 0.634, 0.635, 0.636, 0.637, 0.638, 0.639, 0.64, 0.641, 0.642, 0.643, 0.644, 0.645, 0.646, 0.647, 0.648, 0.649, 0.65, 0.651, 0.652, 0.653, 0.654, 0.655, 0.656, 0.657, 0.658, 0.659, 0.66, 0.661, 0.662, 0.663, 0.664, 0.665, 0.666, 0.667, 0.668, 0.669, 0.67, 0.671, 0.672, 0.673, 0.674, 0.675, 0.676, 0.677, 0.678, 0.679, 0.68, 0.681, 0.682, 0.683, 0.684, 0.685, 0.686, 0.687, 0.688, 0.689, 0.69, 0.691, 0.692, 0.693, 0.694, 0.695, 0.696, 0.697, 0.698, 0.699, 0.7, 0.701, 0.702, 0.703, 0.704, 0.705, 0.706, 0.707, 0.708, 0.709, 0.71, 0.711, 0.712, 0.713, 0.714, 0.715, 0.716, 0.717, 0.718, 0.719, 0.72, 0.721, 0.722, 0.723, 0.724, 0.725, 0.726, 0.727, 0.728, 0.729, 0.73, 0.731, 0.732, 0.733, 0.734, 0.735, 0.736, 0.737, 0.738, 0.739, 0.74, 0.741, 0.742, 0.743, 0.744, 0.745, 0.746, 0.747, 0.748, 0.749, 0.75, 0.751, 0.752, 0.753, 0.754, 0.755, 0.756, 0.757, 0.758, 0.759, 0.76, 0.761, 0.762, 0.763, 0.764, 0.765, 0.766, 0.767, 0.768, 0.769, 0.77, 0.771, 0.772, 0.773, 0.774, 0.775, 0.776, 0.777, 0.778, 0.779, 0.78, 0.781, 0.782, 0.783, 0.784, 0.785, 0.786, 0.787, 0.788, 0.789, 0.79, 0.791, 0.792, 0.793, 0.794, 0.795, 0.796, 0.797, 0.798, 0.799, 0.8, 0.801, 0.802, 0.803, 0.804, 0.805, 0.806, 0.807, 0.808, 0.809, 0.81, 0.811, 0.812, 0.813, 0.814, 0.815, 0.816, 0.817, 0.818, 0.819, 0.82, 0.821, 0.822, 0.823, 0.824, 0.825, 0.826, 0.827, 0.828, 0.829, 0.83, 0.831, 0.832, 0.833, 0.834, 0.835, 0.836, 0.837, 0.838, 0.839, 0.84, 0.841, 0.842, 0.843, 0.844, 0.845, 0.846, 0.847, 0.848, 0.849, 0.85, 0.851, 0.852, 0.853, 0.854, 0.855, 0.856, 0.857, 0.858, 0.859, 0.86, 0.861, 0.862, 0.863, 0.864, 0.865, 0.866, 0.867, 0.868, 0.869, 0.87, 0.871, 0.872, 0.873, 0.874, 0.875, 0.876, 0.877, 0.878, 0.879, 0.88, 0.881, 0.882, 0.883, 0.884, 0.885, 0.886, 0.887, 0.888, 0.889, 0.89, 0.891, 0.892, 0.893, 0.894, 0.895, 0.896, 0.897, 0.898, 0.899, 0.9, 0.901, 0.902, 0.903, 0.904, 0.905, 0.906, 0.907, 0.908, 0.909, 0.91, 0.911, 0.912, 0.913, 0.914, 0.915, 0.916, 0.917, 0.918, 0.919, 0.92, 0.921, 0.922, 0.923, 0.924, 0.925, 0.926, 0.927, 0.928, 0.929, 0.93, 0.931, 0.932, 0.933, 0.934, 0.935, 0.936, 0.937, 0.938, 0.939, 0.94, 0.941, 0.942, 0.943, 0.944, 0.945, 0.946, 0.947, 0.948, 0.949, 0.95, 0.951, 0.952, 0.953, 0.954, 0.955, 0.956, 0.957, 0.958, 0.959, 0.96, 0.961, 0.962, 0.963, 0.964, 0.965, 0.966, 0.967, 0.968, 0.969, 0.97, 0.971, 0.972, 0.973, 0.974, 0.975, 0.976, 0.977, 0.978, 0.979, 0.98, 0.981, 0.982, 0.983, 0.984, 0.985, 0.986, 0.987, 0.988, 0.989, 0.99, 0.991, 0.992, 0.993, 0.994, 0.995, 0.996, 0.997, 0.998, 0.999, 1.0, 1.001, 1.002, 1.003, 1.004, 1.005, 1.006, 1.007, 1.008, 1.009, 1.01, 1.011, 1.012, 1.013, 1.014, 1.015, 1.016, 1.017, 1.018, 1.019, 1.02, 1.021, 1.022, 1.023, 1.024, 1.025, 1.026, 1.027, 1.028, 1.029, 1.03, 1.031, 1.032, 1.033, 1.034, 1.035, 1.036, 1.037, 1.038, 1.039, 1.04, 1.041, 1.042, 1.043, 1.044, 1.045, 1.046, 1.047, 1.048, 1.049, 1.05, 1.051, 1.052, 1.053, 1.054, 1.055, 1.056, 1.057, 1.058, 1.059, 1.06, 1.061, 1.062, 1.063, 1.064, 1.065, 1.066, 1.067, 1.068, 1.069, 1.07, 1.071, 1.072, 1.073, 1.074, 1.075, 1.076, 1.077, 1.078, 1.079, 1.08, 1.081, 1.082, 1.083, 1.084, 1.085, 1.086, 1.087, 1.088, 1.089, 1.09, 1.091, 1.092, 1.093, 1.094, 1.095, 1.096, 1.097, 1.098, 1.099, 1.1, 1.101, 1.102, 1.103, 1.104, 1.105, 1.106, 1.107, 1.108, 1.109, 1.11, 1.111, 1.112, 1.113, 1.114, 1.115, 1.116, 1.117, 1.118, 1.119, 1.12, 1.121, 1.122, 1.123, 1.124, 1.125, 1.126, 1.127, 1.128, 1.129, 1.13, 1.131, 1.132, 1.133, 1.134, 1.135, 1.136, 1.137, 1.138, 1.139, 1.14, 1.141, 1.142, 1.143, 1.144, 1.145, 1.146, 1.147, 1.148, 1.149, 1.15, 1.151, 1.152, 1.153, 1.154, 1.155, 1.156, 1.157, 1.158, 1.159, 1.16, 1.161, 1.162, 1.163, 1.164, 1.165, 1.166, 1.167, 1.168, 1.169, 1.17, 1.171, 1.172, 1.173, 1.174, 1.175, 1.176, 1.177, 1.178, 1.179, 1.18, 1.181, 1.182, 1.183, 1.184, 1.185, 1.186, 1.187, 1.188, 1.189, 1.19, 1.191, 1.192, 1.193, 1.194, 1.195, 1.196, 1.197, 1.198, 1.199, 1.2, 1.201, 1.202, 1.203, 1.204, 1.205, 1.206, 1.207, 1.208, 1.209, 1.21, 1.211, 1.212, 1.213, 1.214, 1.215, 1.216, 1.217, 1.218, 1.219, 1.22, 1.221, 1.222, 1.223, 1.224, 1.225, 1.226, 1.227, 1.228, 1.229, 1.23, 1.231, 1.232, 1.233, 1.234, 1.235, 1.236, 1.237, 1.238, 1.239, 1.24, 1.241, 1.242, 1.243, 1.244, 1.245, 1.246, 1.247, 1.248, 1.249, 1.25, 1.251, 1.252, 1.253, 1.254, 1.255, 1.256, 1.257, 1.258, 1.259, 1.26, 1.261, 1.262, 1.263, 1.264, 1.265, 1.266, 1.267, 1.268, 1.269, 1.27, 1.271, 1.272, 1.273, 1.274, 1.275, 1.276, 1.277, 1.278, 1.279, 1.28, 1.281, 1.282, 1.283, 1.284, 1.285, 1.286, 1.287, 1.288, 1.289, 1.29, 1.291, 1.292, 1.293, 1.294, 1.295, 1.296, 1.297, 1.298, 1.299, 1.3, 1.301, 1.302, 1.303, 1.304, 1.305, 1.306, 1.307, 1.308, 1.309, 1.31, 1.311, 1.312, 1.313, 1.314, 1.315, 1.316, 1.317, 1.318, 1.319, 1.32, 1.321, 1.322, 1.323, 1.324, 1.325, 1.326, 1.327, 1.328, 1.329, 1.33, 1.331, 1.332, 1.333, 1.334, 1.335, 1.336, 1.337, 1.338, 1.339, 1.34, 1.341, 1.342, 1.343, 1.344, 1.345, 1.346, 1.347, 1.348, 1.349, 1.35, 1.351, 1.352, 1.353, 1.354, 1.355, 1.356, 1.357, 1.358, 1.359, 1.36, 1.361, 1.362, 1.363, 1.364, 1.365, 1.366, 1.367, 1.368, 1.369, 1.37, 1.371, 1.372, 1.373, 1.374, 1.375, 1.376, 1.377, 1.378, 1.379, 1.38, 1.381, 1.382, 1.383, 1.384, 1.385, 1.386, 1.387, 1.388, 1.389, 1.39, 1.391, 1.392, 1.393, 1.394, 1.395, 1.396, 1.397, 1.398, 1.399, 1.4, 1.401, 1.402, 1.403, 1.404, 1.405, 1.406, 1.407, 1.408, 1.409, 1.41, 1.412, 1.413, 1.414, 1.415, 1.416, 1.417, 1.418, 1.419, 1.42, 1.421, 1.422, 1.423, 1.424, 1.425, 1.426, 1.427, 1.428, 1.429, 1.43, 1.431, 1.432, 1.433, 1.434, 1.435, 1.436, 1.437, 1.438, 1.439, 1.44, 1.441, 1.442, 1.443, 1.444, 1.445, 1.446, 1.447, 1.448, 1.449, 1.45, 1.451, 1.452, 1.453, 1.454, 1.455, 1.456, 1.457, 1.458, 1.459, 1.46, 1.461, 1.462, 1.463, 1.464, 1.465, 1.466, 1.467, 1.468, 1.469, 1.47, 1.471, 1.472, 1.473, 1.474, 1.475, 1.476, 1.477, 1.478, 1.479, 1.48, 1.481, 1.482, 1.483, 1.484, 1.485, 1.486, 1.487, 1.488, 1.489, 1.49, 1.491, 1.492, 1.493, 1.494, 1.495, 1.496, 1.497, 1.498, 1.499, 1.5, 1.501, 1.502, 1.503, 1.504, 1.505, 1.506, 1.507, 1.508, 1.509, 1.51, 1.511, 1.512, 1.513, 1.514, 1.515, 1.516, 1.517, 1.518, 1.519, 1.52, 1.521, 1.522, 1.523, 1.524, 1.525, 1.526, 1.527, 1.528, 1.529, 1.53, 1.531, 1.532, 1.533, 1.534, 1.535, 1.536, 1.537, 1.538, 1.539, 1.54, 1.541, 1.542, 1.543, 1.544, 1.545, 1.546, 1.547, 1.548, 1.549, 1.55, 1.551, 1.552, 1.553, 1.554, 1.555, 1.556, 1.557, 1.558, 1.559, 1.56, 1.561, 1.562, 1.563, 1.564, 1.565, 1.566, 1.567, 1.568, 1.569, 1.57, 1.571, 1.572, 1.573, 1.574, 1.575, 1.576, 1.577, 1.578, 1.579, 1.58, 1.581, 1.582, 1.583, 1.584, 1.585, 1.586, 1.587, 1.588, 1.589, 1.59, 1.591, 1.592, 1.593, 1.594, 1.595, 1.596, 1.597, 1.598, 1.599, 1.6, 1.601, 1.602, 1.603, 1.604, 1.605, 1.606, 1.607, 1.608, 1.609, 1.61, 1.611, 1.612, 1.613, 1.614, 1.615, 1.616, 1.617, 1.618, 1.619, 1.62, 1.621, 1.622, 1.623, 1.624, 1.625, 1.626, 1.627, 1.628, 1.629, 1.63, 1.631, 1.632, 1.633, 1.634, 1.635, 1.636, 1.637, 1.638, 1.639, 1.64, 1.641, 1.642, 1.643, 1.644, 1.645, 1.646, 1.647, 1.648, 1.649, 1.65, 1.651, 1.652, 1.653, 1.654, 1.655, 1.656, 1.657, 1.658, 1.659, 1.66, 1.661, 1.662, 1.663, 1.664, 1.665, 1.666, 1.667, 1.668, 1.669, 1.67, 1.671, 1.672, 1.673, 1.674, 1.675, 1.676, 1.677, 1.678, 1.679, 1.68, 1.681, 1.682, 1.683, 1.684, 1.685, 1.686, 1.687, 1.688, 1.689, 1.69, 1.691, 1.692, 1.693, 1.694, 1.695, 1.696, 1.697, 1.698, 1.699, 1.7, 1.701, 1.702, 1.703, 1.704, 1.705, 1.706, 1.707, 1.708, 1.709, 1.71, 1.711, 1.712, 1.713, 1.714, 1.715, 1.716, 1.717, 1.718, 1.719, 1.72, 1.721, 1.722, 1.723, 1.724, 1.725, 1.726, 1.727, 1.728, 1.729, 1.73, 1.731, 1.732, 1.733, 1.734, 1.735, 1.736, 1.737, 1.738, 1.739, 1.74, 1.741, 1.742, 1.743, 1.744, 1.745, 1.746, 1.747, 1.748, 1.749, 1.75, 1.751, 1.752, 1.753, 1.754, 1.755, 1.756, 1.757, 1.758, 1.759, 1.76, 1.761, 1.762, 1.763, 1.764, 1.765, 1.766, 1.767, 1.768, 1.769, 1.77, 1.771, 1.772, 1.773, 1.774, 1.775, 1.776, 1.777, 1.778, 1.78, 1.781, 1.782, 1.783, 1.784, 1.785, 1.786, 1.787, 1.788, 1.789, 1.79, 1.791, 1.792, 1.793, 1.795, 1.796, 1.798, 1.799, 1.8, 1.801, 1.802, 1.803, 1.804, 1.806, 1.807, 1.808, 1.809, 1.81, 1.811, 1.812, 1.813, 1.814, 1.815, 1.816, 1.817, 1.819, 1.82, 1.821, 1.822, 1.823, 1.824, 1.825, 1.826, 1.827, 1.829, 1.83, 1.831, 1.832, 1.833, 1.834, 1.835, 1.836, 1.837, 1.84, 1.841, 1.843, 1.844, 1.845, 1.846, 1.847, 1.848, 1.849, 1.85, 1.851, 1.852, 1.853, 1.854, 1.855, 1.857, 1.858, 1.86, 1.861, 1.862, 1.863, 1.865, 1.866, 1.867, 1.868, 1.872, 1.875, 1.876, 1.879, 1.885, 1.886, 1.887, 1.889, 1.891, 1.892, 1.894, 1.895, 1.896, 1.897, 1.898, 1.902, 1.905, 1.906, 1.908, 1.91, 1.914, 1.916, 1.917, 1.92, 1.928, 1.933, 1.935, 1.937, 1.938, 1.943, 1.944, 1.949, 1.954, 1.958, 1.959, 1.961, 1.965, 1.966, 1.969, 1.982, 1.983, 1.984, 1.985, 1.986, 1.987, 1.988, 1.992, 1.997, 2.003, 2.029, 2.03, 2.055, 2.07, 2.101, 2.11, 2.116, 2.224, 2.325, 2.336, 2.358], 'resolution': 0.2}\n",
      "speed vocab size: 23\n"
     ]
    }
   ],
   "source": [
    "## resnet3d feats\n",
    "from neuroformer.DataUtils import split_data_by_interval\n",
    "\n",
    "intervals = np.arange(0, spikes.shape[1] * config.resolution.dt, config.window.curr)\n",
    "train_intervals, test_intervals, finetune_intervals = split_data_by_interval(intervals, r_split=0.8, r_split_ft=0.01)\n",
    "# np.save(f\"/share/edc/home/antonis/neuroformer/_rebuttal/behavior/regression/{DATASET}/intervals_speed.npy\")\n",
    "\n",
    "from neuroformer.DataUtils import Tokenizer\n",
    "# make sure intervals = same size as distance\n",
    "\n",
    "min_shape = min(intervals.shape[0], speed.shape[0]) \n",
    "intervals = intervals[:min_shape]\n",
    "distance = speed[:min_shape]\n",
    "\n",
    "print(f\"intervals.shape: {intervals.shape}\")\n",
    "print(f\"distance.shape: {distance.shape}\")\n",
    "\n",
    "# -------- #\n",
    "\n",
    "\n",
    "spikes_dict = {\n",
    "    \"ID\": spikes,\n",
    "    \"Frames\": stimulus,\n",
    "    \"Interval\": intervals,\n",
    "    \"dt\": config.resolution.dt,\n",
    "    \"id_block_size\": config.block_size.id,\n",
    "    \"prev_id_block_size\": config.block_size.prev_id,\n",
    "    \"frame_block_size\": config.block_size.frame,\n",
    "    \"window\": config.window.curr,\n",
    "    \"window_prev\": config.window.prev,\n",
    "    \"frame_window\": config.window.frame,\n",
    "    \n",
    "}\n",
    "\n",
    "\"\"\" structure:\n",
    "{\n",
    "    type_of_modality:\n",
    "        {name of modality: {'data':data, 'dt': dt, 'predict': True/False},\n",
    "        ...\n",
    "        }\n",
    "    ...\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def visnav_callback(frames, frame_idx, n_frames):\n",
    "    if isinstance(frames, np.ndarray):\n",
    "        frames = torch.from_numpy(frames)\n",
    "    f_idx_0 = max(0, frame_idx - n_frames)\n",
    "    f_idx_1 = f_idx_0 + n_frames\n",
    "    chosen_frames = frames[f_idx_0:f_idx_1].type(torch.float32).unsqueeze(0)\n",
    "    return chosen_frames\n",
    "\n",
    "\n",
    "frames = {'feats': stimulus, 'callback': visnav_callback}\n",
    "modalities = {\n",
    "    'all': \n",
    "            {'speed': \n",
    "                {'data': speed, 'dt': config.window.speed, 'predict': True, 'objective': config.predict.speed.objective}\n",
    "            },\n",
    "}\n",
    "\n",
    "\n",
    "max_window = max(config.window.curr, config.window.prev)\n",
    "dt_range = math.ceil(max_window / dt) + 1\n",
    "n_dt = [round_n(x, dt) for x in np.arange(0, dt_range, dt)]\n",
    "\n",
    "token_types = {\n",
    "    'ID': {'tokens': list(np.arange(0, spikes.shape[0]))},\n",
    "    'dt': {'tokens': n_dt, 'resolution': dt},\n",
    "    'speed': {'tokens': sorted(list(set(speed))), 'resolution': 0.2},\n",
    "}\n",
    "\n",
    "tokenizer = Tokenizer(token_types, max_window, dt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed {'data': array([ 0.032, -0.087, -0.366, ..., -0.657, -0.796, -0.98 ], dtype=float32), 'dt': 0.05, 'predict': True, 'objective': 'regression'}\n"
     ]
    }
   ],
   "source": [
    "for modality_type, modality in modalities.items():\n",
    "    for variable_type, variable in modality.items():\n",
    "        print(variable_type, variable)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%\n",
    "# var_group = 'Interval'\n",
    "# int_trials = df.groupby([var_group, 'Trial']).size()\n",
    "# print(int_trials.mean())\n",
    "# # df.groupby(['Interval', 'Trial']).agg(['nunique'])\n",
    "# n_unique = len(df.groupby([var_group, 'Trial']).size())\n",
    "# df.groupby([var_group, 'Trial']).size().nlargest(int(0.1 * n_unique))\n",
    "# # df.groupby(['Interval_2', 'Trial']).size().mean()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Interval: 0.3\n",
      "Intervals:  24092\n",
      "Window:  0.05\n",
      "Window Prev:  0.25\n",
      "Population Size:  2026\n",
      "ID Population Size:  2026\n",
      "DT Population Size:  2603\n",
      "Using explicitly passed intervals\n",
      "Min Interval: 0.3\n",
      "Intervals:  13484\n",
      "Window:  0.05\n",
      "Window Prev:  0.25\n",
      "Population Size:  2026\n",
      "ID Population Size:  2026\n",
      "DT Population Size:  2603\n",
      "Using explicitly passed intervals\n",
      "Min Interval: 0.3\n",
      "Intervals:  240\n",
      "Window:  0.05\n",
      "Window Prev:  0.25\n",
      "Population Size:  2026\n",
      "ID Population Size:  2026\n",
      "DT Population Size:  2603\n",
      "Using explicitly passed intervals\n"
     ]
    }
   ],
   "source": [
    "from neuroformer.DataUtils import NFDataloader\n",
    "\n",
    "train_dataset = NFDataloader(spikes_dict, tokenizer, config, dataset=DATASET, \n",
    "                             frames=frames, intervals=train_intervals, modalities=modalities)\n",
    "test_dataset = NFDataloader(spikes_dict, tokenizer, config, dataset=DATASET, \n",
    "                            frames=frames, intervals=test_intervals, modalities=modalities)\n",
    "finetune_dataset = NFDataloader(spikes_dict, tokenizer, config, dataset=DATASET, \n",
    "                                frames=frames, intervals=finetune_intervals, modalities=modalities)\n",
    "\n",
    "    \n",
    "# print(f'train: {len(train_dataset)}, test: {len(test_dataset)}')\n",
    "iterable = iter(train_dataset)\n",
    "x, y = next(iterable)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroformer.utils_2 import update_config, dict_to_object, object_to_dict\n",
    "\n",
    "# update config\n",
    "updated_config = update_config(config, modalities, tokenizer, x, y, 2)\n",
    "updated_dict_object = dict_to_object(updated_config)\n",
    "config = updated_dict_object\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/09/2023 11:32:13 - INFO - neuroformer.model_neuroformer_2 -   number of parameters: 2.205133e+07\n",
      "08/09/2023 11:32:13 - INFO - neuroformer.model_neuroformer_2 -   number of parameters: 2.205133e+07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 2603\n"
     ]
    }
   ],
   "source": [
    "from neuroformer.model_neuroformer_2 import GPT, GPTConfig\n",
    "\n",
    "config.id_vocab_size = tokenizer.ID_vocab_size\n",
    "model = GPT(config, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id torch.Size([32, 100]) torch.int64\n",
      "dt torch.Size([32, 100]) torch.int64\n",
      "modalities_speed_value torch.Size([32, 1]) torch.float32\n",
      "modalities_speed_dt torch.Size([32]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "iterable = iter(loader)\n",
    "x, y = next(iterable)\n",
    "# x = all_device(x, 'cuda')\n",
    "# y = all_device(y, 'cuda')\n",
    "recursive_print(y)\n",
    "preds, features, loss = model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroformer.utils_2 import predict_modality\n",
    "\n",
    "MAX_EPOCHS = 500\n",
    "BATCH_SIZE = 32 * 6\n",
    "SHUFFLE = True\n",
    "\n",
    "if config.gru_only:\n",
    "    model_name = \"GRU\"\n",
    "elif config.mlp_only:\n",
    "    model_name = \"MLP\"\n",
    "elif hasattr(config, \"gru2_only\") and config.gru2_only:\n",
    "    # if config.predict.speed.objective == \"regression\":\n",
    "    model_name = \"GRU_2.0\"\n",
    "else:\n",
    "    model_name = \"Neuroformer\"\n",
    "\n",
    "CKPT_PATH = f\"/share/edc/home/antonis/neuroformer/models/NF.15/Visnav_VR_Expt/{DATASET}/{model_name}/speed_{modalities['all']['speed']['objective']}/{str(config.layers)}/{SEED}\"\n",
    "\n",
    "from neuroformer.trainer import TrainerConfig, Trainer\n",
    "\n",
    "tconf = TrainerConfig(max_epochs=MAX_EPOCHS, batch_size=BATCH_SIZE, learning_rate=7e-5, \n",
    "                    num_workers=16, lr_decay=True, patience=3, warmup_tokens=8e7, \n",
    "                    decay_weights=True, weight_decay=1.0, shuffle=SHUFFLE,\n",
    "                    final_tokens=len(train_dataset)*(config.block_size.id) * (MAX_EPOCHS),\n",
    "                    clip_norm=1.0, grad_norm_clip=1.0,\n",
    "                    show_grads=False,\n",
    "                    ckpt_path=CKPT_PATH, no_pbar=False, \n",
    "                    dist=DIST, save_every=0, eval_every=5, min_eval_epoch=50)\n",
    "\n",
    "trainer = Trainer(model, train_dataset, test_dataset, tconf, config)\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "if model_name == \"Neuroformer\":\n",
    "    CKPT_PATH = f\"/share/edc/home/antonis/neuroformer/models/NF.15/Visnav_VR_Expt/{DATASET}/speed_regression/69/\"\n",
    "elif hasattr(config, \"gru2_only\") and config.gru2_only:\n",
    "    if config.predict.speed.objective == \"classification\":\n",
    "        model_name = \"GRU_2.0_cls\"\n",
    "# CKPT_PATH = \"./models/NF.15/Visnav_VR_Expt/lateral/speed_regression/69/\"\n",
    "model.load_state_dict(torch.load(os.path.join(CKPT_PATH, \"_epoch_speed.pt\"), map_location=torch.device('cpu')))\n",
    "tokenizer = pickle.load(open(os.path.join(CKPT_PATH, \"tokenizer.pkl\"), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = next(iterable)\n",
    "# recursive_print(y)\n",
    "# preds, features, loss = model(x, y)\n",
    "\n",
    "# print(\"Loss: \", loss['speed'])\n",
    "# print(\"Preds: \", preds['speed'])\n",
    "# print(\"True: \", y['modalities']['speed']['value'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'medial':\n",
    "    interval_path = \"/share/edc/home/antonis/neuroformer/_rebuttal/behavior/regression/medial/medial_intervals_speed.npy\"\n",
    "    interval_df = np.load(interval_path, allow_pickle=True)\n",
    "if DATASET == 'lateral':\n",
    "    interval_path = \"/share/edc/home/antonis/neuroformer/_rebuttal/behavior/regression/lateral/lateral_intervals_speed.npy\"\n",
    "    interval_df = np.load(interval_path, allow_pickle=True)\n",
    "\n",
    "# n = 7\n",
    "# length = 300\n",
    "\n",
    "# interval_df = interval_df[n*length:(n+1)*length]\n",
    "\n",
    "behavior_dataset = NFDataloader(spikes_dict, tokenizer, config, dataset=DATASET, \n",
    "                                frames=frames, intervals=interval_df, modalities=modalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroformer.utils_2 import predict_modality\n",
    "\n",
    "\n",
    "modality = 'speed'\n",
    "objective = modalities['all'][modality]['objective']\n",
    "behavior_preds = predict_modality(model, behavior_dataset, modality=modality, \n",
    "                                  block_type='modalities', objective=objective)\n",
    "\n",
    "# # save predictions\n",
    "behavior_preds.rename(columns={'modalities_speed_value': 'behavior',\n",
    "                               'interval': 'Interval'}, inplace=True)\n",
    "behavior_preds.to_csv(f\"{CKPT_PATH}/behavior_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from neuroformer.visualize import set_plot_params\n",
    "from neuroformer.visualize import set_research_params\n",
    "\n",
    "# set_research_params()\n",
    "\n",
    "save_path = f\"./_rebuttal/behavior/regression/{DATASET}/{model_name}\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "print(f\"save_path: {save_path}\")\n",
    "behavior_preds.to_csv(os.path.join(save_path, 'behavior_preds.csv'), index=False)\n",
    "\n",
    "x_true, y_true = behavior_preds['cum_interval'], behavior_preds['true']\n",
    "x_pred, y_pred = behavior_preds['cum_interval'], behavior_preds['behavior']\n",
    "\n",
    "# pearson r\n",
    "r, p = pearsonr([float(y) for y in y_pred], [float(y) for y in y_true])\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(y_true, y_pred, s=100, c='k', alpha=0.5)\n",
    "\n",
    "# get the current axis limits after plotting your data\n",
    "xlims = ax.get_xlim()\n",
    "ylims = ax.get_ylim()\n",
    "s_f = 0.8\n",
    "# the line of perfect prediction should span the minimum to the maximum of the current x and y limits\n",
    "combined_limits = [min(xlims[0], ylims[0]) * s_f, max(xlims[1], ylims[1]) * s_f]\n",
    "ax.plot(combined_limits, combined_limits, 'k--', color='red')\n",
    "\n",
    "ax.set_xlabel('True speed', fontsize=20)\n",
    "ax.set_ylabel('Predicted speed', fontsize=20)\n",
    "ax.set_title(f'{model_name}, Regression', fontsize=20)\n",
    "# add pearson r to figure\n",
    "ax.text(0.05, 0.9, 'r = {:.2f}'.format(r), fontsize=20, transform=ax.transAxes)\n",
    "# add p to figure\n",
    "ax.text(0.05, 0.8, 'p < 0.001'.format(p), fontsize=20, transform=ax.transAxes)\n",
    "\n",
    "# axis limits = [-1.5, 1.5]\n",
    "# ax.set_xlim(axis_limits)\n",
    "# ax.set_ylim(axis_limits)\n",
    "plt.savefig(os.path.join(save_path, 'regression_2.pdf'), dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(2.5, 2.5))\n",
    "ax.scatter(y_true, y_pred, c='k', alpha=0.5)\n",
    "\n",
    "# get the current axis limits after plotting your data\n",
    "xlims = ax.get_xlim()\n",
    "ylims = ax.get_ylim()\n",
    "s_f = 0.8\n",
    "# the line of perfect prediction should span the minimum to the maximum of the current x and y limits\n",
    "combined_limits = [min(xlims[0], ylims[0]) * s_f, max(xlims[1], ylims[1]) * s_f]\n",
    "ax.plot(combined_limits, combined_limits, 'k--', color='red')\n",
    "\n",
    "ax.set_xlabel('True speed',)\n",
    "ax.set_ylabel('Predicted speed',)\n",
    "ax.set_title(f'{model_name}, Regression',)\n",
    "# add pearson r to figure\n",
    "ax.text(0.05, 0.9, 'r = {:.2f}'.format(r), transform=ax.transAxes)\n",
    "# add p to figure\n",
    "ax.text(0.05, 0.8, 'p < 0.001'.format(p), transform=ax.transAxes)\n",
    "\n",
    "# axis limits = [-1.5, 1.5]\n",
    "# ax.set_xlim(axis_limits)\n",
    "# ax.set_ylim(axis_limits)\n",
    "plt.savefig(os.path.join(save_path, 'regression_2.pdf'), dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 2.5))\n",
    "x = np.arange(len(behavior_preds))\n",
    "plt.title(f'Speed Predictions, {model_name} Regression vs. True')\n",
    "plt.plot(x, y_true, c='r', label='True')\n",
    "plt.plot(x, y_pred, c='b', label='Regression')\n",
    "plt.xlabel('Time (0.05s)')\n",
    "plt.ylabel('Speed (z-scored)')\n",
    "plt.legend(loc='upper left', framealpha=0.9)\n",
    "plt.savefig(os.path.join(save_path, 'speed_preds.pdf'), bbox_inches='tight')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = np.arange(len(behavior_preds))\n",
    "plt.title(f'Speed Predictions, {model_name} Regression vs. True')\n",
    "plt.plot(x, y_true, c='r', label='True')\n",
    "plt.plot(x, y_pred, c='b', label='Regression')\n",
    "plt.xlabel('Time (0.05s)')\n",
    "plt.ylabel('Speed (z-scored)')\n",
    "plt.legend(loc='upper left', framealpha=0.9)\n",
    "plt.savefig(os.path.join(save_path, 'speed_preds_2.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
