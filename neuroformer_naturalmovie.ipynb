{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import collections\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "import glob\n",
    "from pathlib import Path, PurePath\n",
    "path = Path.cwd()\n",
    "parent_path = path.parents[1]\n",
    "sys.path.append(str(PurePath(parent_path, 'neuroformer')))\n",
    "sys.path.append('neuroformer')\n",
    "sys.path.append('.')\n",
    "sys.path.append('../')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from trainer import Trainer, TrainerConfig\n",
    "from utils import set_seed\n",
    "\n",
    "\n",
    "from scipy import io as scipyio\n",
    "from scipy.special import softmax\n",
    "import skimage\n",
    "import skvideo.io\n",
    "from utils import print_full\n",
    "from scipy.ndimage import gaussian_filter, uniform_filter\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "from visualize import *\n",
    "set_plot_params()\n",
    "parent_path = os.path.dirname(os.path.dirname(os.getcwd())) + \"/\"\n",
    "\n",
    "\n",
    "from model_neuroformer import GPT, GPTConfig, neuralGPTConfig\n",
    "from trainer import Trainer, TrainerConfig\n",
    "\n",
    "import json\n",
    "# for i in {1..10}; do python3 -m gather_atts.py; done\n",
    "\n",
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'NaturalMovie'\n",
    "DATASET = 'NaturalStim'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data/NaturalMovie/\"\n",
    "data_dir = \"./data/NaturalStim/\"\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    print(\"Downloading data...\")\n",
    "    import gdown\n",
    "    url = \"https://drive.google.com/drive/folders/1jgYBERZpXdbAP-E5xcSAHsWSa95Z9IFe?usp=sharing\"\n",
    "    gdown.download_folder(id=url, quiet=False, use_cookies=False, output=\"data/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config files\n",
    "import yaml\n",
    "\n",
    "base_path = f\"./configs/{DATASET}\"\n",
    "\n",
    "with open(os.path.join(base_path, 'mconf.yaml'), 'r') as stream:\n",
    "    mconf = yaml.full_load(stream)\n",
    "\n",
    "with open(os.path.join(base_path, 'tconf.yaml'), 'r') as stream:\n",
    "    tconf = yaml.full_load(stream)\n",
    "\n",
    "# with open(os.path.join(base_path, 'dconf.yaml'), 'r') as stream:\n",
    "#     dconf = yaml.full_load(stream)\n",
    "\n",
    "import omegaconf\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# open yaml as omegacong\n",
    "mconf = OmegaConf.create(mconf)\n",
    "tconf = OmegaConf.create(tconf)\n",
    "# dconf = OmegaConf.create(dconf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_window = 0.05\n",
    "window = 0.05\n",
    "window_prev = 0.2\n",
    "window_behavior = window\n",
    "dt = 0.005\n",
    "dt_frames = 0.05\n",
    "dt_vars = 0.05\n",
    "intervals = None\n",
    "n_frames = frame_window // dt_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## choose modalities ##\n",
    "\n",
    "# behavior\n",
    "visual_stim = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroformer.SpikeVidUtils import trial_df_real, make_intervals\n",
    "from neuroformer.prepare_data import load_natmovie_real\n",
    "\n",
    "response_path = \"././data/NaturalStim/20-NatureMovie_part1-A_spikes(1).mat\"\n",
    "stimulus_path = \"././data/NaturalMovie/stimulus/docuMovie.pt\"\n",
    "df, stimulus = load_natmovie_real(response_path, stimulus_path, dt_frames)\n",
    "\n",
    "\n",
    "df['Interval'] = make_intervals(df, window)\n",
    "df['real_interval'] = make_intervals(df, 0.05)\n",
    "df['Interval_2'] = make_intervals(df, window_prev)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "max_window = max(window, window_prev)\n",
    "dt_range = math.ceil(max_window / dt) + 1  # add first / last interval for SOS / EOS'\n",
    "n_dt = [round(dt * n, 2) for n in range(dt_range)] + ['EOS'] + ['PAD']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Interval  Trial\n",
       "367.25    4        30\n",
       "108.75    3        26\n",
       "0.50      0        25\n",
       "0.30      0        24\n",
       "274.15    0        23\n",
       "                   ..\n",
       "278.15    4        10\n",
       "82.80     4        10\n",
       "111.80    0        10\n",
       "111.15    0        10\n",
       "290.25    2        10\n",
       "Length: 2590, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# int_trials = df.groupby(['Interval', 'Trial']).size()\n",
    "# print(int_trials.mean())\n",
    "# df.groupby(['Interval', 'Trial']).agg(['nunique'])\n",
    "var_group = 'Interval'\n",
    "n_unique = len(df.groupby([var_group, 'Trial']).size())\n",
    "# df.groupby([var_group, 'Trial']).size().nlargest(int(0.2 * n_unique))\n",
    "df.groupby([var_group, 'Trial']).size().sort_values(ascending=False).nlargest(int(0.05 * n_unique))\n",
    "# df.groupby([var_group, 'Trial']).size().sort_values(ascending=False).nsmallest(int(0.99 * n_unique))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## resnet3d feats\n",
    "n_embd = 256\n",
    "n_embd_frames = 64\n",
    "conv_layer = True\n",
    "kernel_size = (1, 8, 8)\n",
    "\n",
    "frame_block_size = ((20 // kernel_size[0] * 64 * 112) // (n_embd_frames))\n",
    "frame_feats = stimulus if visual_stim else None\n",
    "frame_block_size = (20 * 64 * 112) // (n_embd_frames)\n",
    "# frame_block_size = 560\n",
    "prev_id_block_size = 67\n",
    "id_block_size = prev_id_block_size   # 95\n",
    "block_size = frame_block_size + id_block_size + prev_id_block_size # frame_block_size * 2  # small window for faster training\n",
    "frame_memory = 20   # how many frames back does model see\n",
    "window = window\n",
    "\n",
    "neurons = sorted(list(set(df['ID'])))\n",
    "id_stoi = { ch:i for i,ch in enumerate(neurons) }\n",
    "id_itos = { i:ch for i,ch in enumerate(neurons) }\n",
    "\n",
    "neurons = [i for i in range(df['ID'].min(), df['ID'].max() + 1)]\n",
    "feat_encodings = neurons + ['SOS'] + ['EOS'] + ['PAD']  # + pixels \n",
    "stoi = { ch:i for i,ch in enumerate(feat_encodings) }\n",
    "itos = { i:ch for i,ch in enumerate(feat_encodings) }\n",
    "stoi_dt = { ch:i for i,ch in enumerate(n_dt) }\n",
    "itos_dt = { i:ch for i,ch in enumerate(n_dt) }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "r_split = 0.8\n",
    "all_trials = sorted(df['Trial'].unique())\n",
    "train_trials = random.sample(all_trials, int(len(all_trials) * r_split))\n",
    "\n",
    "train_data = df[df['Trial'].isin(train_trials)]\n",
    "test_data = df[~df['Trial'].isin(train_trials)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 164292 Neurons: 1349\n",
      "id block size: 67\n",
      "frames: 2240, id: 67\n",
      "Length: 56931 Neurons: 1349\n",
      "id block size: 67\n",
      "frames: 2240, id: 67\n",
      "train: 39629, test: 12139\n"
     ]
    }
   ],
   "source": [
    "from neuroformer.SpikeVidUtils import SpikeTimeVidData2\n",
    "\n",
    "\n",
    "train_dataset = SpikeTimeVidData2(train_data, None, block_size, id_block_size, frame_block_size, prev_id_block_size, \n",
    "                                  window, dt, frame_memory, stoi, itos, neurons, stoi_dt, itos_dt, frame_feats,\n",
    "                                  pred=False, window_prev=window_prev, frame_window=frame_window,\n",
    "                                  dt_frames=dt_frames, intervals=None, dataset=DATASET,\n",
    "                                  window_behavior=window_behavior)\n",
    "test_dataset = SpikeTimeVidData2(test_data, None, block_size, id_block_size, frame_block_size, prev_id_block_size, \n",
    "                                  window, dt, frame_memory, stoi, itos, neurons, stoi_dt, itos_dt, frame_feats,\n",
    "                                  pred=False, window_prev=window_prev, frame_window=frame_window,\n",
    "                                  dt_frames=dt_frames, intervals=None, dataset=DATASET,\n",
    "                                  dt_vars=dt_vars, \n",
    "                                  window_behavior=window_behavior)\n",
    "\n",
    "print(f'train: {len(train_dataset)}, test: {len(test_dataset)}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(train_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "iterable = iter(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_prev torch.Size([32, 67])\n",
      "dt_prev torch.Size([32, 67])\n",
      "pad_prev torch.Size([32])\n",
      "id torch.Size([32, 67])\n",
      "dt torch.Size([32, 67])\n",
      "pad torch.Size([32])\n",
      "interval torch.Size([32])\n",
      "trial torch.Size([32])\n",
      "frames torch.Size([32, 1, 1, 64, 112])\n",
      "stimulus torch.Size([32])\n",
      "cid torch.Size([32, 2])\n",
      "pid torch.Size([32, 2])\n",
      "f_idx torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iterable)\n",
    "# print(x['behavior'].shape, x['behavior_dt'].shape)\n",
    "for k in x.keys():\n",
    "    print(k, x[k].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573440\n",
      "256 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/21/2023 17:47:04 - INFO - model_neuroformer -   number of parameters: 6.434186e+08\n"
     ]
    }
   ],
   "source": [
    "from model_neuroformer import GPT, GPTConfig\n",
    "\n",
    "layers = (mconf.n_state_layers, mconf.n_state_history_layers, mconf.n_stimulus_layers)   \n",
    "max_epochs = 300\n",
    "batch_size = round((32 * 6))\n",
    "shuffle = True\n",
    "\n",
    "title =  f'window:{window}_prev:{window_prev}_contatpad'\n",
    "\n",
    "model_path = f\"\"\"./models/tensorboard/{DATASET}/{title}/sparse_f:{mconf.sparse_topk_frame}_id:{mconf.sparse_topk_id}/w:{window}_wp:{window_prev}/{6}_Cont:{mconf.contrastive}_window:{window}_f_window:{frame_window}_df:{dt}_blocksize:{id_block_size}_conv_{conv_layer}_shuffle:{shuffle}_batch:{batch_size}_sparse_({mconf.sparse_topk_frame}_{mconf.sparse_topk_id})_blocksz{block_size}_pos_emb:{mconf.pos_emb}_temp_emb:{mconf.temp_emb}_drop:{mconf.id_drop}_dt:{shuffle}_2.0_{max(stoi_dt.values())}_max{dt}_{layers}_{mconf.n_head}_{mconf.n_embd}.pt\"\"\"\n",
    "\n",
    "model_conf = GPTConfig(train_dataset.population_size, block_size,    # frame_block_size\n",
    "                        id_vocab_size=train_dataset.id_population_size,\n",
    "                        frame_block_size=frame_block_size,\n",
    "                        id_block_size=id_block_size,  # frame_block_size\n",
    "                        prev_id_block_size=prev_id_block_size,\n",
    "                        sparse_mask=False, p_sparse=None, \n",
    "                        sparse_topk_frame=None, sparse_topk_id=None, sparse_topk_prev_id=None,\n",
    "                        n_dt=len(n_dt),\n",
    "                        data_size=train_dataset.size,\n",
    "                        class_weights=None,\n",
    "                        pretrain=False,\n",
    "                        n_state_layers=mconf.n_state_layers, n_state_history_layers=mconf.n_state_history_layers,\n",
    "                        n_stimulus_layers=mconf.n_stimulus_layers, self_att_layers=mconf.self_att_layers,\n",
    "                        n_behavior_layers=mconf.n_behavior_layers,\n",
    "                        n_head=mconf.n_head, n_embd=mconf.n_embd, \n",
    "                        contrastive=mconf.contrastive, clip_emb=1024, clip_temp=mconf.clip_temp,\n",
    "                        conv_layer=conv_layer, kernel_size=kernel_size,\n",
    "                        temp_emb=mconf.temp_emb, pos_emb=False,\n",
    "                        id_drop=0.35, im_drop=0.35, b_drop=0.45,\n",
    "                        window=window, window_prev=window_prev, frame_window=frame_window, dt=dt,\n",
    "                        neurons=neurons, stoi_dt=stoi_dt, itos_dt=itos_dt, n_embd_frames=n_embd_frames,\n",
    "                        ignore_index_id=stoi['PAD'], ignore_index_dt=stoi_dt['PAD'])  # 0.35\n",
    "\n",
    "model = GPT(model_conf)\n",
    "# model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "preds, features, loss = model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "features['id'].mean(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tconf = TrainerConfig(max_epochs=max_epochs, batch_size=batch_size, learning_rate=2e-4, \n",
    "                    num_workers=4, lr_decay=True, patience=3, warmup_tokens=8e7, \n",
    "                    decay_weights=True, weight_decay=0.2, shuffle=shuffle,\n",
    "                    final_tokens=len(train_dataset)*(id_block_size) * (max_epochs),\n",
    "                    clip_norm=1.0, grad_norm_clip=1.0,\n",
    "                    dataset='higher_order', mode='predict',\n",
    "                    block_size=train_dataset.block_size,\n",
    "                    id_block_size=train_dataset.id_block_size,\n",
    "                    show_grads=False, plot_raster=False,\n",
    "                    ckpt_path=model_path, no_pbar=False, \n",
    "                    dist=False, save_every=1000)\n",
    "\n",
    "# trainer = Trainer(model, train_dataset, test_dataset, tconf, model_conf)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroformer.utils import predict_raster_recursive_time_auto, process_predictions\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "results_dict = dict()\n",
    "df_pred = None\n",
    "df_true = None\n",
    "\n",
    "top_p = 0.75\n",
    "top_p_t = 0.75\n",
    "temp = 1.25\n",
    "temp_t = 1.25\n",
    "max_interval = 32\n",
    "\n",
    "trials = test_data['Trial'].unique()\n",
    "for trial in trials:   \n",
    "        print(f\"Trial: {trial}\")\n",
    "        df_trial = df[(df['Trial'] == trial) & (df['Interval'] <= max_interval)]\n",
    "        trial_dataset = SpikeTimeVidData2(df_trial,  None, block_size, id_block_size, frame_block_size, prev_id_block_size, \n",
    "                                  window, dt, frame_memory, stoi, itos, neurons, stoi_dt, itos_dt, frame_feats,\n",
    "                                  pred=False, window_prev=window_prev, frame_window=frame_window,\n",
    "                                  dt_frames=dt_frames, intervals=None, dataset=DATASET,\n",
    "                                  window_behavior=window_behavior)\n",
    "        results_trial = predict_raster_recursive_time_auto(model, trial_dataset, window, window_prev, stoi, itos_dt, itos=itos, \n",
    "                                                           sample=True, top_p=top_p, top_p_t=top_p_t, temp=temp, temp_t=temp_t, \n",
    "                                                           frame_end=0, get_dt=True, gpu=False, pred_dt=True)\n",
    "        # results_trial = predict_raster_hungarian(model, loader, itos_dt, top_p=0.75, temp=1)\n",
    "        # print(f\"MAX ID ---- {sorted(results_trial['ID'].unique()[-10])}\")\n",
    "        df_trial_pred, df_trial_true = process_predictions(results_trial, stoi, itos, window)\n",
    "        print(f\"pred: {df_trial_pred.shape}, true: {df_trial_true.shape}\" )\n",
    "        if df_pred is None:\n",
    "            df_pred = df_trial_pred\n",
    "            df_true = df_trial_true\n",
    "        else:\n",
    "            df_pred = pd.concat([df_pred, df_trial_pred])\n",
    "            df_true = pd.concat([df_true, df_trial_true])\n",
    "\n",
    "\n",
    "from neuroformer.analysis import compute_scores\n",
    "scores = compute_scores(df[df['Trial'].isin(trials)], df_pred)\n",
    "print(scores)\n",
    "print(f\"pred: {len(df_pred)}, true: {len(df_true)}\" )\n",
    "\n",
    "\n",
    "dir_name = os.path.dirname(model_path)\n",
    "model_name = os.path.basename(model_path)\n",
    "df_pred.to_csv(os.path.join(dir_name, F'df_pred_.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis import get_rates_trial, calc_corr_psth\n",
    "\n",
    "df_1 = df[df['Trial'].isin(trials) & (df['Interval'] <= max_interval)]\n",
    "df_pred_full = df_pred\n",
    "\n",
    "window_pred = 0.5\n",
    "window_pred = window if window_pred is None else window_pred\n",
    "intervals = np.array(sorted(set(df['Interval'].unique()) & set(df['Interval'].unique())))\n",
    "labels = np.array([round(window_pred + window_pred*n, 2) for n in range(0, int(max(df_pred_full['Interval']) / window_pred))])\n",
    "ids = sorted(set(df['ID'].unique()) & set(df['ID'].unique()))\n",
    "\n",
    "rates_pred = get_rates_trial(df_pred_full, labels)\n",
    "rates_1 = get_rates_trial(df_1, labels)\n",
    "\n",
    "top_corr_pred = calc_corr_psth(rates_pred, rates_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Evaluate results\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from neuroformer.visualize import *\n",
    "\n",
    "\n",
    "len_pred, len_true = len(df_pred_full), len(df_1)\n",
    "print(f\"len_pred: {len_pred}, len_true: {len_true}\")\n",
    "\n",
    "accuracy = get_accuracy(df_pred, df_1)\n",
    "pred_scores = compute_scores(df_1, df_pred_full)\n",
    "\n",
    "print(f\"pred: {pred_scores}\")\n",
    "\n",
    "n_bins = 30\n",
    "set_plot_white()\n",
    "plt.figure(figsize=(10, 10), facecolor='white')\n",
    "plt.title(f'PSTH Correlations (V1 + AL) {title}', fontsize=25)\n",
    "plt.ylabel('Count (n)', fontsize=25)\n",
    "plt.xlabel('Pearson r', fontsize=25)\n",
    "# plt.hist(top_corr_real_2, label='real - real3', alpha=0.6)\n",
    "plt.hist(top_corr_pred, label='real - simulated', alpha=0.6, bins=30)\n",
    "plt.legend(fontsize=20)\n",
    "\n",
    "dir_name = os.path.dirname(model_path)\n",
    "model_name = os.path.basename(model_path)\n",
    "\n",
    "top_p = 0\n",
    "save_title = f'_top_p{top_p}'\n",
    "plt.savefig(os.path.join(dir_name, F'psth_corr_{save_title}_.svg'))\n",
    "df_pred.to_csv(os.path.join(dir_name, F'df_pred_{save_title}_.csv'))\n",
    "\n",
    "plot_distribution(df_1, df_pred, save_path=os.path.join(dir_name, F'psth_dist_.svg'))\n",
    "\n",
    "total_scores = dict()\n",
    "total_scores['pred'] = pred_scores\n",
    "\n",
    "print(f\"model: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iterable)\n",
    "\n",
    "T = len(x['id'])\n",
    "P = x['pad'] - 1\n",
    "T_prev = len(x['id_prev'])\n",
    "P_prev = x['pad_prev'] - 4\n",
    "\n",
    "iv = float(x['interval'])\n",
    "\n",
    "xid = x['id'][: T - P]\n",
    "xid = [itos[int(i)] for i in xid]\n",
    "\n",
    "xid_prev = x['id_prev'][: T_prev - P_prev]\n",
    "xid_prev = [itos[int(i)] for i in xid_prev]\n",
    "\n",
    "print(f\"iv: {iv}, ix+window: {iv + window} pid: {x['pid']} cid: {x['cid']}\")\n",
    "print(f\"x: {xid}\")\n",
    "\n",
    "print(f\"xid_prev: {xid_prev}\")\n",
    "\n",
    "tdiff = 0\n",
    "t_var = 'Time' # 'Interval'\n",
    "int_var = 'cid'\n",
    "# df[(df[t_var] >= iv - tdiff) & (df[t_var] <= iv + (window + tdiff)) & (df['Trial'] == int(x['trial']))]\n",
    "# df[(df[t_var] >= float(x[int_var][0]) - tdiff) & (df[t_var] <= float(x[int_var][1] + tdiff)) & (df['Trial'] == int(x['trial']))]\n",
    "df[(df[t_var] > float(x[int_var][0]) - tdiff) & (df[t_var] <= float(x['cid'][1] + tdiff)) & (df['Trial'] == int(x['trial']))]\n",
    "\n",
    "# t_var = 'Time' # 'Interval'\n",
    "# int_var = 'pid'\n",
    "# df[(df[t_var] > round(float(x[int_var][0]), 2) - tdiff) & (df[t_var] <= round(float(x[int_var][1]), 2)) & (df['Trial'] == int(x['trial']))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
