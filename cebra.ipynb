{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter notebook\n",
      " // CONTRASTIVE: False //\n",
      " // VISUAL: True //\n",
      " // PAST_STATE: True //\n",
      " // PREDICT_BEHAVIOR: True //\n",
      " // BEHAVIOR: True //\n",
      " // FUSE_STIM_BEHAVIOR: False //\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pathlib\n",
    "import glob\n",
    "import os\n",
    "import collections\n",
    "import json\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "import glob\n",
    "from pathlib import Path, PurePath\n",
    "path = Path.cwd()\n",
    "parent_path = path.parents[1]\n",
    "sys.path.append(str(PurePath(parent_path, 'neuroformer')))\n",
    "sys.path.append('neuroformer')\n",
    "sys.path.append('.')\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from neuroformer.model_neuroformer import GPT, GPTConfig, neuralGPTConfig\n",
    "from neuroformer.trainer import Trainer, TrainerConfig\n",
    "from neuroformer.utils import set_seed, update_object, check_common_attrs\n",
    "from neuroformer.visualize import set_plot_params\n",
    "from neuroformer.SpikeVidUtils import round_n, set_intervals\n",
    "set_plot_params()\n",
    "\n",
    "from scipy import io as scipyio\n",
    "from scipy.special import softmax\n",
    "import skimage\n",
    "import skvideo.io\n",
    "from scipy.ndimage import gaussian_filter, uniform_filter\n",
    "\n",
    "parent_path = os.path.dirname(os.path.dirname(os.getcwd())) + \"/\"\n",
    "import argparse\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument(\"--infer\", action=\"store_true\", help=\"Inference mode\")\n",
    "    parser.add_argument(\"--train\", action=\"store_true\", default=False, help=\"Train mode\")\n",
    "    parser.add_argument(\"--infer\", action=\"store_true\", default=False, help=\"Inference mode\")    \n",
    "    parser.add_argument(\"--finetune\", action=\"store_true\", default=False, help=\"Finetune\")\n",
    "    parser.add_argument(\"--pdata\", type=float, default=None, help=\"Proportion of data to finetune on\")\n",
    "    parser.add_argument(\"--dataset\", type=str, default=\"first\", help=\"Dataset\")\n",
    "    parser.add_argument(\"--dist\", action=\"store_true\", default=False, help=\"Distrinuted training\")\n",
    "    parser.add_argument(\"--resume\", type=str, default=None, help=\"Resume from checkpoint\")\n",
    "    parser.add_argument(\"--rand_perm\", action=\"store_true\", default=False, help=\"Randomly permute the ID column\")\n",
    "    parser.add_argument(\"--mconf\", type=str, default=None, help=\"Path to model config file\")\n",
    "    parser.add_argument(\"--downstream\", action=\"store_true\", default=False, help=\"Downstream task\")\n",
    "    parser.add_argument(\"--freeze_model\", action=\"store_true\", default=False, help=\"Freeze model\")\n",
    "    parser.add_argument(\"--title\", type=str, default=None)\n",
    "    parser.add_argument(\"--seed\", type=int, default=25)\n",
    "    parser.add_argument(\"--behavior\", action=\"store_true\", default=False, help=\"Behavior task\")\n",
    "    parser.add_argument(\"--predict_behavior\", action=\"store_true\", default=False, help=\"Predict behavior\")\n",
    "    # parser.add_argument(\"--behavior_vars\", type=str, default=None, help=\"Behavior variables\")\n",
    "    parser.add_argument(\"--behavior_vars\", nargs='+', default=None, help=\"Behavior variables\")\n",
    "    parser.add_argument(\"--round_vars\", action=\"store_true\", default=False, help=\"Round variables\")\n",
    "    parser.add_argument(\"--past_state\", action=\"store_true\", default=False, help=\"Input past state\")\n",
    "    parser.add_argument(\"--visual\", action=\"store_true\", default=False, help=\"Visualize\")\n",
    "    parser.add_argument(\"--contrastive\", action=\"store_true\", default=False, help=\"Contrastive\")\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=0)\n",
    "    parser.add_argument(\"--fuse_stim_behavior\", action=\"store_true\", default=False, help=\"Fuse stimulus and behavior\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     args = parse_args()\n",
    "#     INFERENCE = not args.train\n",
    "# else:\n",
    "#     INFERENCE = True\n",
    "\n",
    "# check if jupyter notebook\n",
    "\n",
    "try:\n",
    "    shell = get_ipython().__class__.__name__\n",
    "    print(\"Running in Jupyter notebook\")\n",
    "    TRAIN = False\n",
    "    FINETUNE = False\n",
    "    PDATA = 0.1\n",
    "    INFERENCE = True\n",
    "    DATASET = \"lateral\"\n",
    "    DIST = False\n",
    "    DOWNSTREAM = False\n",
    "    RESUME = None\n",
    "    RAND_PERM = False\n",
    "    MCONF = \"./configs/visnav/lateral_speed_predict/mconf.yaml\"\n",
    "    FREEZE_MODEL = False\n",
    "    TITLE = None\n",
    "    SEED = 25\n",
    "    BEHAVIOR = True\n",
    "    PREDICT_BEHAVIOR = True\n",
    "    BEHAVIOR_VARS = ['speed']\n",
    "    ROUND_VARS = False\n",
    "    PAST_STATE = True\n",
    "    VISUAL = True\n",
    "    CONTRASTIVE = False\n",
    "    FUSE_STIM_BEHAVIOR = False\n",
    "except:\n",
    "    print(\"Running in terminal\")\n",
    "    args = parse_args()\n",
    "    TRAIN = args.train\n",
    "    FINETUNE = args.finetune\n",
    "    PDATA = args.pdata\n",
    "    INFERENCE = args.infer\n",
    "    DATASET = args.dataset\n",
    "    DIST = args.dist\n",
    "    DOWNSTREAM = args.downstream\n",
    "    RESUME = args.resume\n",
    "    RAND_PERM = args.rand_perm\n",
    "    MCONF = args.mconf\n",
    "    FREEZE_MODEL = args.freeze_model\n",
    "    TITLE = args.title\n",
    "    SEED = args.seed\n",
    "    BEHAVIOR = args.behavior\n",
    "    PREDICT_BEHAVIOR = args.predict_behavior\n",
    "    BEHAVIOR_VARS = args.behavior_vars\n",
    "    ROUND_VARS = args.round_vars\n",
    "    PAST_STATE = args.past_state\n",
    "    VISUAL = args.visual\n",
    "    CONTRASTIVE = args.contrastive\n",
    "    FUSE_STIM_BEHAVIOR = args.fuse_stim_behavior\n",
    "    \n",
    "set_seed(25)\n",
    "\n",
    "print(f\" // CONTRASTIVE: {CONTRASTIVE} //\")\n",
    "print(f\" // VISUAL: {VISUAL} //\")\n",
    "print(f\" // PAST_STATE: {PAST_STATE} //\")\n",
    "print(f\" // PREDICT_BEHAVIOR: {PREDICT_BEHAVIOR} //\")\n",
    "print(f\" // BEHAVIOR: {BEHAVIOR} //\")\n",
    "print(f\" // FUSE_STIM_BEHAVIOR: {FUSE_STIM_BEHAVIOR} //\")\n",
    "\n",
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroformer.prepare_data import DataLinks\n",
    "\n",
    "ds = \"LateralVRDataset\"\n",
    "ds = \"MedialVRDataset\"\n",
    "ds = \"VisNav_VR_Expt\"\n",
    "data_dir = f\"data/VisNav_VR_Expt/\"\n",
    "DATA_POINTERS = getattr(DataLinks, ds)\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    print(\"Downloading data...\")\n",
    "    import gdown\n",
    "    url = DATA_POINTERS['url']\n",
    "    gdown.download_folder(id=url, quiet=False, use_cookies=False, output=DATA_POINTERS['DIRECTORY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config files\n",
    "import yaml\n",
    "\n",
    "# base_path = \"configs/visnav/predict_behavior\"\n",
    "base_path = \"./configs/visnav/lateral_phi_th/better_kernel\" if MCONF is None else os.path.dirname(MCONF)\n",
    "\n",
    "with open(os.path.join(base_path, 'mconf.yaml'), 'r') as stream:\n",
    "    mconf = yaml.full_load(stream)\n",
    "\n",
    "with open(os.path.join(base_path, 'tconf.yaml'), 'r') as stream:\n",
    "    tconf = yaml.full_load(stream)\n",
    "\n",
    "with open(os.path.join(base_path, 'dconf.yaml'), 'r') as stream:\n",
    "    dconf = yaml.full_load(stream)\n",
    "\n",
    "# open yaml as omegacong\n",
    "mconf = OmegaConf.create(mconf)\n",
    "tconf = OmegaConf.create(tconf)\n",
    "dconf = OmegaConf.create(dconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ./data/VisNav_VR_Expt/LateralVRDataset\n"
     ]
    }
   ],
   "source": [
    "import mat73\n",
    "\n",
    "# data_path = DATA_POINTERS['RESPONSE_PATH']\n",
    "if DATASET in [\"first\", \"visnav\"]:\n",
    "    data_path = \"./data/VisNav_VR_Expt\"\n",
    "elif DATASET == \"medial\":\n",
    "    data_path = \"./data/VisNav_VR_Expt/MedialVRDataset/\"\n",
    "elif DATASET == \"lateral\":\n",
    "    data_path = \"./data/VisNav_VR_Expt/LateralVRDataset\"\n",
    "\n",
    "print(f\"Loading data from {data_path}\")\n",
    "# stimulus = np.load(os.path.join(data_path, \"stimulus.npy\"), allow_pickle=True)\n",
    "# response = np.load(os.path.join(data_path, \"response.npy\"), allow_pickle=True)\n",
    "# trial_data = np.load(os.path.join(data_path, \"trial_data.npy\"), allow_pickle=True)\n",
    "data = mat73.loadmat(os.path.join(data_path, \"experiment_data.mat\"))['neuroformer']\n",
    "\n",
    "# data_response_path = \"/data5/antonis/neuroformer/data/VisNav_VR_Expt/yiyi/experiment_data_selected.mat\"\n",
    "# data_response = scipy.io.loadmat(data_response_path)\n",
    "# neurons_sel1 = \"./data/VisNav_VR_Expt/yiyi/sel1.csv\"\n",
    "# neurons_sel1 = pd.read_csv(neurons_sel1)\n",
    "# neurons_sel1 = np.array(neurons_sel1).flatten()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common attributes: {}\n"
     ]
    }
   ],
   "source": [
    "if INFERENCE or FINETUNE or TRAIN:\n",
    "    window = mconf.window\n",
    "    window_prev = mconf.window_prev\n",
    "    frame_window = mconf.frame_window\n",
    "    window_behavior = mconf.window_behavior if hasattr(mconf, 'window_behavior') else None\n",
    "    dt = mconf.dt\n",
    "    dt_frames = mconf.dt_frames if hasattr(mconf, 'dt_frames') else 0.05\n",
    "    dt_vars = mconf.dt_vars if hasattr(mconf, 'dt_vars') else 0.05\n",
    "    dt_speed = mconf.dt_speed if hasattr(mconf, 'dt_speed') else 0.2\n",
    "    intervals = None\n",
    "else:\n",
    "    window = 0.05\n",
    "    window_prev = 0.25\n",
    "    frame_window = window + window_prev\n",
    "    window_behavior = window\n",
    "    dt = 0.005\n",
    "    dt_frames = 0.05\n",
    "    dt_vars = 0.05\n",
    "    dt_speed = 0.2\n",
    "    intervals = None\n",
    "\n",
    "# set attrs that are not equal\n",
    "common_attrs = check_common_attrs(mconf, tconf, dconf)\n",
    "print(f\"Common attributes: {common_attrs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " // using behavior vars: ['speed'] //\n"
     ]
    }
   ],
   "source": [
    "## choose modalities ##\n",
    "\n",
    "# behavior\n",
    "behavior = BEHAVIOR\n",
    "# behavior_vars = ['t', 'eyerad', 'phi', 'speed', 'th']\n",
    "behavior_vars = ['speed'] if BEHAVIOR_VARS is None else BEHAVIOR_VARS\n",
    "n_behavior = len(behavior_vars)\n",
    "predict_behavior = PREDICT_BEHAVIOR\n",
    "# stimulus\n",
    "visual_stim = VISUAL\n",
    "\n",
    "print(f\" // using behavior vars: {BEHAVIOR_VARS} //\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['spks'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['spiketimes'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['eyerad', 'phi', 'rewards', 'speed', 'spiketimes', 't', 'th', 'trialsummary', 'vid_sm'])\n"
     ]
    }
   ],
   "source": [
    "from neuroformer.SpikeVidUtils import trial_df, get_df_visnav, make_intervals, set_trials\n",
    "\n",
    "stimulus = data['vid_sm']\n",
    "response = data['spiketimes']['spks']\n",
    "trial_data = data['trialsummary']\n",
    "# response = data_response['spiketime_sel2']['spks']\n",
    "\n",
    "print(data.keys())\n",
    "\n",
    "df = get_df_visnav(response, trial_data, dt_vars)\n",
    "# df = df[df['ID'].isin(neurons_sel1)].reset_index(drop=True)\n",
    "\n",
    "if behavior or predict_behavior is True:\n",
    "    df_behavior = pd.DataFrame({k: data[k] for k in behavior_vars + ['t']})\n",
    "    # rename t to time\n",
    "    df_behavior = df_behavior.rename(columns={'t': 'Time'}) if df_behavior is not None else None\n",
    "    df_behavior = set_trials(df_behavior, trial_data) \n",
    "    df_behavior['Interval'] = make_intervals(df_behavior, window)\n",
    "    df_behavior['Interval_2'] = make_intervals(df_behavior, window_prev)\n",
    "\n",
    "    # prepare speed variables\n",
    "    if 'speed' in df_behavior.columns:\n",
    "        df_behavior['speed'] = df_behavior['speed'].apply(lambda x: round_n(x, dt_speed))\n",
    "        dt_range_speed = df_behavior['speed'].min(), df_behavior['speed'].max()\n",
    "        dt_range_speed = np.arange(dt_range_speed[0], dt_range_speed[1] + dt_speed, dt_speed)\n",
    "        n_behavior = len(dt_range_speed)\n",
    "        stoi_speed = { round_n(ch, dt_speed):i for i,ch in enumerate(dt_range_speed) }\n",
    "        itos_speed = { i:round_n(ch, dt_speed) for i,ch in enumerate(dt_range_speed) }\n",
    "    else:\n",
    "        n_behavior = None\n",
    "        stoi_speed = None\n",
    "        itos_speed = None\n",
    "        assert predict_behavior is False\n",
    "    \n",
    "    if ROUND_VARS:\n",
    "        print(f\" // ROUNDING behavior vars to {dt} //\")\n",
    "        dt_phi = 0.2\n",
    "        dt_th = 0.2\n",
    "        df_behavior['phi'] = df_behavior['phi'].apply(lambda x: round_n(x, dt_phi))\n",
    "        df_behavior['th'] = df_behavior['th'].apply(lambda x: round_n(x, dt_th))\n",
    "\n",
    "        # prepare phi variables\n",
    "        dt_range_phi = df_behavior['phi']\n",
    "        dt_range_phi = np.arange(dt_range_phi[0], dt_range_phi[1] + dt_phi, dt_phi)\n",
    "        stoi_phi = { round_n(ch, dt_phi):i for i,ch in enumerate(dt_range_phi) }\n",
    "        itos_phi = { i:round_n(ch, dt_phi) for i,ch in enumerate(dt_range_phi) }\n",
    "\n",
    "        # prepare th variables\n",
    "        dt_range_th =  df_behavior['th']\n",
    "        dt_range_th = np.arange(dt_range_th[0], dt_range_th[1] + dt_th, dt_th)\n",
    "        stoi_th = { round_n(ch, dt_th):i for i,ch in enumerate(dt_range_th) }\n",
    "        itos_th = { i:round_n(ch, dt_th) for i,ch in enumerate(dt_range_th) }\n",
    "\n",
    "    # assert (window_behavior) % dt_vars < 1e-5, \"window + window_prev must be divisible by dt_vars\"\n",
    "    samples_per_behavior = int((window + window_prev) // dt_vars)\n",
    "    behavior_block_size = int((window + window_prev) // dt_vars) * (len(df_behavior.columns) - 1)\n",
    "else:\n",
    "    behavior = False \n",
    "    df_behavior = None\n",
    "    behavior_vars = None\n",
    "    behavior_block_size = 0\n",
    "    samples_per_behavior = 0\n",
    "    stoi_speed = None\n",
    "    itos_speed = None\n",
    "    dt_range_speed = None\n",
    "    n_behavior = None\n",
    "    stoi_phi = None\n",
    "    itos_phi = None\n",
    "    dt_range_phi = None\n",
    "    stoi_th = None\n",
    "    itos_th = None\n",
    "    dt_range_th = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroformer.SpikeVidUtils import make_intervals\n",
    "\n",
    "df['Interval'] = make_intervals(df, window)\n",
    "df['real_interval'] = make_intervals(df, 0.05)\n",
    "df['Interval_2'] = make_intervals(df, window_prev)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "max_window = max(window, window_prev)\n",
    "dt_range = math.ceil(max_window / dt) + 1  # add first / last interval for SOS / EOS'\n",
    "n_dt = [round(dt * n, 2) for n in range(dt_range)] + ['EOS'] + ['PAD']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neural_data: (5000, 50)\n",
      "continuous_label: (5000, 3)\n",
      "discrete_label: (5000,)\n"
     ]
    }
   ],
   "source": [
    "timesteps = 5000\n",
    "neurons = 50\n",
    "out_dim = 8\n",
    "\n",
    "neural_data = np.random.normal(0,1,(timesteps, neurons))\n",
    "continuous_label = np.random.normal(0,1,(timesteps, 3))\n",
    "discrete_label = np.random.randint(0,10,(timesteps,))\n",
    "\n",
    "print(f\"neural_data: {neural_data.shape}\")\n",
    "print(f\"continuous_label: {continuous_label.shape}\")\n",
    "print(f\"discrete_label: {discrete_label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2023"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 399)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(response[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 862)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(response[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1253776/2371499772.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  response_arr = np.array(response)\n"
     ]
    }
   ],
   "source": [
    "response_arr = np.array(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming your spike_timing array and total time T are defined\n",
    "# spike_timing = np.array(...)\n",
    "T = 2000\n",
    "spike_timing = np.array(response[1]).shape\n",
    "# Define the number of bins you want\n",
    "n_bins = 100\n",
    "\n",
    "# Define the bin edges based on your total time and number of bins\n",
    "bin_edges = np.linspace(0, T, n_bins + 1)\n",
    "\n",
    "# Use np.histogram to count the number of spikes in each bin\n",
    "binned_spikes, _ = np.histogram(spike_timing, bins=bin_edges)\n",
    "\n",
    "# Now binned_spikes is your binned array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cebra' has no attribute 'load_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1253776/2547617122.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the .npz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mneural_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcebra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"neural_data.npz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"neural\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# ... and similarly load the .h5 file, providing the columns to keep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cebra' has no attribute 'load_data'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import cebra\n",
    "\n",
    "# Load the .npz\n",
    "neural_data = cebra.load_data(file=\"neural_data.npz\", key=\"neural\")\n",
    "\n",
    "# ... and similarly load the .h5 file, providing the columns to keep\n",
    "continuous_label = cebra.load_data(file=\"auxiliary_behavior_data.h5\", key=\"auxiliary_variables\", columns=[\"continuous1\", \"continuous2\", \"continuous3\"])\n",
    "discrete_label = cebra.load_data(file=\"auxiliary_behavior_data.h5\", key=\"auxiliary_variables\", columns=[\"discrete\"]).flatten()\n",
    "\n",
    "\n",
    "# 1. Train a CEBRA-Time model on the whole dataset\n",
    "cebra_model = cebra.CEBRA(max_iterations=10)\n",
    "cebra_model.fit(neural_data)\n",
    "embedding = cebra_model.transform(neural_data)\n",
    "\n",
    "# 2. Split the embedding and label to decode into train/validation sets\n",
    "(\n",
    "     train_embedding,\n",
    "     valid_embedding,\n",
    "     train_discrete_label,\n",
    "     valid_discrete_label,\n",
    ") = train_test_split(embedding,\n",
    "                     discrete_label,\n",
    "                     test_size=0.3)\n",
    "\n",
    "# 3. Train the decoder on the training set\n",
    "decoder = cebra.KNNDecoder()\n",
    "decoder.fit(train_embedding, train_discrete_label)\n",
    "\n",
    "# 4. Get the score on the validation set\n",
    "score = decoder.score(valid_embedding, valid_discrete_label)\n",
    "\n",
    "# 5. Get the discrete labels predictions\n",
    "prediction = decoder.predict(valid_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cebra'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3719373/2059032347.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcebra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the .npz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mneural_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcebra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"neural_data.npz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"neural\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cebra'"
     ]
    }
   ],
   "source": [
    "import cebra\n",
    "\n",
    "# Load the .npz\n",
    "neural_data = cebra.load_data(file=\"neural_data.npz\", key=\"neural\")\n",
    "\n",
    "# ... and similarly load the .h5 file, providing the columns to keep\n",
    "continuous_label = cebra.load_data(file=\"auxiliary_behavior_data.h5\", key=\"auxiliary_variables\", columns=[\"continuous1\", \"continuous2\", \"continuous3\"])\n",
    "discrete_label = cebra.load_data(file=\"auxiliary_behavior_data.h5\", key=\"auxiliary_variables\", columns=[\"discrete\"]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
