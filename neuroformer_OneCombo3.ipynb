{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4F6CeGQy9xVu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import collections\n",
        "from torch.utils import data\n",
        "\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "sys.path.append('../')\n",
        "\n",
        "from einops import rearrange\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as FeatureAlphaDropout\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "import math\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from scipy import io as scipyio\n",
        "import skimage\n",
        "import skvideo.io\n",
        "\n",
        "import os\n",
        "import glob\n",
        "parent_path = os.path.dirname(os.path.dirname(os.getcwd())) + \"/\"\n",
        "sys.path.append(\"neuroformer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EmB0YuTW-zdZ"
      },
      "outputs": [],
      "source": [
        "# set up logging\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zIJX2fry_rAH"
      },
      "outputs": [],
      "source": [
        "from utils import set_seed\n",
        "set_seed(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5U-Mr5sF_s-3",
        "outputId": "bbb0deae-6896-4c65-c0c4-6470bdd6ca46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n",
            "im_sack size: torch.Size([5760, 1, 64, 112])\n"
          ]
        }
      ],
      "source": [
        "# R3D: (3 x T x H x W)\n",
        "\n",
        "from SpikeVidUtils import image_dataset\n",
        "\n",
        "stim_folder = \"/home/antonis/projects/slab/git/slab/transformer_exp/code/data/OneCombo3/stimuli\"\n",
        "im_path = ['/Combined Stimuli 3-grating.tif',\n",
        "           '/Combined Stimuli 3-Movie2.tif',\n",
        "           '/Combined Stimuli 3-Movie3.tif']\n",
        "\n",
        "train_path = \"/content/stimulus\"\n",
        "train_path = \"/Users/antonis/Downloads/OneCombo3/stimuli\"\n",
        "video_stack = [skimage.io.imread(stim_folder + vid) for vid in im_path]\n",
        "print(glob.glob(train_path + '/*.tif'))\n",
        "video_stack = np.concatenate(video_stack, axis=0)\n",
        "\n",
        "# video_stack = skimage.io.imread(\"/home/antonis/projects/slab/git/slab/transformer_exp/code/data/OneCombo3/stimuli/Combined Stimuli 3-grating.tif\")\n",
        "# video_stack = image_dataset(video_stack)\n",
        "# video_stack = video_stack[::3]  # convert from 60 to 20 fps\n",
        "# video_stack = video_stack.view(1, video_stack.shape[0], video_stack.shape[1], video_stack.shape[2], video_stack.shape[3])\n",
        "\n",
        "video_stack = image_dataset(video_stack)\n",
        "video_stack = video_stack[::3]  # convert from 60 to 20 fps\n",
        "video_stack = video_stack.view(3, video_stack.shape[0] // 3, video_stack.shape[1], video_stack.shape[2], video_stack.shape[3])\n",
        "# video_stack = video_stack.transpose(-1, -2)\n",
        "\n",
        "# rearrange(video_stack[0, 0:2].transpose(0,1), 'c t (h p1) (w p2) -> (t h w) (p1 p2 c)', p1=16, p2=16).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge_QNMrICWR7",
        "outputId": "40e9b4a3-c870-4eeb-8fff-82355db2f4b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 640, 1, 64, 112])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "video_stack.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "UWOkHke3_y4A",
        "outputId": "dcd40d85-e4c6-4e5b-b31b-68fcd419eafd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff56f86cd00>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "plt.figure()\n",
        "plt.imshow(video_stack[0, 1, 0].permute(0, 1))\n",
        "plt.figure()\n",
        "plt.imshow(video_stack[1, 1, 0].permute(0, 1))\n",
        "plt.figure()\n",
        "plt.imshow(video_stack[2, 1, 0].permute(0, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "rRFtpwOs_0W4"
      },
      "outputs": [],
      "source": [
        "# spike_path = \"/home/antonis/projects/slab/git/slab/transformer_exp/code/data/SImNew3D/neural/NatureMoviePart1-A\" # \"code/data/SImIm/simNeu_3D_WithNorm__Combo3.mat\" \n",
        "from SpikeVidUtils import trial_df_combo3\n",
        "\n",
        "spike_data = scipyio.loadmat(\"/home/antonis/projects/slab/git/slab/transformer_exp/code/data/OneCombo3/spiketrain.mat\")\n",
        "spike_data = np.squeeze(spike_data['spiketrain'].T, axis=-1)\n",
        "spike_data = [trial_df_combo3(spike_data, n_stim) for n_stim in range(3)]\n",
        "spike_data = pd.concat(spike_data, axis=0)\n",
        "\n",
        "spike_data['Trial'] = spike_data['Trial'] + 1\n",
        "spike_data['Time'] = spike_data['Time'] * 0.0751\n",
        "spike_data = spike_data[(spike_data['Time'] > 0) & (spike_data['Time'] <= 32)]\n",
        "\n",
        "# vid_duration = [len(vid) * 1/20 for vid in vid_list]\n",
        "\n",
        "df = spike_data\n",
        "del spike_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_sim = pd.read_csv(\"data/full_sim__model_weighted_shuffle_decay_True_perceiver_2.0_dt_0.05_eos_8_8_256.csv\").iloc[1:, 1:].reset_index(drop=True)\n",
        "# df_sim = df_sim[df_sim['ID'] <= 164]\n",
        "# df_sim['Trial'] = df_sim['Trial'] + df['Trial'].max()\n",
        "# df = pd.concat([df, df_sim], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "SR7Vs-Pn_3oo"
      },
      "outputs": [],
      "source": [
        "# df = pd.read_csv(parent_path + \"code/data/OneCombo3/Combo3_all_stim.csv\")\n",
        "window = 0.5\n",
        "dt = 0.05\n",
        "\n",
        "from SpikeVidUtils import make_intervals\n",
        "\n",
        "df['Interval'] = make_intervals(df, window)\n",
        "# df['Interval_dt'] = make_intervals(df, dt)\n",
        "# df['Interval_dt'] = (df['Interval_dt'] - df['Interval'] + window).round(3)\n",
        "df = df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "wwyX2t-z_44J"
      },
      "outputs": [],
      "source": [
        "# n_dt = sorted((df['Interval_dt'].unique()).round(2)) \n",
        "dt_range = int(window / dt) + 1  # add first / last interval for SOS / EOS'\n",
        "n_dt = [round(dt * n, 2) for n in range(dt_range)]\n",
        "df['Time'] = df['Time'].round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgNYdciA_5xo",
        "outputId": "9be97414-3f42-49e8-ebbf-b887cf2d221b"
      },
      "outputs": [],
      "source": [
        "# df.groupby(['Interval', 'Trial']).size().plot.bar()\n",
        "# df.groupby(['Interval', 'Trial']).agg(['nunique'])\n",
        "# df.groupby(['Interval', 'Trial']).size().nlargest(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vd3P4ASE_7gy",
        "outputId": "3f834a76-ced6-4023-bd1b-8a420f7d0e2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from SpikeVidUtils import SpikeTimeVidData\n",
        "\n",
        "## qv-vae feats\n",
        "# frames = torch.load(parent_path + \"code/data/SImNew3D/stimulus/vq-vae_code_feats-24-05-4x4x4.pt\").numpy() + 2\n",
        "# frame_feats = torch.load(parent_path + \"code/data/SImNew3D/stimulus/vq-vae_embed_feats-24-05-4x4x4.pt\").numpy()\n",
        "# frame_block_size = frames.shape[-1] - 1\n",
        "\n",
        "## resnet3d feats\n",
        "frame_feats = video_stack.transpose(1, 2)\n",
        "\n",
        "frame_block_size = 560\n",
        "prev_id_block_size = 52\n",
        "id_block_size = 52   # 95\n",
        "block_size = frame_block_size + id_block_size + prev_id_block_size # frame_block_size * 2  # small window for faster training\n",
        "frame_memory = 20   # how many frames back does model see\n",
        "window = window\n",
        "\n",
        "neurons = sorted(list(set(df['ID'])))\n",
        "id_stoi = { ch:i for i,ch in enumerate(neurons) }\n",
        "id_itos = { i:ch for i,ch in enumerate(neurons) }\n",
        "\n",
        "# translate neural embeddings to separate them from ID embeddings\n",
        "# frames = frames + [*id_stoi.keys()][-1] \n",
        "neurons = [i for i in range(df['ID'].min(), df['ID'].max() + 1)]\n",
        "# pixels = sorted(np.unique(frames).tolist())\n",
        "feat_encodings = neurons + ['SOS'] + ['EOS'] + ['PAD']  # + pixels \n",
        "stoi = { ch:i for i,ch in enumerate(feat_encodings) }\n",
        "itos = { i:ch for i,ch in enumerate(feat_encodings) }\n",
        "stoi_dt = { ch:i for i,ch in enumerate(n_dt) }\n",
        "itos_dt = { i:ch for i,ch in enumerate(n_dt) }\n",
        "max(list(itos_dt.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "O1ZJuUWTDpwk"
      },
      "outputs": [],
      "source": [
        "# train_len = round(len(df)*(4/5))\n",
        "# test_len = round(len(df) - train_len)\n",
        "\n",
        "# train_data = df[:train_len]\n",
        "# test_data = df[train_len:train_len + test_len].reset_index().drop(['index'], axis=1)\n",
        "\n",
        "n = []\n",
        "for n_stim in range(df['Trial'].max() // 20):\n",
        "    n_trial = [3, 15, 5, 18]\n",
        "    for n_trial in n_trial:\n",
        "        trial = (n_stim + 1) * 20 - n_trial\n",
        "        n.append(trial)\n",
        "train_data = df[~df['Trial'].isin(n)].reset_index(drop=True)\n",
        "test_data = df[df['Trial'].isin(n)].reset_index(drop=True)\n",
        "small_data = df[df['Trial'].isin([5])].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luwjZJiBDuJV",
        "outputId": "46a1af29-90f1-4d64-c785-f35316d319cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length: 20449 Neurons: 168 Pixels: 0.\n",
            "id block size: 52\n",
            "frames: 560, id: 52\n",
            "Length: 5163 Neurons: 168 Pixels: 0.\n",
            "id block size: 52\n",
            "frames: 560, id: 52\n",
            "Length: 393 Neurons: 168 Pixels: 0.\n",
            "id block size: 52\n",
            "frames: 560, id: 52\n",
            "train: 2941, test: 729\n"
          ]
        }
      ],
      "source": [
        "from SpikeVidUtils import SpikeTimeVidData2\n",
        "\n",
        "# train_dat1aset = spikeTimeData(spikes, block_size, dt, stoi, itos)\n",
        "\n",
        "train_dataset = SpikeTimeVidData2(train_data, None, block_size, id_block_size, frame_block_size, prev_id_block_size, window, dt, frame_memory, stoi, itos, neurons, stoi_dt, itos_dt, frame_feats, pred=False)\n",
        "test_dataset = SpikeTimeVidData2(test_data, None, block_size, id_block_size, frame_block_size, prev_id_block_size, window, dt, frame_memory, stoi, itos, neurons, stoi_dt, itos_dt, frame_feats, pred=False)\n",
        "# dataset = SpikeTimeVidData(df, frames, frame_feats, block_size, frame_block_size, prev_id_block_size, window, frame_memory, stoi, itos)\n",
        "# single_batch = SpikeTimeVidData(df[df['Trial'].isin([5])], None, block_size, frame_block_size, prev_id_block_size, window, frame_memory, stoi, itos, neurons, stoi_dt, itos_dt, frame_feats)\n",
        "small_dataset = SpikeTimeVidData2(small_data, None, block_size, id_block_size, frame_block_size, prev_id_block_size, window, dt, frame_memory, stoi, itos, neurons, stoi_dt, itos_dt, frame_feats, pred=False)\n",
        "\n",
        "\n",
        "print(f'train: {len(train_dataset)}, test: {len(test_dataset)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "iterable = iter(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "x, y = next(iterable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([165, 166, 167, 167, 167, 167, 167, 167, 167, 167, 167])"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x['id_prev'][:len(x['id_prev']) - x['pad']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([165, 150,  47,  57,  23, 149,  13, 157, 156, 112,  26])"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x['id'][:len(x['id']) - x['pad']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([150,  47,  57,  23, 149,  13, 157, 156, 112,  26, 166])"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y['id'][:len(y['id']) - x['pad']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 1,  3,  3,  4,  5,  6,  7,  8,  8,  9, 10])"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y['dt'][:len(y['id']) - x['pad']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([52])"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x['id'].size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x['id'].size() == x['id_prev'].size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([52])"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x['id_prev'].size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "jYyJLHClVS5M"
      },
      "outputs": [],
      "source": [
        "def get_class_weights(dataset):\n",
        "  dt = []\n",
        "  id = []\n",
        "  for x, y in dataset:\n",
        "    id.extend([stoi['SOS']] + y['id'][:len(y['id']) - x['pad']].flatten().tolist() + [stoi['PAD']]) # -1 in pad to include PAD token\n",
        "    dt.extend([stoi_dt[0]] + y['dt'][:len(y['dt']) - x['pad']].flatten().tolist() + [dataset.dt_max]) # -1 in pad to include PAD token\n",
        "\n",
        "  id = pd.DataFrame(id)\n",
        "  dt = pd.DataFrame(dt)\n",
        "\n",
        "  id_freq = id.groupby([0]).size()\n",
        "  dt_freq = dt.groupby([0]).size()\n",
        "\n",
        "  id_ones = np.ones(dataset.id_population_size)\n",
        "  dt_ones = np.ones(dataset.dt_population_size)\n",
        "\n",
        "  id_ones[id_freq.index] = (1 / id_freq) * id_freq.max() / id_freq.max()\n",
        "  dt_ones[dt_freq.index] = (1 / dt_freq) * dt_freq.max() / dt_freq.max()\n",
        "  \n",
        "  class_freq = dict()\n",
        "  class_freq['id'] = torch.tensor(id_ones, dtype=torch.float32)\n",
        "  class_freq['dt'] = torch.tensor(dt_ones, dtype=torch.float32)\n",
        "  \n",
        "  return class_freq \n",
        "\n",
        "class_weights = get_class_weights(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTLKWWrqDyDd",
        "outputId": "7f7fdeff-145f-4412-ed24-6662db6ff997"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "02/04/2022 15:56:31 - INFO - model_neuroformer -   number of parameters: 1.452416e+07\n"
          ]
        }
      ],
      "source": [
        "from model_neuroformer import GPT, GPTConfig, neuralGPTConfig, Decoder\n",
        "# initialize config class and model (holds hyperparameters)\n",
        "mconf = GPTConfig(train_dataset.population_size, block_size,    # frame_block_size\n",
        "                        id_vocab_size=train_dataset.id_population_size,\n",
        "                        frame_block_size=frame_block_size,\n",
        "                        id_block_size=id_block_size,  # frame_block_size\n",
        "                        prev_id_block_size=prev_id_block_size,\n",
        "                        sparse_mask=True, p_sparse=0.25,\n",
        "                        n_dt=len(n_dt),\n",
        "                        data_size=train_dataset.size,\n",
        "                        class_weights=class_weights,\n",
        "                        pretrain=False,\n",
        "                        n_state_layers=6, n_state_history_layers=6, n_stimulus_layers=6,\n",
        "                        n_layer=8, n_head=8, n_embd=256,\n",
        "                        temp_emb=True, pos_emb= True,\n",
        "                        id_drop=0.2, im_drop=0.2)\n",
        "model = GPT(mconf)\n",
        "# model.load_state_dict(torch.load(\"/home/antonis/projects/slab/git/neuroformer/models/neuroformer:False_perceiver_2.0_dt:0.05_eos_(4, 4, 8)_8_256.pt\", map_location='cpu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H0AqcZLqxBH",
        "outputId": "2ce22b00-a8b7-4b49-b846-42066ade9266"
      },
      "outputs": [],
      "source": [
        "# model.load_state_dict(torch.load(\"/content/drive/MyDrive/slab/models/OneCombo3/model_8_4_256.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "AlY4cxbxVrN4"
      },
      "outputs": [],
      "source": [
        "# loader = DataLoader(small_dataset, shuffle=False, pin_memory=False,\n",
        "#                                   batch_size=1, num_workers=1)\n",
        "# x, y = next(iter(loader))\n",
        "# model = model.to('cpu')\n",
        "\n",
        "# preds, features, loss = model(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "tsor = torch.randint(1, 10, (1, 3, 4, 100), dtype=torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(5.0200)"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tsor[0, 1, 1].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "tsor_norm = F.normalize(tsor, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.0869)"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tsor_norm[0, 0, 1].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V_Lm4vumDzfM",
        "outputId": "e956c2e8-a6ed-4e55-9253-df310e4b763c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 1  id_train: 2.45563  time_train: 1.16282  total_loss: 3.61845 lr 3.993600e-05: 100%|██████████| 16/16 [00:08<00:00,  1.87it/s]\n",
            "02/04/2022 15:56:41 - INFO - trainer -   id_test: 2.37372    time_test: 1.14084  total_loss: 3.51456\n",
            "02/04/2022 15:56:42 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 2  id_train: 2.36140  time_train: 1.15804  total_loss: 3.51944 lr 8.000000e-05: 100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n",
            "02/04/2022 15:56:51 - INFO - trainer -   id_test: 2.28808    time_test: 1.12308  total_loss: 3.41116\n",
            "02/04/2022 15:56:51 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 3  id_train: 2.24504  time_train: 1.11297  total_loss: 3.35801 lr 7.999968e-05: 100%|██████████| 16/16 [00:08<00:00,  1.86it/s]\n",
            "02/04/2022 15:57:01 - INFO - trainer -   id_test: 2.24569    time_test: 1.09033  total_loss: 3.33601\n",
            "02/04/2022 15:57:01 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 4  id_train: 2.25625  time_train: 1.08704  total_loss: 3.34329 lr 7.999874e-05: 100%|██████████| 16/16 [00:08<00:00,  1.91it/s]\n",
            "02/04/2022 15:57:11 - INFO - trainer -   id_test: 2.26038    time_test: 1.08278  total_loss: 3.34316\n",
            "epoch 5  id_train: 2.18418  time_train: 1.04627  total_loss: 3.23045 lr 7.999718e-05: 100%|██████████| 16/16 [00:08<00:00,  1.87it/s]\n",
            "02/04/2022 15:57:20 - INFO - trainer -   id_test: 2.22721    time_test: 1.05438  total_loss: 3.28159\n",
            "02/04/2022 15:57:20 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 6  id_train: 2.16810  time_train: 1.07293  total_loss: 3.24104 lr 7.999500e-05: 100%|██████████| 16/16 [00:08<00:00,  1.90it/s]\n",
            "02/04/2022 15:57:30 - INFO - trainer -   id_test: 2.21818    time_test: 1.04604  total_loss: 3.26422\n",
            "02/04/2022 15:57:30 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 7  id_train: 2.18513  time_train: 1.06256  total_loss: 3.24769 lr 7.999220e-05: 100%|██████████| 16/16 [00:08<00:00,  1.89it/s]\n",
            "02/04/2022 15:57:40 - INFO - trainer -   id_test: 2.23215    time_test: 1.05686  total_loss: 3.28901\n",
            "epoch 8  id_train: 2.23119  time_train: 1.06547  total_loss: 3.29665 lr 7.998878e-05: 100%|██████████| 16/16 [00:08<00:00,  1.85it/s]\n",
            "02/04/2022 15:57:49 - INFO - trainer -   id_test: 2.18604    time_test: 1.04171  total_loss: 3.22776\n",
            "02/04/2022 15:57:49 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 9  id_train: 2.20662  time_train: 1.07238  total_loss: 3.27900 lr 7.998474e-05: 100%|██████████| 16/16 [00:08<00:00,  1.89it/s]\n",
            "02/04/2022 15:57:59 - INFO - trainer -   id_test: 2.23342    time_test: 1.04530  total_loss: 3.27872\n",
            "epoch 10  id_train: 2.22369  time_train: 1.03220  total_loss: 3.25589 lr 7.998008e-05: 100%|██████████| 16/16 [00:08<00:00,  1.87it/s]\n",
            "02/04/2022 15:58:09 - INFO - trainer -   id_test: 2.17115    time_test: 1.02585  total_loss: 3.19700\n",
            "02/04/2022 15:58:09 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 11  id_train: 2.20139  time_train: 1.03972  total_loss: 3.24110 lr 7.997480e-05: 100%|██████████| 16/16 [00:08<00:00,  1.87it/s]\n",
            "02/04/2022 15:58:18 - INFO - trainer -   id_test: 2.24206    time_test: 0.99688  total_loss: 3.23894\n",
            "epoch 12  id_train: 2.21734  time_train: 1.03325  total_loss: 3.25059 lr 7.996890e-05: 100%|██████████| 16/16 [00:08<00:00,  1.86it/s]\n",
            "02/04/2022 15:58:28 - INFO - trainer -   id_test: 2.16200    time_test: 0.96655  total_loss: 3.12855\n",
            "02/04/2022 15:58:28 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 13  id_train: 2.18314  time_train: 0.94855  total_loss: 3.13169 lr 7.996238e-05: 100%|██████████| 16/16 [00:08<00:00,  1.86it/s]\n",
            "02/04/2022 15:58:38 - INFO - trainer -   id_test: 2.15987    time_test: 0.93666  total_loss: 3.09652\n",
            "02/04/2022 15:58:38 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 14  id_train: 2.20381  time_train: 0.95159  total_loss: 3.15540 lr 7.995525e-05: 100%|██████████| 16/16 [00:08<00:00,  1.86it/s]\n",
            "02/04/2022 15:58:48 - INFO - trainer -   id_test: 2.20186    time_test: 0.91956  total_loss: 3.12142\n",
            "epoch 15  id_train: 2.15372  time_train: 0.94758  total_loss: 3.10130 lr 7.994749e-05: 100%|██████████| 16/16 [00:08<00:00,  1.87it/s]\n",
            "02/04/2022 15:58:57 - INFO - trainer -   id_test: 2.18993    time_test: 0.92708  total_loss: 3.11701\n",
            "epoch 16  id_train: 2.13535  time_train: 0.93673  total_loss: 3.07207 lr 7.993911e-05: 100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n",
            "02/04/2022 15:59:07 - INFO - trainer -   id_test: 2.18085    time_test: 0.89431  total_loss: 3.07516\n",
            "02/04/2022 15:59:07 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 17  id_train: 2.18680  time_train: 0.91988  total_loss: 3.10667 lr 7.993012e-05: 100%|██████████| 16/16 [00:08<00:00,  1.78it/s]\n",
            "02/04/2022 15:59:17 - INFO - trainer -   id_test: 2.12624    time_test: 0.89278  total_loss: 3.01902\n",
            "02/04/2022 15:59:17 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 18  id_train: 2.21989  time_train: 0.89884  total_loss: 3.11874 lr 7.992051e-05: 100%|██████████| 16/16 [00:09<00:00,  1.77it/s]\n",
            "02/04/2022 15:59:28 - INFO - trainer -   id_test: 2.18730    time_test: 0.86996  total_loss: 3.05726\n",
            "epoch 19  id_train: 2.14601  time_train: 0.88543  total_loss: 3.03144 lr 7.991027e-05: 100%|██████████| 16/16 [00:09<00:00,  1.77it/s]\n",
            "02/04/2022 15:59:38 - INFO - trainer -   id_test: 2.16818    time_test: 0.88152  total_loss: 3.04970\n",
            "epoch 20  id_train: 2.15736  time_train: 0.86756  total_loss: 3.02492 lr 7.989942e-05: 100%|██████████| 16/16 [00:08<00:00,  1.80it/s]\n",
            "02/04/2022 15:59:48 - INFO - trainer -   id_test: 2.15904    time_test: 0.88409  total_loss: 3.04312\n",
            "epoch 21  id_train: 2.19302  time_train: 0.88264  total_loss: 3.07566 lr 7.988796e-05: 100%|██████████| 16/16 [00:08<00:00,  1.79it/s]\n",
            "02/04/2022 15:59:58 - INFO - trainer -   id_test: 2.13324    time_test: 0.86638  total_loss: 2.99962\n",
            "02/04/2022 15:59:58 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 22  id_train: 2.16227  time_train: 0.90039  total_loss: 3.06266 lr 7.987587e-05: 100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n",
            "02/04/2022 16:00:08 - INFO - trainer -   id_test: 2.17609    time_test: 0.84961  total_loss: 3.02570\n",
            "epoch 23  id_train: 2.16624  time_train: 0.87652  total_loss: 3.04276 lr 7.986316e-05: 100%|██████████| 16/16 [00:08<00:00,  1.81it/s]\n",
            "02/04/2022 16:00:18 - INFO - trainer -   id_test: 2.12586    time_test: 0.87016  total_loss: 2.99602\n",
            "02/04/2022 16:00:18 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 24  id_train: 2.11599  time_train: 0.89696  total_loss: 3.01295 lr 7.984984e-05: 100%|██████████| 16/16 [00:09<00:00,  1.77it/s]\n",
            "02/04/2022 16:00:28 - INFO - trainer -   id_test: 2.12626    time_test: 0.87657  total_loss: 3.00283\n",
            "epoch 25  id_train: 2.07869  time_train: 0.88566  total_loss: 2.96435 lr 7.983590e-05: 100%|██████████| 16/16 [00:08<00:00,  1.80it/s]\n",
            "02/04/2022 16:00:38 - INFO - trainer -   id_test: 2.17086    time_test: 0.85101  total_loss: 3.02187\n",
            "epoch 26  id_train: 2.21244  time_train: 0.84530  total_loss: 3.05774 lr 7.982134e-05: 100%|██████████| 16/16 [00:08<00:00,  1.80it/s]\n",
            "02/04/2022 16:00:48 - INFO - trainer -   id_test: 2.18569    time_test: 0.85200  total_loss: 3.03769\n",
            "epoch 27  id_train: 2.13262  time_train: 0.86696  total_loss: 2.99958 lr 7.980617e-05: 100%|██████████| 16/16 [00:08<00:00,  1.80it/s]\n",
            "02/04/2022 16:00:58 - INFO - trainer -   id_test: 2.15512    time_test: 0.85074  total_loss: 3.00586\n",
            "epoch 28  id_train: 2.17805  time_train: 0.84127  total_loss: 3.01932 lr 7.979038e-05: 100%|██████████| 16/16 [00:08<00:00,  1.80it/s]\n",
            "02/04/2022 16:01:08 - INFO - trainer -   id_test: 2.15191    time_test: 0.83451  total_loss: 2.98641\n",
            "02/04/2022 16:01:08 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 29  id_train: 2.16018  time_train: 0.84980  total_loss: 3.00998 lr 7.977397e-05: 100%|██████████| 16/16 [00:08<00:00,  1.78it/s]\n",
            "02/04/2022 16:01:18 - INFO - trainer -   id_test: 2.16777    time_test: 0.83697  total_loss: 3.00475\n",
            "epoch 30  id_train: 2.13143  time_train: 0.86505  total_loss: 2.99647 lr 7.975695e-05: 100%|██████████| 16/16 [00:08<00:00,  1.79it/s]\n",
            "02/04/2022 16:01:28 - INFO - trainer -   id_test: 2.13836    time_test: 0.87649  total_loss: 3.01485\n",
            "epoch 31  id_train: 2.11021  time_train: 0.85767  total_loss: 2.96789 lr 7.973930e-05: 100%|██████████| 16/16 [00:08<00:00,  1.80it/s]\n",
            "02/04/2022 16:01:38 - INFO - trainer -   id_test: 2.14997    time_test: 0.82437  total_loss: 2.97434\n",
            "02/04/2022 16:01:38 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 32  id_train: 2.14168  time_train: 0.83333  total_loss: 2.97500 lr 7.972105e-05: 100%|██████████| 16/16 [00:09<00:00,  1.77it/s]\n",
            "02/04/2022 16:01:49 - INFO - trainer -   id_test: 2.14260    time_test: 0.83542  total_loss: 2.97802\n",
            "epoch 33  id_train: 2.08160  time_train: 0.83944  total_loss: 2.92103 lr 7.970218e-05: 100%|██████████| 16/16 [00:08<00:00,  1.78it/s]\n",
            "02/04/2022 16:01:59 - INFO - trainer -   id_test: 2.13033    time_test: 0.84227  total_loss: 2.97261\n",
            "02/04/2022 16:01:59 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 34  id_train: 2.05399  time_train: 0.85094  total_loss: 2.90494 lr 7.968269e-05: 100%|██████████| 16/16 [00:09<00:00,  1.77it/s]\n",
            "02/04/2022 16:02:09 - INFO - trainer -   id_test: 2.14794    time_test: 0.83126  total_loss: 2.97921\n",
            "epoch 35  id_train: 2.19209  time_train: 0.84461  total_loss: 3.03670 lr 7.966259e-05: 100%|██████████| 16/16 [00:08<00:00,  1.81it/s]\n",
            "02/04/2022 16:02:19 - INFO - trainer -   id_test: 2.11943    time_test: 0.82063  total_loss: 2.94007\n",
            "02/04/2022 16:02:19 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 36  id_train: 2.10537  time_train: 0.83439  total_loss: 2.93977 lr 7.964187e-05: 100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n",
            "02/04/2022 16:02:29 - INFO - trainer -   id_test: 2.13413    time_test: 0.82650  total_loss: 2.96063\n",
            "epoch 37  id_train: 2.13726  time_train: 0.84113  total_loss: 2.97839 lr 7.962054e-05: 100%|██████████| 16/16 [00:08<00:00,  1.78it/s]\n",
            "02/04/2022 16:02:39 - INFO - trainer -   id_test: 2.10052    time_test: 0.83644  total_loss: 2.93696\n",
            "02/04/2022 16:02:39 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 38  id_train: 2.06379  time_train: 0.84274  total_loss: 2.90654 lr 7.959859e-05: 100%|██████████| 16/16 [00:09<00:00,  1.76it/s]\n",
            "02/04/2022 16:02:49 - INFO - trainer -   id_test: 2.16661    time_test: 0.80354  total_loss: 2.97015\n",
            "epoch 39  id_train: 2.08984  time_train: 0.83171  total_loss: 2.92155 lr 7.957603e-05: 100%|██████████| 16/16 [00:09<00:00,  1.74it/s]\n",
            "02/04/2022 16:03:00 - INFO - trainer -   id_test: 2.11408    time_test: 0.81724  total_loss: 2.93131\n",
            "02/04/2022 16:03:00 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 40  id_train: 2.09883  time_train: 0.84685  total_loss: 2.94568 lr 7.955286e-05: 100%|██████████| 16/16 [00:09<00:00,  1.77it/s]\n",
            "02/04/2022 16:03:10 - INFO - trainer -   id_test: 2.11522    time_test: 0.81412  total_loss: 2.92934\n",
            "02/04/2022 16:03:10 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 41  id_train: 2.09337  time_train: 0.82174  total_loss: 2.91510 lr 7.952908e-05: 100%|██████████| 16/16 [00:09<00:00,  1.76it/s]\n",
            "02/04/2022 16:03:20 - INFO - trainer -   id_test: 2.14429    time_test: 0.81734  total_loss: 2.96163\n",
            "epoch 42  id_train: 2.06858  time_train: 0.81568  total_loss: 2.88427 lr 7.950468e-05: 100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n",
            "02/04/2022 16:03:30 - INFO - trainer -   id_test: 2.11365    time_test: 0.82602  total_loss: 2.93967\n",
            "epoch 43  id_train: 2.02089  time_train: 0.83417  total_loss: 2.85506 lr 7.947967e-05: 100%|██████████| 16/16 [00:08<00:00,  1.80it/s]\n",
            "02/04/2022 16:03:40 - INFO - trainer -   id_test: 2.13135    time_test: 0.82328  total_loss: 2.95463\n",
            "epoch 44  id_train: 2.12056  time_train: 0.83328  total_loss: 2.95384 lr 7.945405e-05: 100%|██████████| 16/16 [00:08<00:00,  1.78it/s]\n",
            "02/04/2022 16:03:50 - INFO - trainer -   id_test: 2.12225    time_test: 0.80110  total_loss: 2.92335\n",
            "02/04/2022 16:03:50 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 45  id_train: 2.07087  time_train: 0.84319  total_loss: 2.91407 lr 7.942781e-05: 100%|██████████| 16/16 [00:08<00:00,  1.80it/s]\n",
            "02/04/2022 16:04:00 - INFO - trainer -   id_test: 2.12467    time_test: 0.80537  total_loss: 2.93003\n",
            "epoch 46  id_train: 2.07392  time_train: 0.80341  total_loss: 2.87733 lr 7.940097e-05: 100%|██████████| 16/16 [00:09<00:00,  1.75it/s]\n",
            "02/04/2022 16:04:10 - INFO - trainer -   id_test: 2.14106    time_test: 0.80064  total_loss: 2.94169\n",
            "epoch 47  id_train: 2.09711  time_train: 0.80720  total_loss: 2.90430 lr 7.937352e-05: 100%|██████████| 16/16 [00:04<00:00,  3.62it/s]\n",
            "02/04/2022 16:04:16 - INFO - trainer -   id_test: 2.13623    time_test: 0.82297  total_loss: 2.95920\n",
            "epoch 48  id_train: 2.07090  time_train: 0.81406  total_loss: 2.88496 lr 7.934545e-05: 100%|██████████| 16/16 [00:04<00:00,  3.57it/s]\n",
            "02/04/2022 16:04:21 - INFO - trainer -   id_test: 2.12928    time_test: 0.80801  total_loss: 2.93729\n",
            "epoch 49  id_train: 2.12057  time_train: 0.80229  total_loss: 2.92285 lr 7.931678e-05: 100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n",
            "02/04/2022 16:04:31 - INFO - trainer -   id_test: 2.09136    time_test: 0.80890  total_loss: 2.90026\n",
            "02/04/2022 16:04:31 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 50  id_train: 2.05030  time_train: 0.83611  total_loss: 2.88641 lr 7.928749e-05: 100%|██████████| 16/16 [00:08<00:00,  1.85it/s]\n",
            "02/04/2022 16:04:41 - INFO - trainer -   id_test: 2.09158    time_test: 0.81501  total_loss: 2.90658\n",
            "epoch 51  id_train: 2.05374  time_train: 0.82536  total_loss: 2.87911 lr 7.925760e-05: 100%|██████████| 16/16 [00:08<00:00,  1.86it/s]\n",
            "02/04/2022 16:04:50 - INFO - trainer -   id_test: 2.10391    time_test: 0.80610  total_loss: 2.91002\n",
            "epoch 52  id_train: 2.08749  time_train: 0.79580  total_loss: 2.88329 lr 7.922710e-05: 100%|██████████| 16/16 [00:04<00:00,  3.55it/s]\n",
            "02/04/2022 16:04:56 - INFO - trainer -   id_test: 2.10882    time_test: 0.80786  total_loss: 2.91668\n",
            "epoch 53  id_train: 2.09268  time_train: 0.81170  total_loss: 2.90438 lr 7.919599e-05: 100%|██████████| 16/16 [00:04<00:00,  3.60it/s]\n",
            "02/04/2022 16:05:01 - INFO - trainer -   id_test: 2.09748    time_test: 0.82189  total_loss: 2.91938\n",
            "epoch 54  id_train: 2.04197  time_train: 0.80115  total_loss: 2.84312 lr 7.916427e-05: 100%|██████████| 16/16 [00:04<00:00,  3.59it/s]\n",
            "02/04/2022 16:05:07 - INFO - trainer -   id_test: 2.09437    time_test: 0.81793  total_loss: 2.91230\n",
            "epoch 55  id_train: 2.14978  time_train: 0.81007  total_loss: 2.95986 lr 7.913195e-05: 100%|██████████| 16/16 [00:04<00:00,  3.58it/s]\n",
            "02/04/2022 16:05:12 - INFO - trainer -   id_test: 2.06813    time_test: 0.82333  total_loss: 2.89146\n",
            "02/04/2022 16:05:12 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 56  id_train: 2.06663  time_train: 0.81409  total_loss: 2.88073 lr 7.909902e-05: 100%|██████████| 16/16 [00:04<00:00,  3.62it/s]\n",
            "02/04/2022 16:05:17 - INFO - trainer -   id_test: 2.10421    time_test: 0.83010  total_loss: 2.93431\n",
            "epoch 57  id_train: 2.11374  time_train: 0.83039  total_loss: 2.94413 lr 7.906548e-05: 100%|██████████| 16/16 [00:04<00:00,  3.62it/s]\n",
            "02/04/2022 16:05:23 - INFO - trainer -   id_test: 2.14182    time_test: 0.80444  total_loss: 2.94626\n",
            "epoch 58  id_train: 2.01218  time_train: 0.81947  total_loss: 2.83165 lr 7.903134e-05: 100%|██████████| 16/16 [00:04<00:00,  3.59it/s]\n",
            "02/04/2022 16:05:28 - INFO - trainer -   id_test: 2.12757    time_test: 0.79542  total_loss: 2.92299\n",
            "epoch 59  id_train: 2.13544  time_train: 0.79952  total_loss: 2.93496 lr 7.899660e-05: 100%|██████████| 16/16 [00:04<00:00,  3.55it/s]\n",
            "02/04/2022 16:05:34 - INFO - trainer -   id_test: 2.11340    time_test: 0.78670  total_loss: 2.90010\n",
            "epoch 60  id_train: 2.02048  time_train: 0.83205  total_loss: 2.85253 lr 7.896125e-05: 100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n",
            "02/04/2022 16:05:43 - INFO - trainer -   id_test: 2.07511    time_test: 0.80913  total_loss: 2.88425\n",
            "02/04/2022 16:05:44 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 61  id_train: 2.11953  time_train: 0.76935  total_loss: 2.88889 lr 7.892529e-05: 100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n",
            "02/04/2022 16:05:53 - INFO - trainer -   id_test: 2.06786    time_test: 0.81114  total_loss: 2.87900\n",
            "02/04/2022 16:05:53 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 62  id_train: 2.06380  time_train: 0.79421  total_loss: 2.85801 lr 7.888873e-05: 100%|██████████| 16/16 [00:08<00:00,  1.86it/s]\n",
            "02/04/2022 16:06:03 - INFO - trainer -   id_test: 2.07059    time_test: 0.81375  total_loss: 2.88434\n",
            "epoch 63  id_train: 2.08191  time_train: 0.81027  total_loss: 2.89217 lr 7.885157e-05: 100%|██████████| 16/16 [00:04<00:00,  3.63it/s]\n",
            "02/04/2022 16:06:08 - INFO - trainer -   id_test: 2.08391    time_test: 0.80032  total_loss: 2.88424\n",
            "epoch 64  id_train: 2.10302  time_train: 0.79150  total_loss: 2.89451 lr 7.881381e-05: 100%|██████████| 16/16 [00:04<00:00,  3.66it/s]\n",
            "02/04/2022 16:06:14 - INFO - trainer -   id_test: 2.10605    time_test: 0.79970  total_loss: 2.90575\n",
            "epoch 65  id_train: 2.08895  time_train: 0.79072  total_loss: 2.87967 lr 7.877545e-05: 100%|██████████| 16/16 [00:04<00:00,  3.58it/s]\n",
            "02/04/2022 16:06:19 - INFO - trainer -   id_test: 2.12418    time_test: 0.78907  total_loss: 2.91325\n",
            "epoch 66  id_train: 2.10944  time_train: 0.81084  total_loss: 2.92028 lr 7.873648e-05: 100%|██████████| 16/16 [00:04<00:00,  3.64it/s]\n",
            "02/04/2022 16:06:24 - INFO - trainer -   id_test: 2.12715    time_test: 0.80581  total_loss: 2.93296\n",
            "epoch 67  id_train: 2.00769  time_train: 0.83324  total_loss: 2.84093 lr 7.869692e-05: 100%|██████████| 16/16 [00:04<00:00,  3.63it/s]\n",
            "02/04/2022 16:06:30 - INFO - trainer -   id_test: 2.08757    time_test: 0.78586  total_loss: 2.87343\n",
            "02/04/2022 16:06:30 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 68  id_train: 2.10314  time_train: 0.77352  total_loss: 2.87666 lr 7.865675e-05: 100%|██████████| 16/16 [00:04<00:00,  3.64it/s]\n",
            "02/04/2022 16:06:35 - INFO - trainer -   id_test: 2.11052    time_test: 0.79809  total_loss: 2.90861\n",
            "epoch 69  id_train: 1.97147  time_train: 0.82405  total_loss: 2.79553 lr 7.861599e-05: 100%|██████████| 16/16 [00:07<00:00,  2.06it/s]\n",
            "02/04/2022 16:06:44 - INFO - trainer -   id_test: 2.07126    time_test: 0.80251  total_loss: 2.87376\n",
            "epoch 70  id_train: 2.14181  time_train: 0.76453  total_loss: 2.90635 lr 7.857463e-05: 100%|██████████| 16/16 [00:08<00:00,  1.85it/s]\n",
            "02/04/2022 16:06:54 - INFO - trainer -   id_test: 2.08685    time_test: 0.79483  total_loss: 2.88169\n",
            "epoch 71  id_train: 2.06562  time_train: 0.80003  total_loss: 2.86565 lr 7.853267e-05: 100%|██████████| 16/16 [00:08<00:00,  1.86it/s]\n",
            "02/04/2022 16:07:03 - INFO - trainer -   id_test: 2.10113    time_test: 0.79483  total_loss: 2.89596\n",
            "epoch 72  id_train: 2.20803  time_train: 0.78073  total_loss: 2.98876 lr 7.849011e-05: 100%|██████████| 16/16 [00:07<00:00,  2.05it/s]\n",
            "02/04/2022 16:07:12 - INFO - trainer -   id_test: 2.11259    time_test: 0.79065  total_loss: 2.90325\n",
            "epoch 73  id_train: 2.07179  time_train: 0.79028  total_loss: 2.86207 lr 7.844696e-05: 100%|██████████| 16/16 [00:04<00:00,  3.58it/s]\n",
            "02/04/2022 16:07:17 - INFO - trainer -   id_test: 2.10187    time_test: 0.80099  total_loss: 2.90286\n",
            "epoch 74  id_train: 2.07801  time_train: 0.82126  total_loss: 2.89927 lr 7.840321e-05: 100%|██████████| 16/16 [00:04<00:00,  3.57it/s]\n",
            "02/04/2022 16:07:23 - INFO - trainer -   id_test: 2.12484    time_test: 0.77278  total_loss: 2.89762\n",
            "epoch 75  id_train: 2.01900  time_train: 0.82283  total_loss: 2.84183 lr 7.835886e-05: 100%|██████████| 16/16 [00:04<00:00,  3.63it/s]\n",
            "02/04/2022 16:07:28 - INFO - trainer -   id_test: 2.05315    time_test: 0.81444  total_loss: 2.86760\n",
            "02/04/2022 16:07:28 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 76  id_train: 1.98565  time_train: 0.82440  total_loss: 2.81004 lr 7.831392e-05: 100%|██████████| 16/16 [00:04<00:00,  3.61it/s]\n",
            "02/04/2022 16:07:33 - INFO - trainer -   id_test: 2.13189    time_test: 0.78482  total_loss: 2.91671\n",
            "epoch 77  id_train: 2.11079  time_train: 0.80171  total_loss: 2.91250 lr 7.826839e-05: 100%|██████████| 16/16 [00:04<00:00,  3.61it/s]\n",
            "02/04/2022 16:07:39 - INFO - trainer -   id_test: 2.14252    time_test: 0.78386  total_loss: 2.92638\n",
            "epoch 78  id_train: 2.07485  time_train: 0.79225  total_loss: 2.86710 lr 7.822226e-05: 100%|██████████| 16/16 [00:04<00:00,  3.62it/s]\n",
            "02/04/2022 16:07:44 - INFO - trainer -   id_test: 2.08578    time_test: 0.80188  total_loss: 2.88766\n",
            "epoch 79  id_train: 2.06665  time_train: 0.82036  total_loss: 2.88701 lr 7.817555e-05: 100%|██████████| 16/16 [00:04<00:00,  3.65it/s]\n",
            "02/04/2022 16:07:49 - INFO - trainer -   id_test: 2.12481    time_test: 0.78573  total_loss: 2.91054\n",
            "epoch 80  id_train: 2.10400  time_train: 0.79956  total_loss: 2.90356 lr 7.812824e-05: 100%|██████████| 16/16 [00:04<00:00,  3.53it/s]\n",
            "02/04/2022 16:07:55 - INFO - trainer -   id_test: 2.10193    time_test: 0.77754  total_loss: 2.87947\n",
            "epoch 81  id_train: 2.06678  time_train: 0.82800  total_loss: 2.89478 lr 7.808034e-05: 100%|██████████| 16/16 [00:04<00:00,  3.63it/s]\n",
            "02/04/2022 16:08:00 - INFO - trainer -   id_test: 2.14111    time_test: 0.79222  total_loss: 2.93333\n",
            "epoch 82  id_train: 1.96667  time_train: 0.81078  total_loss: 2.77745 lr 7.803185e-05: 100%|██████████| 16/16 [00:04<00:00,  3.57it/s]\n",
            "02/04/2022 16:08:05 - INFO - trainer -   id_test: 2.11829    time_test: 0.81119  total_loss: 2.92948\n",
            "epoch 83  id_train: 2.07507  time_train: 0.81102  total_loss: 2.88609 lr 7.798277e-05: 100%|██████████| 16/16 [00:04<00:00,  3.65it/s]\n",
            "02/04/2022 16:08:11 - INFO - trainer -   id_test: 2.10393    time_test: 0.77438  total_loss: 2.87830\n",
            "epoch 84  id_train: 2.02459  time_train: 0.80973  total_loss: 2.83432 lr 7.793310e-05: 100%|██████████| 16/16 [00:04<00:00,  3.57it/s]\n",
            "02/04/2022 16:08:16 - INFO - trainer -   id_test: 2.10714    time_test: 0.79440  total_loss: 2.90154\n",
            "epoch 85  id_train: 2.07793  time_train: 0.79530  total_loss: 2.87323 lr 7.788284e-05: 100%|██████████| 16/16 [00:04<00:00,  3.62it/s]\n",
            "02/04/2022 16:08:21 - INFO - trainer -   id_test: 2.06972    time_test: 0.81646  total_loss: 2.88618\n",
            "epoch 86  id_train: 1.95754  time_train: 0.82737  total_loss: 2.78490 lr 7.783200e-05: 100%|██████████| 16/16 [00:04<00:00,  3.56it/s]\n",
            "02/04/2022 16:08:27 - INFO - trainer -   id_test: 2.10048    time_test: 0.79371  total_loss: 2.89419\n",
            "epoch 87  id_train: 2.04274  time_train: 0.81090  total_loss: 2.85365 lr 7.778057e-05: 100%|██████████| 16/16 [00:04<00:00,  3.55it/s]\n",
            "02/04/2022 16:08:32 - INFO - trainer -   id_test: 2.11381    time_test: 0.78243  total_loss: 2.89624\n",
            "epoch 88  id_train: 2.04995  time_train: 0.77814  total_loss: 2.82808 lr 7.772855e-05: 100%|██████████| 16/16 [00:04<00:00,  3.62it/s]\n",
            "02/04/2022 16:08:37 - INFO - trainer -   id_test: 2.10147    time_test: 0.81161  total_loss: 2.91308\n",
            "epoch 89  id_train: 2.02354  time_train: 0.78182  total_loss: 2.80536 lr 7.767595e-05: 100%|██████████| 16/16 [00:04<00:00,  3.62it/s]\n",
            "02/04/2022 16:08:43 - INFO - trainer -   id_test: 2.09276    time_test: 0.79910  total_loss: 2.89186\n",
            "epoch 90  id_train: 2.01220  time_train: 0.79145  total_loss: 2.80365 lr 7.762277e-05: 100%|██████████| 16/16 [00:04<00:00,  3.60it/s]\n",
            "02/04/2022 16:08:48 - INFO - trainer -   id_test: 2.12863    time_test: 0.78835  total_loss: 2.91698\n",
            "epoch 91  id_train: 2.00068  time_train: 0.82379  total_loss: 2.82447 lr 7.756900e-05: 100%|██████████| 16/16 [00:04<00:00,  3.60it/s]\n",
            "02/04/2022 16:08:53 - INFO - trainer -   id_test: 2.11791    time_test: 0.78026  total_loss: 2.89816\n",
            "epoch 92  id_train: 2.05545  time_train: 0.81889  total_loss: 2.87434 lr 7.751465e-05: 100%|██████████| 16/16 [00:04<00:00,  3.63it/s]\n",
            "02/04/2022 16:08:59 - INFO - trainer -   id_test: 2.14621    time_test: 0.79036  total_loss: 2.93657\n",
            "epoch 93  id_train: 2.06685  time_train: 0.76429  total_loss: 2.83114 lr 7.745972e-05: 100%|██████████| 16/16 [00:04<00:00,  3.65it/s]\n",
            "02/04/2022 16:09:04 - INFO - trainer -   id_test: 2.06660    time_test: 0.79608  total_loss: 2.86268\n",
            "02/04/2022 16:09:04 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 94  id_train: 2.00228  time_train: 0.79946  total_loss: 2.80174 lr 7.740421e-05: 100%|██████████| 16/16 [00:04<00:00,  3.62it/s]\n",
            "02/04/2022 16:09:09 - INFO - trainer -   id_test: 2.14809    time_test: 0.79697  total_loss: 2.94507\n",
            "epoch 95  id_train: 2.02380  time_train: 0.80557  total_loss: 2.82937 lr 7.734812e-05: 100%|██████████| 16/16 [00:04<00:00,  3.60it/s]\n",
            "02/04/2022 16:09:15 - INFO - trainer -   id_test: 2.10190    time_test: 0.79637  total_loss: 2.89827\n",
            "epoch 96  id_train: 2.01948  time_train: 0.77643  total_loss: 2.79590 lr 7.729145e-05: 100%|██████████| 16/16 [00:04<00:00,  3.64it/s]\n",
            "02/04/2022 16:09:20 - INFO - trainer -   id_test: 2.06599    time_test: 0.78401  total_loss: 2.85000\n",
            "02/04/2022 16:09:20 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 97  id_train: 2.03908  time_train: 0.78326  total_loss: 2.82234 lr 7.723420e-05: 100%|██████████| 16/16 [00:04<00:00,  3.57it/s]\n",
            "02/04/2022 16:09:26 - INFO - trainer -   id_test: 2.08214    time_test: 0.80247  total_loss: 2.88461\n",
            "epoch 98  id_train: 2.04358  time_train: 0.76993  total_loss: 2.81351 lr 7.717638e-05: 100%|██████████| 16/16 [00:04<00:00,  3.60it/s]\n",
            "02/04/2022 16:09:31 - INFO - trainer -   id_test: 2.07107    time_test: 0.78497  total_loss: 2.85604\n",
            "epoch 99  id_train: 1.99754  time_train: 0.80138  total_loss: 2.79892 lr 7.711797e-05: 100%|██████████| 16/16 [00:04<00:00,  3.59it/s]\n",
            "02/04/2022 16:09:36 - INFO - trainer -   id_test: 2.08826    time_test: 0.78745  total_loss: 2.87571\n",
            "epoch 100  id_train: 2.07851  time_train: 0.74514  total_loss: 2.82365 lr 7.705900e-05: 100%|██████████| 16/16 [00:04<00:00,  3.62it/s]\n",
            "02/04/2022 16:09:42 - INFO - trainer -   id_test: 2.06582    time_test: 0.80382  total_loss: 2.86963\n",
            "epoch 101  id_train: 2.04189  time_train: 0.76423  total_loss: 2.80613 lr 7.699945e-05: 100%|██████████| 16/16 [00:04<00:00,  3.61it/s]\n",
            "02/04/2022 16:09:47 - INFO - trainer -   id_test: 2.13169    time_test: 0.77210  total_loss: 2.90380\n",
            "epoch 102  id_train: 2.06471  time_train: 0.80057  total_loss: 2.86528 lr 7.693932e-05: 100%|██████████| 16/16 [00:04<00:00,  3.68it/s]\n",
            "02/04/2022 16:09:52 - INFO - trainer -   id_test: 2.10593    time_test: 0.78701  total_loss: 2.89294\n",
            "epoch 103  id_train: 2.04211  time_train: 0.79407  total_loss: 2.83618 lr 7.687863e-05: 100%|██████████| 16/16 [00:04<00:00,  3.64it/s]\n",
            "02/04/2022 16:09:57 - INFO - trainer -   id_test: 2.04869    time_test: 0.80939  total_loss: 2.85808\n",
            "epoch 104  id_train: 2.02615  time_train: 0.81163  total_loss: 2.83778 lr 7.681736e-05: 100%|██████████| 16/16 [00:04<00:00,  3.67it/s]\n",
            "02/04/2022 16:10:03 - INFO - trainer -   id_test: 2.05985    time_test: 0.79752  total_loss: 2.85737\n",
            "epoch 105  id_train: 2.13098  time_train: 0.75871  total_loss: 2.88970 lr 7.675552e-05: 100%|██████████| 16/16 [00:04<00:00,  3.62it/s]\n",
            "02/04/2022 16:10:08 - INFO - trainer -   id_test: 2.10552    time_test: 0.78853  total_loss: 2.89405\n",
            "epoch 106  id_train: 2.11066  time_train: 0.81475  total_loss: 2.92541 lr 7.669312e-05: 100%|██████████| 16/16 [00:04<00:00,  3.59it/s]\n",
            "02/04/2022 16:10:13 - INFO - trainer -   id_test: 2.08296    time_test: 0.77703  total_loss: 2.86000\n",
            "epoch 107  id_train: 2.06260  time_train: 0.78778  total_loss: 2.85038 lr 7.663014e-05: 100%|██████████| 16/16 [00:04<00:00,  3.63it/s]\n",
            "02/04/2022 16:10:19 - INFO - trainer -   id_test: 2.10523    time_test: 0.77449  total_loss: 2.87972\n",
            "epoch 108  id_train: 1.99501  time_train: 0.78802  total_loss: 2.78302 lr 7.656659e-05: 100%|██████████| 16/16 [00:04<00:00,  3.61it/s]\n",
            "02/04/2022 16:10:24 - INFO - trainer -   id_test: 2.11337    time_test: 0.78621  total_loss: 2.89959\n",
            "epoch 109  id_train: 2.13694  time_train: 0.76382  total_loss: 2.90076 lr 7.650248e-05: 100%|██████████| 16/16 [00:04<00:00,  3.64it/s]\n",
            "02/04/2022 16:10:29 - INFO - trainer -   id_test: 2.06409    time_test: 0.79000  total_loss: 2.85409\n",
            "epoch 110  id_train: 2.05798  time_train: 0.76217  total_loss: 2.82015 lr 7.643781e-05: 100%|██████████| 16/16 [00:04<00:00,  3.64it/s]\n",
            "02/04/2022 16:10:35 - INFO - trainer -   id_test: 2.05111    time_test: 0.79494  total_loss: 2.84604\n",
            "02/04/2022 16:10:35 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 111  id_train: 1.98784  time_train: 0.79872  total_loss: 2.78656 lr 7.637257e-05: 100%|██████████| 16/16 [00:04<00:00,  3.63it/s]\n",
            "02/04/2022 16:10:40 - INFO - trainer -   id_test: 2.08960    time_test: 0.78631  total_loss: 2.87591\n",
            "epoch 112  id_train: 2.04417  time_train: 0.78627  total_loss: 2.83045 lr 7.630676e-05: 100%|██████████| 16/16 [00:04<00:00,  3.64it/s]\n",
            "02/04/2022 16:10:45 - INFO - trainer -   id_test: 2.12586    time_test: 0.77821  total_loss: 2.90407\n",
            "epoch 113  id_train: 1.95555  time_train: 0.80591  total_loss: 2.76146 lr 7.624039e-05: 100%|██████████| 16/16 [00:04<00:00,  3.66it/s]\n",
            "02/04/2022 16:10:51 - INFO - trainer -   id_test: 2.08664    time_test: 0.77920  total_loss: 2.86584\n",
            "epoch 114  id_train: 1.98143  time_train: 0.80540  total_loss: 2.78683 lr 7.617347e-05: 100%|██████████| 16/16 [00:04<00:00,  3.66it/s]\n",
            "02/04/2022 16:10:56 - INFO - trainer -   id_test: 2.11150    time_test: 0.78412  total_loss: 2.89562\n",
            "epoch 115  id_train: 2.07247  time_train: 0.76068  total_loss: 2.83315 lr 7.610598e-05: 100%|██████████| 16/16 [00:04<00:00,  3.60it/s]\n",
            "02/04/2022 16:11:01 - INFO - trainer -   id_test: 2.12158    time_test: 0.78366  total_loss: 2.90524\n",
            "epoch 116  id_train: 2.02204  time_train: 0.77865  total_loss: 2.80069 lr 7.603793e-05: 100%|██████████| 16/16 [00:04<00:00,  3.61it/s]\n",
            "02/04/2022 16:11:07 - INFO - trainer -   id_test: 2.12732    time_test: 0.79044  total_loss: 2.91776\n",
            "epoch 117  id_train: 2.04401  time_train: 0.74787  total_loss: 2.79188 lr 7.596932e-05: 100%|██████████| 16/16 [00:04<00:00,  3.67it/s]\n",
            "02/04/2022 16:11:12 - INFO - trainer -   id_test: 2.07959    time_test: 0.77920  total_loss: 2.85879\n",
            "epoch 118  id_train: 2.04507  time_train: 0.81701  total_loss: 2.86208 lr 7.590015e-05: 100%|██████████| 16/16 [00:04<00:00,  3.61it/s]\n",
            "02/04/2022 16:11:17 - INFO - trainer -   id_test: 2.07760    time_test: 0.79423  total_loss: 2.87183\n",
            "epoch 119  id_train: 2.10171  time_train: 0.79529  total_loss: 2.89700 lr 7.583043e-05: 100%|██████████| 16/16 [00:04<00:00,  3.58it/s]\n",
            "02/04/2022 16:11:23 - INFO - trainer -   id_test: 2.06744    time_test: 0.78977  total_loss: 2.85721\n",
            "epoch 120  id_train: 2.00587  time_train: 0.81096  total_loss: 2.81683 lr 7.576016e-05: 100%|██████████| 16/16 [00:04<00:00,  3.68it/s]\n",
            "02/04/2022 16:11:28 - INFO - trainer -   id_test: 2.13694    time_test: 0.78713  total_loss: 2.92407\n",
            "epoch 121  id_train: 2.00168  time_train: 0.79390  total_loss: 2.79557 lr 7.568932e-05: 100%|██████████| 16/16 [00:04<00:00,  3.56it/s]\n",
            "02/04/2022 16:11:33 - INFO - trainer -   id_test: 2.09716    time_test: 0.78768  total_loss: 2.88484\n",
            "epoch 122  id_train: 1.99508  time_train: 0.78963  total_loss: 2.78472 lr 7.561794e-05: 100%|██████████| 16/16 [00:04<00:00,  3.68it/s]\n",
            "02/04/2022 16:11:38 - INFO - trainer -   id_test: 2.13397    time_test: 0.76500  total_loss: 2.89897\n",
            "epoch 123  id_train: 2.01940  time_train: 0.79737  total_loss: 2.81677 lr 7.554600e-05: 100%|██████████| 16/16 [00:04<00:00,  3.65it/s]\n",
            "02/04/2022 16:11:44 - INFO - trainer -   id_test: 2.08290    time_test: 0.79803  total_loss: 2.88093\n",
            "epoch 124  id_train: 2.04651  time_train: 0.83215  total_loss: 2.87866 lr 7.547352e-05: 100%|██████████| 16/16 [00:04<00:00,  3.64it/s]\n",
            "02/04/2022 16:11:49 - INFO - trainer -   id_test: 2.10800    time_test: 0.79193  total_loss: 2.89992\n",
            "epoch 125  id_train: 2.03377  time_train: 0.78416  total_loss: 2.81794 lr 7.540048e-05: 100%|██████████| 16/16 [00:04<00:00,  3.66it/s]\n",
            "02/04/2022 16:11:54 - INFO - trainer -   id_test: 2.11216    time_test: 0.77952  total_loss: 2.89168\n",
            "epoch 126  id_train: 2.11394  time_train: 0.77149  total_loss: 2.88543 lr 7.532690e-05: 100%|██████████| 16/16 [00:04<00:00,  3.66it/s]\n",
            "02/04/2022 16:12:00 - INFO - trainer -   id_test: 2.08157    time_test: 0.79202  total_loss: 2.87359\n",
            "epoch 127  id_train: 2.04948  time_train: 0.75385  total_loss: 2.80334 lr 7.525276e-05: 100%|██████████| 16/16 [00:04<00:00,  3.64it/s]\n",
            "02/04/2022 16:12:05 - INFO - trainer -   id_test: 2.10958    time_test: 0.77366  total_loss: 2.88323\n",
            "epoch 128  id_train: 2.04306  time_train: 0.79344  total_loss: 2.83650 lr 7.517808e-05: 100%|██████████| 16/16 [00:04<00:00,  3.68it/s]\n",
            "02/04/2022 16:12:10 - INFO - trainer -   id_test: 2.08795    time_test: 0.79969  total_loss: 2.88764\n",
            "epoch 129  id_train: 2.01040  time_train: 0.79960  total_loss: 2.81000 lr 7.510286e-05: 100%|██████████| 16/16 [00:04<00:00,  3.64it/s]\n",
            "02/04/2022 16:12:15 - INFO - trainer -   id_test: 2.06640    time_test: 0.80209  total_loss: 2.86849\n",
            "epoch 130  id_train: 1.99997  time_train: 0.78997  total_loss: 2.78994 lr 7.502709e-05: 100%|██████████| 16/16 [00:04<00:00,  3.62it/s]\n",
            "02/04/2022 16:12:21 - INFO - trainer -   id_test: 2.09476    time_test: 0.77029  total_loss: 2.86506\n",
            "epoch 131  id_train: 2.04160  time_train: 0.78832  total_loss: 2.82992 lr 7.495078e-05: 100%|██████████| 16/16 [00:04<00:00,  3.61it/s]\n",
            "02/04/2022 16:12:26 - INFO - trainer -   id_test: 2.05063    time_test: 0.80738  total_loss: 2.85801\n",
            "epoch 132  id_train: 2.04328  time_train: 0.77545  total_loss: 2.81873 lr 7.487393e-05: 100%|██████████| 16/16 [00:04<00:00,  3.62it/s]\n",
            "02/04/2022 16:12:31 - INFO - trainer -   id_test: 2.09148    time_test: 0.79187  total_loss: 2.88336\n",
            "epoch 133  id_train: 2.05010  time_train: 0.78859  total_loss: 2.83868 lr 7.479654e-05: 100%|██████████| 16/16 [00:04<00:00,  3.60it/s]\n",
            "02/04/2022 16:12:37 - INFO - trainer -   id_test: 2.11994    time_test: 0.77103  total_loss: 2.89097\n",
            "epoch 134  id_train: 1.96745  time_train: 0.78511  total_loss: 2.75256 lr 7.471860e-05: 100%|██████████| 16/16 [00:04<00:00,  3.65it/s]\n",
            "02/04/2022 16:12:42 - INFO - trainer -   id_test: 2.11964    time_test: 0.78169  total_loss: 2.90134\n",
            "epoch 135  id_train: 2.03070  time_train: 0.75317  total_loss: 2.78387 lr 7.464013e-05: 100%|██████████| 16/16 [00:04<00:00,  3.63it/s]\n",
            "02/04/2022 16:12:47 - INFO - trainer -   id_test: 2.11867    time_test: 0.80107  total_loss: 2.91974\n",
            "epoch 136  id_train: 2.01568  time_train: 0.78051  total_loss: 2.79619 lr 7.456113e-05: 100%|██████████| 16/16 [00:04<00:00,  3.65it/s]\n",
            "02/04/2022 16:12:53 - INFO - trainer -   id_test: 2.11268    time_test: 0.78136  total_loss: 2.89403\n",
            "epoch 137  id_train: 2.03262  time_train: 0.76831  total_loss: 2.80094 lr 7.448158e-05: 100%|██████████| 16/16 [00:04<00:00,  3.61it/s]\n",
            "02/04/2022 16:12:58 - INFO - trainer -   id_test: 2.08435    time_test: 0.79704  total_loss: 2.88138\n",
            "epoch 138  id_train: 1.94294  time_train: 0.78859  total_loss: 2.73153 lr 7.440151e-05: 100%|██████████| 16/16 [00:04<00:00,  3.59it/s]\n",
            "02/04/2022 16:13:03 - INFO - trainer -   id_test: 2.07523    time_test: 0.77601  total_loss: 2.85124\n",
            "epoch 139  id_train: 1.97382  time_train: 0.79054  total_loss: 2.76436 lr 7.432090e-05: 100%|██████████| 16/16 [00:04<00:00,  3.64it/s]\n",
            "02/04/2022 16:13:09 - INFO - trainer -   id_test: 2.10123    time_test: 0.78916  total_loss: 2.89038\n",
            "epoch 140  id_train: 2.08593  time_train: 0.74551  total_loss: 2.83143 lr 7.423975e-05: 100%|██████████| 16/16 [00:04<00:00,  3.61it/s]\n",
            "02/04/2022 16:13:14 - INFO - trainer -   id_test: 2.12142    time_test: 0.75286  total_loss: 2.87428\n",
            "epoch 141  id_train: 2.06258  time_train: 0.78235  total_loss: 2.84493 lr 7.415808e-05: 100%|██████████| 16/16 [00:04<00:00,  3.65it/s]\n",
            "02/04/2022 16:13:19 - INFO - trainer -   id_test: 2.11659    time_test: 0.78048  total_loss: 2.89707\n",
            "epoch 142  id_train: 2.01635  time_train: 0.80523  total_loss: 2.82159 lr 7.407588e-05: 100%|██████████| 16/16 [00:04<00:00,  3.63it/s]\n",
            "02/04/2022 16:13:24 - INFO - trainer -   id_test: 2.06651    time_test: 0.78674  total_loss: 2.85325\n",
            "epoch 143  id_train: 2.04102  time_train: 0.77920  total_loss: 2.82022 lr 7.399315e-05: 100%|██████████| 16/16 [00:04<00:00,  3.55it/s]\n",
            "02/04/2022 16:13:30 - INFO - trainer -   id_test: 2.10194    time_test: 0.78696  total_loss: 2.88890\n",
            "epoch 144  id_train: 2.00703  time_train: 0.80868  total_loss: 2.81571 lr 7.390989e-05: 100%|██████████| 16/16 [00:04<00:00,  3.57it/s]\n",
            "02/04/2022 16:13:35 - INFO - trainer -   id_test: 2.09668    time_test: 0.78981  total_loss: 2.88649\n",
            "epoch 145  id_train: 2.05684  time_train: 0.75922  total_loss: 2.81605 lr 7.382611e-05: 100%|██████████| 16/16 [00:04<00:00,  3.61it/s]\n",
            "02/04/2022 16:13:41 - INFO - trainer -   id_test: 2.11937    time_test: 0.79137  total_loss: 2.91074\n",
            "epoch 146  id_train: 2.11389  time_train: 0.74787  total_loss: 2.86175 lr 7.374180e-05: 100%|██████████| 16/16 [00:04<00:00,  3.68it/s]\n",
            "02/04/2022 16:13:46 - INFO - trainer -   id_test: 2.10312    time_test: 0.80127  total_loss: 2.90439\n",
            "epoch 147  id_train: 2.07121  time_train: 0.76511  total_loss: 2.83632 lr 7.365697e-05: 100%|██████████| 16/16 [00:07<00:00,  2.04it/s]\n",
            "02/04/2022 16:13:55 - INFO - trainer -   id_test: 2.08104    time_test: 0.79025  total_loss: 2.87129\n",
            "epoch 148  id_train: 2.05339  time_train: 0.76186  total_loss: 2.81525 lr 7.357162e-05: 100%|██████████| 16/16 [00:09<00:00,  1.78it/s]\n",
            "02/04/2022 16:14:05 - INFO - trainer -   id_test: 2.08215    time_test: 0.78989  total_loss: 2.87204\n",
            "epoch 149  id_train: 1.99911  time_train: 0.78659  total_loss: 2.78570 lr 7.348575e-05: 100%|██████████| 16/16 [00:04<00:00,  3.30it/s]\n",
            "02/04/2022 16:14:11 - INFO - trainer -   id_test: 2.11258    time_test: 0.78354  total_loss: 2.89611\n",
            "epoch 150  id_train: 1.95322  time_train: 0.78191  total_loss: 2.73513 lr 7.339936e-05: 100%|██████████| 16/16 [00:04<00:00,  3.34it/s]\n",
            "02/04/2022 16:14:16 - INFO - trainer -   id_test: 2.05637    time_test: 0.80378  total_loss: 2.86015\n",
            "epoch 151  id_train: 2.00727  time_train: 0.78057  total_loss: 2.78785 lr 7.331245e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:14:22 - INFO - trainer -   id_test: 2.11429    time_test: 0.77819  total_loss: 2.89248\n",
            "epoch 152  id_train: 2.02551  time_train: 0.78731  total_loss: 2.81282 lr 7.322503e-05: 100%|██████████| 16/16 [00:04<00:00,  3.42it/s]\n",
            "02/04/2022 16:14:27 - INFO - trainer -   id_test: 2.06706    time_test: 0.77678  total_loss: 2.84384\n",
            "02/04/2022 16:14:27 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 153  id_train: 1.97376  time_train: 0.77747  total_loss: 2.75123 lr 7.313709e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:14:33 - INFO - trainer -   id_test: 2.06717    time_test: 0.80000  total_loss: 2.86717\n",
            "epoch 154  id_train: 1.99414  time_train: 0.78838  total_loss: 2.78252 lr 7.304864e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:14:39 - INFO - trainer -   id_test: 2.09140    time_test: 0.78672  total_loss: 2.87812\n",
            "epoch 155  id_train: 2.01390  time_train: 0.79253  total_loss: 2.80643 lr 7.295967e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:14:44 - INFO - trainer -   id_test: 2.09858    time_test: 0.80329  total_loss: 2.90187\n",
            "epoch 156  id_train: 2.02349  time_train: 0.77668  total_loss: 2.80017 lr 7.287020e-05: 100%|██████████| 16/16 [00:04<00:00,  3.54it/s]\n",
            "02/04/2022 16:14:50 - INFO - trainer -   id_test: 2.07046    time_test: 0.80139  total_loss: 2.87185\n",
            "epoch 157  id_train: 2.04372  time_train: 0.77858  total_loss: 2.82230 lr 7.278021e-05: 100%|██████████| 16/16 [00:04<00:00,  3.56it/s]\n",
            "02/04/2022 16:14:55 - INFO - trainer -   id_test: 2.08799    time_test: 0.78161  total_loss: 2.86960\n",
            "epoch 158  id_train: 2.02107  time_train: 0.78286  total_loss: 2.80392 lr 7.268972e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:15:00 - INFO - trainer -   id_test: 2.06322    time_test: 0.78113  total_loss: 2.84435\n",
            "epoch 159  id_train: 1.99963  time_train: 0.78398  total_loss: 2.78361 lr 7.259872e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:15:06 - INFO - trainer -   id_test: 2.13580    time_test: 0.80749  total_loss: 2.94329\n",
            "epoch 160  id_train: 1.94750  time_train: 0.81883  total_loss: 2.76633 lr 7.250722e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:15:11 - INFO - trainer -   id_test: 2.09775    time_test: 0.77917  total_loss: 2.87692\n",
            "epoch 161  id_train: 1.99617  time_train: 0.76913  total_loss: 2.76530 lr 7.241521e-05: 100%|██████████| 16/16 [00:04<00:00,  3.39it/s]\n",
            "02/04/2022 16:15:17 - INFO - trainer -   id_test: 2.09555    time_test: 0.77193  total_loss: 2.86748\n",
            "epoch 162  id_train: 2.08730  time_train: 0.72549  total_loss: 2.81279 lr 7.232270e-05: 100%|██████████| 16/16 [00:04<00:00,  3.39it/s]\n",
            "02/04/2022 16:15:23 - INFO - trainer -   id_test: 2.12674    time_test: 0.78294  total_loss: 2.90967\n",
            "epoch 163  id_train: 2.01894  time_train: 0.77947  total_loss: 2.79841 lr 7.222968e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:15:28 - INFO - trainer -   id_test: 2.06260    time_test: 0.77753  total_loss: 2.84013\n",
            "02/04/2022 16:15:28 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 164  id_train: 1.99639  time_train: 0.78617  total_loss: 2.78256 lr 7.213617e-05: 100%|██████████| 16/16 [00:04<00:00,  3.52it/s]\n",
            "02/04/2022 16:15:34 - INFO - trainer -   id_test: 2.10947    time_test: 0.78831  total_loss: 2.89778\n",
            "epoch 165  id_train: 1.95313  time_train: 0.80906  total_loss: 2.76219 lr 7.204216e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:15:39 - INFO - trainer -   id_test: 2.12067    time_test: 0.76739  total_loss: 2.88806\n",
            "epoch 166  id_train: 2.09539  time_train: 0.77512  total_loss: 2.87051 lr 7.194766e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:15:45 - INFO - trainer -   id_test: 2.09972    time_test: 0.79113  total_loss: 2.89085\n",
            "epoch 167  id_train: 1.99605  time_train: 0.76772  total_loss: 2.76377 lr 7.185266e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:15:50 - INFO - trainer -   id_test: 2.06010    time_test: 0.80120  total_loss: 2.86130\n",
            "epoch 168  id_train: 2.02422  time_train: 0.76725  total_loss: 2.79147 lr 7.175716e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:15:56 - INFO - trainer -   id_test: 2.06338    time_test: 0.78173  total_loss: 2.84512\n",
            "epoch 169  id_train: 2.00805  time_train: 0.81441  total_loss: 2.82246 lr 7.166118e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:16:01 - INFO - trainer -   id_test: 2.06920    time_test: 0.79673  total_loss: 2.86592\n",
            "epoch 170  id_train: 2.01129  time_train: 0.77287  total_loss: 2.78415 lr 7.156470e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:16:07 - INFO - trainer -   id_test: 2.13512    time_test: 0.77523  total_loss: 2.91035\n",
            "epoch 171  id_train: 2.05121  time_train: 0.78079  total_loss: 2.83200 lr 7.146773e-05: 100%|██████████| 16/16 [00:04<00:00,  3.50it/s]\n",
            "02/04/2022 16:16:12 - INFO - trainer -   id_test: 2.09494    time_test: 0.78483  total_loss: 2.87977\n",
            "epoch 172  id_train: 2.01930  time_train: 0.80105  total_loss: 2.82035 lr 7.137028e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:16:18 - INFO - trainer -   id_test: 2.09407    time_test: 0.75987  total_loss: 2.85393\n",
            "epoch 173  id_train: 2.06136  time_train: 0.73217  total_loss: 2.79354 lr 7.127234e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:16:23 - INFO - trainer -   id_test: 2.07887    time_test: 0.79335  total_loss: 2.87222\n",
            "epoch 174  id_train: 2.01424  time_train: 0.78326  total_loss: 2.79751 lr 7.117391e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:16:29 - INFO - trainer -   id_test: 2.13424    time_test: 0.81200  total_loss: 2.94624\n",
            "epoch 175  id_train: 1.98334  time_train: 0.74601  total_loss: 2.72934 lr 7.107500e-05: 100%|██████████| 16/16 [00:04<00:00,  3.52it/s]\n",
            "02/04/2022 16:16:34 - INFO - trainer -   id_test: 2.06066    time_test: 0.78098  total_loss: 2.84164\n",
            "epoch 176  id_train: 2.00423  time_train: 0.80393  total_loss: 2.80816 lr 7.097562e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:16:40 - INFO - trainer -   id_test: 2.07614    time_test: 0.79168  total_loss: 2.86781\n",
            "epoch 177  id_train: 1.95787  time_train: 0.78414  total_loss: 2.74201 lr 7.087575e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:16:45 - INFO - trainer -   id_test: 2.07691    time_test: 0.78434  total_loss: 2.86125\n",
            "epoch 178  id_train: 2.05000  time_train: 0.75351  total_loss: 2.80351 lr 7.077540e-05: 100%|██████████| 16/16 [00:04<00:00,  3.44it/s]\n",
            "02/04/2022 16:16:51 - INFO - trainer -   id_test: 2.08794    time_test: 0.76241  total_loss: 2.85035\n",
            "epoch 179  id_train: 1.97002  time_train: 0.76687  total_loss: 2.73689 lr 7.067457e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:16:56 - INFO - trainer -   id_test: 2.06876    time_test: 0.81089  total_loss: 2.87966\n",
            "epoch 180  id_train: 2.03166  time_train: 0.78745  total_loss: 2.81911 lr 7.057327e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:17:02 - INFO - trainer -   id_test: 2.06301    time_test: 0.81941  total_loss: 2.88242\n",
            "epoch 181  id_train: 2.04161  time_train: 0.79707  total_loss: 2.83869 lr 7.047150e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:17:07 - INFO - trainer -   id_test: 2.04724    time_test: 0.80051  total_loss: 2.84774\n",
            "epoch 182  id_train: 1.98515  time_train: 0.78009  total_loss: 2.76524 lr 7.036925e-05: 100%|██████████| 16/16 [00:04<00:00,  3.44it/s]\n",
            "02/04/2022 16:17:13 - INFO - trainer -   id_test: 2.04666    time_test: 0.79660  total_loss: 2.84326\n",
            "epoch 183  id_train: 2.02117  time_train: 0.75017  total_loss: 2.77134 lr 7.026653e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:17:19 - INFO - trainer -   id_test: 2.08342    time_test: 0.77609  total_loss: 2.85951\n",
            "epoch 184  id_train: 2.06039  time_train: 0.76980  total_loss: 2.83019 lr 7.016335e-05: 100%|██████████| 16/16 [00:04<00:00,  3.54it/s]\n",
            "02/04/2022 16:17:24 - INFO - trainer -   id_test: 2.08899    time_test: 0.78944  total_loss: 2.87842\n",
            "epoch 185  id_train: 2.00308  time_train: 0.77036  total_loss: 2.77345 lr 7.005969e-05: 100%|██████████| 16/16 [00:04<00:00,  3.42it/s]\n",
            "02/04/2022 16:17:30 - INFO - trainer -   id_test: 2.08053    time_test: 0.78949  total_loss: 2.87002\n",
            "epoch 186  id_train: 1.95823  time_train: 0.76728  total_loss: 2.72550 lr 6.995558e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:17:35 - INFO - trainer -   id_test: 2.06395    time_test: 0.78563  total_loss: 2.84958\n",
            "epoch 187  id_train: 1.98726  time_train: 0.81811  total_loss: 2.80537 lr 6.985099e-05: 100%|██████████| 16/16 [00:04<00:00,  3.38it/s]\n",
            "02/04/2022 16:17:41 - INFO - trainer -   id_test: 2.13743    time_test: 0.77123  total_loss: 2.90866\n",
            "epoch 188  id_train: 1.99488  time_train: 0.77691  total_loss: 2.77179 lr 6.974595e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:17:46 - INFO - trainer -   id_test: 2.05721    time_test: 0.78916  total_loss: 2.84637\n",
            "epoch 189  id_train: 2.01639  time_train: 0.77241  total_loss: 2.78880 lr 6.964044e-05: 100%|██████████| 16/16 [00:04<00:00,  3.50it/s]\n",
            "02/04/2022 16:17:52 - INFO - trainer -   id_test: 2.05489    time_test: 0.79186  total_loss: 2.84675\n",
            "epoch 190  id_train: 2.06368  time_train: 0.77465  total_loss: 2.83833 lr 6.953447e-05: 100%|██████████| 16/16 [00:04<00:00,  3.54it/s]\n",
            "02/04/2022 16:17:57 - INFO - trainer -   id_test: 2.08388    time_test: 0.80506  total_loss: 2.88893\n",
            "epoch 191  id_train: 2.00396  time_train: 0.79980  total_loss: 2.80375 lr 6.942805e-05: 100%|██████████| 16/16 [00:04<00:00,  3.50it/s]\n",
            "02/04/2022 16:18:03 - INFO - trainer -   id_test: 2.05063    time_test: 0.80887  total_loss: 2.85950\n",
            "epoch 192  id_train: 2.01704  time_train: 0.77329  total_loss: 2.79033 lr 6.932117e-05: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n",
            "02/04/2022 16:18:08 - INFO - trainer -   id_test: 2.12239    time_test: 0.77987  total_loss: 2.90226\n",
            "epoch 193  id_train: 2.04936  time_train: 0.73776  total_loss: 2.78712 lr 6.921383e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:18:14 - INFO - trainer -   id_test: 2.06306    time_test: 0.79783  total_loss: 2.86089\n",
            "epoch 194  id_train: 1.98496  time_train: 0.74920  total_loss: 2.73417 lr 6.910605e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:18:19 - INFO - trainer -   id_test: 2.13075    time_test: 0.79295  total_loss: 2.92370\n",
            "epoch 195  id_train: 1.94382  time_train: 0.77212  total_loss: 2.71594 lr 6.899781e-05: 100%|██████████| 16/16 [00:04<00:00,  3.52it/s]\n",
            "02/04/2022 16:18:25 - INFO - trainer -   id_test: 2.04397    time_test: 0.81052  total_loss: 2.85448\n",
            "epoch 196  id_train: 1.98144  time_train: 0.77303  total_loss: 2.75447 lr 6.888912e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:18:30 - INFO - trainer -   id_test: 2.07427    time_test: 0.78332  total_loss: 2.85759\n",
            "epoch 197  id_train: 2.01485  time_train: 0.77549  total_loss: 2.79034 lr 6.877998e-05: 100%|██████████| 16/16 [00:04<00:00,  3.43it/s]\n",
            "02/04/2022 16:18:36 - INFO - trainer -   id_test: 2.08487    time_test: 0.79442  total_loss: 2.87930\n",
            "epoch 198  id_train: 2.00781  time_train: 0.77075  total_loss: 2.77856 lr 6.867040e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:18:41 - INFO - trainer -   id_test: 2.04606    time_test: 0.78813  total_loss: 2.83419\n",
            "02/04/2022 16:18:41 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 199  id_train: 2.01398  time_train: 0.75649  total_loss: 2.77047 lr 6.856038e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:18:47 - INFO - trainer -   id_test: 2.08049    time_test: 0.78978  total_loss: 2.87027\n",
            "epoch 200  id_train: 1.94669  time_train: 0.73870  total_loss: 2.68539 lr 6.844991e-05: 100%|██████████| 16/16 [00:04<00:00,  3.59it/s]\n",
            "02/04/2022 16:18:52 - INFO - trainer -   id_test: 2.11032    time_test: 0.78366  total_loss: 2.89398\n",
            "epoch 201  id_train: 1.98845  time_train: 0.73925  total_loss: 2.72770 lr 6.833900e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:18:58 - INFO - trainer -   id_test: 2.08967    time_test: 0.78351  total_loss: 2.87318\n",
            "epoch 202  id_train: 2.02146  time_train: 0.73724  total_loss: 2.75870 lr 6.822765e-05: 100%|██████████| 16/16 [00:04<00:00,  3.50it/s]\n",
            "02/04/2022 16:19:03 - INFO - trainer -   id_test: 2.07344    time_test: 0.78904  total_loss: 2.86248\n",
            "epoch 203  id_train: 2.06343  time_train: 0.77357  total_loss: 2.83700 lr 6.811586e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:19:09 - INFO - trainer -   id_test: 2.11211    time_test: 0.80340  total_loss: 2.91551\n",
            "epoch 204  id_train: 1.99988  time_train: 0.76085  total_loss: 2.76073 lr 6.800364e-05: 100%|██████████| 16/16 [00:04<00:00,  3.53it/s]\n",
            "02/04/2022 16:19:14 - INFO - trainer -   id_test: 2.07461    time_test: 0.78859  total_loss: 2.86320\n",
            "epoch 205  id_train: 1.93932  time_train: 0.78361  total_loss: 2.72294 lr 6.789099e-05: 100%|██████████| 16/16 [00:04<00:00,  3.38it/s]\n",
            "02/04/2022 16:19:20 - INFO - trainer -   id_test: 2.11131    time_test: 0.79328  total_loss: 2.90459\n",
            "epoch 206  id_train: 1.97914  time_train: 0.77210  total_loss: 2.75124 lr 6.777790e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:19:25 - INFO - trainer -   id_test: 2.02850    time_test: 0.80335  total_loss: 2.83185\n",
            "02/04/2022 16:19:25 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 207  id_train: 1.95103  time_train: 0.77807  total_loss: 2.72911 lr 6.766438e-05: 100%|██████████| 16/16 [00:04<00:00,  3.36it/s]\n",
            "02/04/2022 16:19:31 - INFO - trainer -   id_test: 2.11460    time_test: 0.77023  total_loss: 2.88483\n",
            "epoch 208  id_train: 1.96052  time_train: 0.75033  total_loss: 2.71085 lr 6.755043e-05: 100%|██████████| 16/16 [00:04<00:00,  3.25it/s]\n",
            "02/04/2022 16:19:37 - INFO - trainer -   id_test: 2.10293    time_test: 0.80079  total_loss: 2.90372\n",
            "epoch 209  id_train: 2.04949  time_train: 0.78475  total_loss: 2.83424 lr 6.743606e-05: 100%|██████████| 16/16 [00:05<00:00,  3.20it/s]\n",
            "02/04/2022 16:19:43 - INFO - trainer -   id_test: 2.07044    time_test: 0.78475  total_loss: 2.85519\n",
            "epoch 210  id_train: 2.02448  time_train: 0.76612  total_loss: 2.79060 lr 6.732126e-05: 100%|██████████| 16/16 [00:04<00:00,  3.29it/s]\n",
            "02/04/2022 16:19:49 - INFO - trainer -   id_test: 2.12018    time_test: 0.81147  total_loss: 2.93165\n",
            "epoch 211  id_train: 2.01634  time_train: 0.73420  total_loss: 2.75053 lr 6.720604e-05: 100%|██████████| 16/16 [00:04<00:00,  3.65it/s]\n",
            "02/04/2022 16:19:54 - INFO - trainer -   id_test: 2.10135    time_test: 0.78781  total_loss: 2.88915\n",
            "epoch 212  id_train: 2.08314  time_train: 0.77577  total_loss: 2.85892 lr 6.709039e-05: 100%|██████████| 16/16 [00:04<00:00,  3.59it/s]\n",
            "02/04/2022 16:19:59 - INFO - trainer -   id_test: 2.08579    time_test: 0.78931  total_loss: 2.87510\n",
            "epoch 213  id_train: 2.06893  time_train: 0.75894  total_loss: 2.82787 lr 6.697433e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:20:05 - INFO - trainer -   id_test: 2.08451    time_test: 0.79425  total_loss: 2.87876\n",
            "epoch 214  id_train: 1.94724  time_train: 0.78729  total_loss: 2.73453 lr 6.685785e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:20:10 - INFO - trainer -   id_test: 2.10147    time_test: 0.78277  total_loss: 2.88424\n",
            "epoch 215  id_train: 2.02840  time_train: 0.75414  total_loss: 2.78255 lr 6.674095e-05: 100%|██████████| 16/16 [00:04<00:00,  3.28it/s]\n",
            "02/04/2022 16:20:16 - INFO - trainer -   id_test: 2.12159    time_test: 0.79464  total_loss: 2.91623\n",
            "epoch 216  id_train: 1.99484  time_train: 0.77435  total_loss: 2.76919 lr 6.662364e-05: 100%|██████████| 16/16 [00:04<00:00,  3.27it/s]\n",
            "02/04/2022 16:20:22 - INFO - trainer -   id_test: 2.11255    time_test: 0.79075  total_loss: 2.90330\n",
            "epoch 217  id_train: 1.90248  time_train: 0.77780  total_loss: 2.68027 lr 6.650591e-05: 100%|██████████| 16/16 [00:04<00:00,  3.30it/s]\n",
            "02/04/2022 16:20:28 - INFO - trainer -   id_test: 2.06592    time_test: 0.79624  total_loss: 2.86217\n",
            "epoch 218  id_train: 1.96132  time_train: 0.73958  total_loss: 2.70090 lr 6.638777e-05: 100%|██████████| 16/16 [00:04<00:00,  3.30it/s]\n",
            "02/04/2022 16:20:34 - INFO - trainer -   id_test: 2.05813    time_test: 0.79330  total_loss: 2.85143\n",
            "epoch 219  id_train: 1.98975  time_train: 0.75210  total_loss: 2.74185 lr 6.626923e-05: 100%|██████████| 16/16 [00:04<00:00,  3.26it/s]\n",
            "02/04/2022 16:20:40 - INFO - trainer -   id_test: 2.06969    time_test: 0.81278  total_loss: 2.88247\n",
            "epoch 220  id_train: 1.97216  time_train: 0.77824  total_loss: 2.75040 lr 6.615028e-05: 100%|██████████| 16/16 [00:04<00:00,  3.59it/s]\n",
            "02/04/2022 16:20:45 - INFO - trainer -   id_test: 2.12156    time_test: 0.79850  total_loss: 2.92006\n",
            "epoch 221  id_train: 1.97316  time_train: 0.74548  total_loss: 2.71865 lr 6.603092e-05: 100%|██████████| 16/16 [00:04<00:00,  3.67it/s]\n",
            "02/04/2022 16:20:50 - INFO - trainer -   id_test: 2.12444    time_test: 0.79933  total_loss: 2.92377\n",
            "epoch 222  id_train: 1.99760  time_train: 0.75949  total_loss: 2.75709 lr 6.591116e-05: 100%|██████████| 16/16 [00:04<00:00,  3.62it/s]\n",
            "02/04/2022 16:20:56 - INFO - trainer -   id_test: 2.08894    time_test: 0.79009  total_loss: 2.87903\n",
            "epoch 223  id_train: 1.98596  time_train: 0.78161  total_loss: 2.76756 lr 6.579100e-05: 100%|██████████| 16/16 [00:04<00:00,  3.63it/s]\n",
            "02/04/2022 16:21:01 - INFO - trainer -   id_test: 2.12152    time_test: 0.78644  total_loss: 2.90795\n",
            "epoch 224  id_train: 1.99564  time_train: 0.78411  total_loss: 2.77976 lr 6.567043e-05: 100%|██████████| 16/16 [00:04<00:00,  3.59it/s]\n",
            "02/04/2022 16:21:06 - INFO - trainer -   id_test: 2.06359    time_test: 0.80341  total_loss: 2.86700\n",
            "epoch 225  id_train: 1.90331  time_train: 0.78796  total_loss: 2.69127 lr 6.554947e-05: 100%|██████████| 16/16 [00:04<00:00,  3.63it/s]\n",
            "02/04/2022 16:21:12 - INFO - trainer -   id_test: 2.06110    time_test: 0.81185  total_loss: 2.87295\n",
            "epoch 226  id_train: 1.93502  time_train: 0.77956  total_loss: 2.71458 lr 6.542812e-05: 100%|██████████| 16/16 [00:04<00:00,  3.67it/s]\n",
            "02/04/2022 16:21:17 - INFO - trainer -   id_test: 2.07400    time_test: 0.77654  total_loss: 2.85054\n",
            "epoch 227  id_train: 2.06268  time_train: 0.71251  total_loss: 2.77519 lr 6.530637e-05: 100%|██████████| 16/16 [00:04<00:00,  3.56it/s]\n",
            "02/04/2022 16:21:22 - INFO - trainer -   id_test: 2.06851    time_test: 0.80830  total_loss: 2.87681\n",
            "epoch 228  id_train: 2.00749  time_train: 0.73957  total_loss: 2.74706 lr 6.518423e-05: 100%|██████████| 16/16 [00:04<00:00,  3.67it/s]\n",
            "02/04/2022 16:21:27 - INFO - trainer -   id_test: 2.07110    time_test: 0.78124  total_loss: 2.85234\n",
            "epoch 229  id_train: 2.01493  time_train: 0.73571  total_loss: 2.75064 lr 6.506169e-05: 100%|██████████| 16/16 [00:04<00:00,  3.53it/s]\n",
            "02/04/2022 16:21:33 - INFO - trainer -   id_test: 2.10535    time_test: 0.77886  total_loss: 2.88420\n",
            "epoch 230  id_train: 1.98243  time_train: 0.75829  total_loss: 2.74072 lr 6.493877e-05: 100%|██████████| 16/16 [00:04<00:00,  3.73it/s]\n",
            "02/04/2022 16:21:38 - INFO - trainer -   id_test: 2.07242    time_test: 0.77724  total_loss: 2.84966\n",
            "epoch 231  id_train: 1.92773  time_train: 0.77568  total_loss: 2.70340 lr 6.481547e-05: 100%|██████████| 16/16 [00:04<00:00,  3.55it/s]\n",
            "02/04/2022 16:21:43 - INFO - trainer -   id_test: 2.12090    time_test: 0.78901  total_loss: 2.90990\n",
            "epoch 232  id_train: 1.91888  time_train: 0.78132  total_loss: 2.70020 lr 6.469177e-05: 100%|██████████| 16/16 [00:04<00:00,  3.65it/s]\n",
            "02/04/2022 16:21:49 - INFO - trainer -   id_test: 2.15693    time_test: 0.78096  total_loss: 2.93789\n",
            "epoch 233  id_train: 2.00621  time_train: 0.74760  total_loss: 2.75382 lr 6.456770e-05: 100%|██████████| 16/16 [00:04<00:00,  3.57it/s]\n",
            "02/04/2022 16:21:54 - INFO - trainer -   id_test: 2.09638    time_test: 0.78744  total_loss: 2.88382\n",
            "epoch 234  id_train: 1.93968  time_train: 0.77236  total_loss: 2.71204 lr 6.444325e-05: 100%|██████████| 16/16 [00:04<00:00,  3.68it/s]\n",
            "02/04/2022 16:21:59 - INFO - trainer -   id_test: 2.05089    time_test: 0.78665  total_loss: 2.83753\n",
            "epoch 235  id_train: 1.90236  time_train: 0.77439  total_loss: 2.67675 lr 6.431841e-05: 100%|██████████| 16/16 [00:04<00:00,  3.57it/s]\n",
            "02/04/2022 16:22:05 - INFO - trainer -   id_test: 2.08642    time_test: 0.79885  total_loss: 2.88526\n",
            "epoch 236  id_train: 1.95351  time_train: 0.76331  total_loss: 2.71682 lr 6.419320e-05: 100%|██████████| 16/16 [00:04<00:00,  3.68it/s]\n",
            "02/04/2022 16:22:10 - INFO - trainer -   id_test: 2.08735    time_test: 0.79564  total_loss: 2.88299\n",
            "epoch 237  id_train: 1.95272  time_train: 0.76531  total_loss: 2.71803 lr 6.406761e-05: 100%|██████████| 16/16 [00:04<00:00,  3.57it/s]\n",
            "02/04/2022 16:22:15 - INFO - trainer -   id_test: 2.06902    time_test: 0.79928  total_loss: 2.86830\n",
            "epoch 238  id_train: 1.95686  time_train: 0.73795  total_loss: 2.69482 lr 6.394166e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:22:21 - INFO - trainer -   id_test: 2.10435    time_test: 0.80215  total_loss: 2.90651\n",
            "epoch 239  id_train: 1.92832  time_train: 0.78994  total_loss: 2.71826 lr 6.381533e-05: 100%|██████████| 16/16 [00:05<00:00,  3.11it/s]\n",
            "02/04/2022 16:22:27 - INFO - trainer -   id_test: 2.09249    time_test: 0.78979  total_loss: 2.88228\n",
            "epoch 240  id_train: 1.98455  time_train: 0.74312  total_loss: 2.72767 lr 6.368863e-05: 100%|██████████| 16/16 [00:04<00:00,  3.36it/s]\n",
            "02/04/2022 16:22:33 - INFO - trainer -   id_test: 2.11581    time_test: 0.80137  total_loss: 2.91718\n",
            "epoch 241  id_train: 1.99078  time_train: 0.75839  total_loss: 2.74917 lr 6.356156e-05: 100%|██████████| 16/16 [00:04<00:00,  3.44it/s]\n",
            "02/04/2022 16:22:38 - INFO - trainer -   id_test: 2.08767    time_test: 0.79221  total_loss: 2.87988\n",
            "epoch 242  id_train: 1.91307  time_train: 0.77580  total_loss: 2.68887 lr 6.343413e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:22:44 - INFO - trainer -   id_test: 2.09399    time_test: 0.80212  total_loss: 2.89611\n",
            "epoch 243  id_train: 1.99367  time_train: 0.75003  total_loss: 2.74371 lr 6.330634e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:22:49 - INFO - trainer -   id_test: 2.06635    time_test: 0.79478  total_loss: 2.86114\n",
            "epoch 244  id_train: 1.94880  time_train: 0.77673  total_loss: 2.72553 lr 6.317818e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:22:55 - INFO - trainer -   id_test: 2.10262    time_test: 0.79023  total_loss: 2.89285\n",
            "epoch 245  id_train: 2.00158  time_train: 0.73769  total_loss: 2.73927 lr 6.304967e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:23:00 - INFO - trainer -   id_test: 2.08989    time_test: 0.78200  total_loss: 2.87189\n",
            "epoch 246  id_train: 2.02785  time_train: 0.76524  total_loss: 2.79308 lr 6.292079e-05: 100%|██████████| 16/16 [00:04<00:00,  3.38it/s]\n",
            "02/04/2022 16:23:06 - INFO - trainer -   id_test: 2.09205    time_test: 0.80249  total_loss: 2.89454\n",
            "epoch 247  id_train: 1.92130  time_train: 0.75450  total_loss: 2.67579 lr 6.279157e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:23:11 - INFO - trainer -   id_test: 2.07817    time_test: 0.81166  total_loss: 2.88983\n",
            "epoch 248  id_train: 1.91548  time_train: 0.77088  total_loss: 2.68636 lr 6.266199e-05: 100%|██████████| 16/16 [00:04<00:00,  3.41it/s]\n",
            "02/04/2022 16:23:17 - INFO - trainer -   id_test: 2.14256    time_test: 0.80262  total_loss: 2.94519\n",
            "epoch 249  id_train: 1.91070  time_train: 0.76540  total_loss: 2.67611 lr 6.253206e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:23:22 - INFO - trainer -   id_test: 2.09666    time_test: 0.78112  total_loss: 2.87778\n",
            "epoch 250  id_train: 1.98638  time_train: 0.77224  total_loss: 2.75861 lr 6.240178e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:23:28 - INFO - trainer -   id_test: 2.09221    time_test: 0.79545  total_loss: 2.88766\n",
            "epoch 251  id_train: 1.98601  time_train: 0.78970  total_loss: 2.77571 lr 6.227115e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:23:34 - INFO - trainer -   id_test: 2.10247    time_test: 0.79003  total_loss: 2.89250\n",
            "epoch 252  id_train: 1.94827  time_train: 0.78711  total_loss: 2.73538 lr 6.214018e-05: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n",
            "02/04/2022 16:23:39 - INFO - trainer -   id_test: 2.07883    time_test: 0.80489  total_loss: 2.88372\n",
            "epoch 253  id_train: 1.99806  time_train: 0.76649  total_loss: 2.76456 lr 6.200886e-05: 100%|██████████| 16/16 [00:04<00:00,  3.43it/s]\n",
            "02/04/2022 16:23:45 - INFO - trainer -   id_test: 2.09462    time_test: 0.81619  total_loss: 2.91081\n",
            "epoch 254  id_train: 1.98700  time_train: 0.75370  total_loss: 2.74070 lr 6.187720e-05: 100%|██████████| 16/16 [00:04<00:00,  3.44it/s]\n",
            "02/04/2022 16:23:50 - INFO - trainer -   id_test: 2.08790    time_test: 0.77325  total_loss: 2.86115\n",
            "epoch 255  id_train: 1.93660  time_train: 0.77274  total_loss: 2.70934 lr 6.174521e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:23:56 - INFO - trainer -   id_test: 2.12311    time_test: 0.81542  total_loss: 2.93853\n",
            "epoch 256  id_train: 1.91475  time_train: 0.76307  total_loss: 2.67782 lr 6.161287e-05: 100%|██████████| 16/16 [00:04<00:00,  3.43it/s]\n",
            "02/04/2022 16:24:01 - INFO - trainer -   id_test: 2.11047    time_test: 0.79464  total_loss: 2.90511\n",
            "epoch 257  id_train: 1.99075  time_train: 0.77254  total_loss: 2.76329 lr 6.148020e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:24:07 - INFO - trainer -   id_test: 2.05016    time_test: 0.80132  total_loss: 2.85147\n",
            "epoch 258  id_train: 1.87076  time_train: 0.77789  total_loss: 2.64865 lr 6.134720e-05: 100%|██████████| 16/16 [00:04<00:00,  3.44it/s]\n",
            "02/04/2022 16:24:12 - INFO - trainer -   id_test: 2.11632    time_test: 0.79335  total_loss: 2.90967\n",
            "epoch 259  id_train: 2.02476  time_train: 0.76009  total_loss: 2.78484 lr 6.121387e-05: 100%|██████████| 16/16 [00:04<00:00,  3.50it/s]\n",
            "02/04/2022 16:24:18 - INFO - trainer -   id_test: 2.09045    time_test: 0.79466  total_loss: 2.88510\n",
            "epoch 260  id_train: 2.00429  time_train: 0.78028  total_loss: 2.78458 lr 6.108021e-05: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n",
            "02/04/2022 16:24:23 - INFO - trainer -   id_test: 2.12456    time_test: 0.76973  total_loss: 2.89429\n",
            "epoch 261  id_train: 1.98341  time_train: 0.77506  total_loss: 2.75847 lr 6.094622e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:24:29 - INFO - trainer -   id_test: 2.14036    time_test: 0.78062  total_loss: 2.92098\n",
            "epoch 262  id_train: 1.97970  time_train: 0.73939  total_loss: 2.71909 lr 6.081191e-05: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n",
            "02/04/2022 16:24:35 - INFO - trainer -   id_test: 2.07954    time_test: 0.78720  total_loss: 2.86674\n",
            "epoch 263  id_train: 1.98583  time_train: 0.75553  total_loss: 2.74137 lr 6.067728e-05: 100%|██████████| 16/16 [00:04<00:00,  3.44it/s]\n",
            "02/04/2022 16:24:40 - INFO - trainer -   id_test: 2.09749    time_test: 0.78790  total_loss: 2.88538\n",
            "epoch 264  id_train: 1.95478  time_train: 0.78090  total_loss: 2.73568 lr 6.054232e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:24:46 - INFO - trainer -   id_test: 2.12556    time_test: 0.79718  total_loss: 2.92274\n",
            "epoch 265  id_train: 2.00423  time_train: 0.80270  total_loss: 2.80693 lr 6.040705e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:24:51 - INFO - trainer -   id_test: 2.11986    time_test: 0.77823  total_loss: 2.89809\n",
            "epoch 266  id_train: 1.92737  time_train: 0.78855  total_loss: 2.71592 lr 6.027146e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:24:57 - INFO - trainer -   id_test: 2.10809    time_test: 0.78890  total_loss: 2.89699\n",
            "epoch 267  id_train: 2.01389  time_train: 0.77066  total_loss: 2.78455 lr 6.013556e-05: 100%|██████████| 16/16 [00:04<00:00,  3.54it/s]\n",
            "02/04/2022 16:25:02 - INFO - trainer -   id_test: 2.10317    time_test: 0.80894  total_loss: 2.91210\n",
            "epoch 268  id_train: 1.94887  time_train: 0.75812  total_loss: 2.70698 lr 5.999934e-05: 100%|██████████| 16/16 [00:04<00:00,  3.50it/s]\n",
            "02/04/2022 16:25:08 - INFO - trainer -   id_test: 2.09643    time_test: 0.79240  total_loss: 2.88884\n",
            "epoch 269  id_train: 1.87831  time_train: 0.77368  total_loss: 2.65200 lr 5.986282e-05: 100%|██████████| 16/16 [00:04<00:00,  3.54it/s]\n",
            "02/04/2022 16:25:13 - INFO - trainer -   id_test: 2.12195    time_test: 0.78930  total_loss: 2.91125\n",
            "epoch 270  id_train: 2.03272  time_train: 0.72232  total_loss: 2.75504 lr 5.972598e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:25:19 - INFO - trainer -   id_test: 2.01864    time_test: 0.80063  total_loss: 2.81927\n",
            "02/04/2022 16:25:19 - INFO - trainer -   saving /home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\n",
            "epoch 271  id_train: 1.95308  time_train: 0.75627  total_loss: 2.70935 lr 5.958884e-05: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n",
            "02/04/2022 16:25:24 - INFO - trainer -   id_test: 2.04416    time_test: 0.79144  total_loss: 2.83560\n",
            "epoch 272  id_train: 1.96224  time_train: 0.77681  total_loss: 2.73905 lr 5.945140e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:25:30 - INFO - trainer -   id_test: 2.11227    time_test: 0.80280  total_loss: 2.91507\n",
            "epoch 273  id_train: 1.95739  time_train: 0.76995  total_loss: 2.72734 lr 5.931366e-05: 100%|██████████| 16/16 [00:04<00:00,  3.52it/s]\n",
            "02/04/2022 16:25:35 - INFO - trainer -   id_test: 2.04393    time_test: 0.79837  total_loss: 2.84230\n",
            "epoch 274  id_train: 1.92758  time_train: 0.74085  total_loss: 2.66843 lr 5.917562e-05: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n",
            "02/04/2022 16:25:41 - INFO - trainer -   id_test: 2.03629    time_test: 0.79573  total_loss: 2.83202\n",
            "epoch 275  id_train: 1.97072  time_train: 0.74363  total_loss: 2.71435 lr 5.903727e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:25:46 - INFO - trainer -   id_test: 2.08906    time_test: 0.78822  total_loss: 2.87728\n",
            "epoch 276  id_train: 1.90334  time_train: 0.74228  total_loss: 2.64562 lr 5.889864e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:25:52 - INFO - trainer -   id_test: 2.13553    time_test: 0.80476  total_loss: 2.94029\n",
            "epoch 277  id_train: 1.94886  time_train: 0.75350  total_loss: 2.70237 lr 5.875971e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:25:57 - INFO - trainer -   id_test: 2.07535    time_test: 0.79657  total_loss: 2.87192\n",
            "epoch 278  id_train: 1.96857  time_train: 0.74553  total_loss: 2.71410 lr 5.862049e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:26:03 - INFO - trainer -   id_test: 2.10024    time_test: 0.78851  total_loss: 2.88875\n",
            "epoch 279  id_train: 1.92615  time_train: 0.77494  total_loss: 2.70109 lr 5.848098e-05: 100%|██████████| 16/16 [00:04<00:00,  3.44it/s]\n",
            "02/04/2022 16:26:08 - INFO - trainer -   id_test: 2.14756    time_test: 0.82902  total_loss: 2.97658\n",
            "epoch 280  id_train: 1.94779  time_train: 0.78875  total_loss: 2.73654 lr 5.834119e-05: 100%|██████████| 16/16 [00:04<00:00,  3.44it/s]\n",
            "02/04/2022 16:26:14 - INFO - trainer -   id_test: 2.07709    time_test: 0.80724  total_loss: 2.88433\n",
            "epoch 281  id_train: 1.96519  time_train: 0.74648  total_loss: 2.71167 lr 5.820111e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:26:20 - INFO - trainer -   id_test: 2.13661    time_test: 0.78390  total_loss: 2.92051\n",
            "epoch 282  id_train: 1.90421  time_train: 0.76923  total_loss: 2.67344 lr 5.806075e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:26:25 - INFO - trainer -   id_test: 2.12308    time_test: 0.80313  total_loss: 2.92621\n",
            "epoch 283  id_train: 1.99551  time_train: 0.74117  total_loss: 2.73668 lr 5.792011e-05: 100%|██████████| 16/16 [00:04<00:00,  3.53it/s]\n",
            "02/04/2022 16:26:30 - INFO - trainer -   id_test: 2.09186    time_test: 0.79440  total_loss: 2.88626\n",
            "epoch 284  id_train: 1.99223  time_train: 0.77312  total_loss: 2.76535 lr 5.777919e-05: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n",
            "02/04/2022 16:26:36 - INFO - trainer -   id_test: 2.12197    time_test: 0.77380  total_loss: 2.89577\n",
            "epoch 285  id_train: 1.93680  time_train: 0.75220  total_loss: 2.68900 lr 5.763800e-05: 100%|██████████| 16/16 [00:04<00:00,  3.50it/s]\n",
            "02/04/2022 16:26:41 - INFO - trainer -   id_test: 2.12264    time_test: 0.82200  total_loss: 2.94463\n",
            "epoch 286  id_train: 1.90640  time_train: 0.76989  total_loss: 2.67629 lr 5.749653e-05: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n",
            "02/04/2022 16:26:47 - INFO - trainer -   id_test: 2.09219    time_test: 0.80937  total_loss: 2.90156\n",
            "epoch 287  id_train: 1.90145  time_train: 0.77829  total_loss: 2.67975 lr 5.735480e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:26:53 - INFO - trainer -   id_test: 2.09078    time_test: 0.80081  total_loss: 2.89159\n",
            "epoch 288  id_train: 1.92549  time_train: 0.77182  total_loss: 2.69731 lr 5.721279e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:26:58 - INFO - trainer -   id_test: 2.13823    time_test: 0.79319  total_loss: 2.93142\n",
            "epoch 289  id_train: 1.96035  time_train: 0.79673  total_loss: 2.75708 lr 5.707051e-05: 100%|██████████| 16/16 [00:04<00:00,  3.42it/s]\n",
            "02/04/2022 16:27:04 - INFO - trainer -   id_test: 2.14326    time_test: 0.78228  total_loss: 2.92554\n",
            "epoch 290  id_train: 2.01582  time_train: 0.77153  total_loss: 2.78736 lr 5.692798e-05: 100%|██████████| 16/16 [00:04<00:00,  3.50it/s]\n",
            "02/04/2022 16:27:09 - INFO - trainer -   id_test: 2.11441    time_test: 0.80917  total_loss: 2.92358\n",
            "epoch 291  id_train: 1.89930  time_train: 0.77945  total_loss: 2.67875 lr 5.678518e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:27:15 - INFO - trainer -   id_test: 2.12341    time_test: 0.78438  total_loss: 2.90778\n",
            "epoch 292  id_train: 1.97462  time_train: 0.72174  total_loss: 2.69636 lr 5.664212e-05: 100%|██████████| 16/16 [00:04<00:00,  3.43it/s]\n",
            "02/04/2022 16:27:20 - INFO - trainer -   id_test: 2.07730    time_test: 0.80410  total_loss: 2.88140\n",
            "epoch 293  id_train: 2.01207  time_train: 0.72863  total_loss: 2.74071 lr 5.649880e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:27:26 - INFO - trainer -   id_test: 2.10499    time_test: 0.78836  total_loss: 2.89335\n",
            "epoch 294  id_train: 1.95598  time_train: 0.77676  total_loss: 2.73273 lr 5.635522e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:27:31 - INFO - trainer -   id_test: 2.12964    time_test: 0.78003  total_loss: 2.90967\n",
            "epoch 295  id_train: 1.90125  time_train: 0.78994  total_loss: 2.69120 lr 5.621139e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:27:37 - INFO - trainer -   id_test: 2.13091    time_test: 0.79621  total_loss: 2.92711\n",
            "epoch 296  id_train: 1.97613  time_train: 0.77680  total_loss: 2.75293 lr 5.606732e-05: 100%|██████████| 16/16 [00:04<00:00,  3.35it/s]\n",
            "02/04/2022 16:27:43 - INFO - trainer -   id_test: 2.08920    time_test: 0.78646  total_loss: 2.87566\n",
            "epoch 297  id_train: 2.02821  time_train: 0.72625  total_loss: 2.75446 lr 5.592299e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:27:48 - INFO - trainer -   id_test: 2.12233    time_test: 0.79368  total_loss: 2.91601\n",
            "epoch 298  id_train: 2.03826  time_train: 0.72138  total_loss: 2.75964 lr 5.577841e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:27:54 - INFO - trainer -   id_test: 2.08356    time_test: 0.78928  total_loss: 2.87284\n",
            "epoch 299  id_train: 1.92663  time_train: 0.75298  total_loss: 2.67961 lr 5.563359e-05: 100%|██████████| 16/16 [00:04<00:00,  3.52it/s]\n",
            "02/04/2022 16:27:59 - INFO - trainer -   id_test: 2.09948    time_test: 0.78593  total_loss: 2.88541\n",
            "epoch 300  id_train: 1.94402  time_train: 0.74597  total_loss: 2.68999 lr 5.548853e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:28:04 - INFO - trainer -   id_test: 2.09646    time_test: 0.79300  total_loss: 2.88946\n",
            "epoch 301  id_train: 1.93653  time_train: 0.74051  total_loss: 2.67704 lr 5.534323e-05: 100%|██████████| 16/16 [00:04<00:00,  3.52it/s]\n",
            "02/04/2022 16:28:10 - INFO - trainer -   id_test: 2.11843    time_test: 0.79204  total_loss: 2.91047\n",
            "epoch 302  id_train: 1.95541  time_train: 0.76160  total_loss: 2.71701 lr 5.519769e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:28:15 - INFO - trainer -   id_test: 2.13404    time_test: 0.78184  total_loss: 2.91588\n",
            "epoch 303  id_train: 1.97931  time_train: 0.74889  total_loss: 2.72819 lr 5.505192e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:28:21 - INFO - trainer -   id_test: 2.15527    time_test: 0.79073  total_loss: 2.94601\n",
            "epoch 304  id_train: 1.88079  time_train: 0.72033  total_loss: 2.60112 lr 5.490591e-05: 100%|██████████| 16/16 [00:04<00:00,  3.37it/s]\n",
            "02/04/2022 16:28:27 - INFO - trainer -   id_test: 2.14461    time_test: 0.80477  total_loss: 2.94938\n",
            "epoch 305  id_train: 1.97284  time_train: 0.73709  total_loss: 2.70993 lr 5.475967e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:28:32 - INFO - trainer -   id_test: 2.13019    time_test: 0.78814  total_loss: 2.91833\n",
            "epoch 306  id_train: 2.03002  time_train: 0.74230  total_loss: 2.77233 lr 5.461320e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:28:38 - INFO - trainer -   id_test: 2.09968    time_test: 0.80686  total_loss: 2.90654\n",
            "epoch 307  id_train: 2.04157  time_train: 0.72430  total_loss: 2.76587 lr 5.446651e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:28:43 - INFO - trainer -   id_test: 2.11672    time_test: 0.78354  total_loss: 2.90025\n",
            "epoch 308  id_train: 1.92782  time_train: 0.77013  total_loss: 2.69795 lr 5.431959e-05: 100%|██████████| 16/16 [00:04<00:00,  3.42it/s]\n",
            "02/04/2022 16:28:49 - INFO - trainer -   id_test: 2.13032    time_test: 0.80439  total_loss: 2.93471\n",
            "epoch 309  id_train: 1.94076  time_train: 0.73759  total_loss: 2.67835 lr 5.417245e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:28:54 - INFO - trainer -   id_test: 2.09668    time_test: 0.83123  total_loss: 2.92791\n",
            "epoch 310  id_train: 1.92165  time_train: 0.73537  total_loss: 2.65702 lr 5.402509e-05: 100%|██████████| 16/16 [00:04<00:00,  3.44it/s]\n",
            "02/04/2022 16:29:00 - INFO - trainer -   id_test: 2.14448    time_test: 0.78466  total_loss: 2.92915\n",
            "epoch 311  id_train: 1.95793  time_train: 0.71281  total_loss: 2.67075 lr 5.387751e-05: 100%|██████████| 16/16 [00:04<00:00,  3.44it/s]\n",
            "02/04/2022 16:29:05 - INFO - trainer -   id_test: 2.12098    time_test: 0.80765  total_loss: 2.92863\n",
            "epoch 312  id_train: 1.96531  time_train: 0.76803  total_loss: 2.73334 lr 5.372972e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:29:11 - INFO - trainer -   id_test: 2.10844    time_test: 0.79865  total_loss: 2.90710\n",
            "epoch 313  id_train: 1.89745  time_train: 0.75255  total_loss: 2.65001 lr 5.358171e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:29:16 - INFO - trainer -   id_test: 2.08910    time_test: 0.80270  total_loss: 2.89180\n",
            "epoch 314  id_train: 1.94394  time_train: 0.77076  total_loss: 2.71470 lr 5.343350e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:29:22 - INFO - trainer -   id_test: 2.16462    time_test: 0.78273  total_loss: 2.94735\n",
            "epoch 315  id_train: 1.94873  time_train: 0.75598  total_loss: 2.70471 lr 5.328507e-05: 100%|██████████| 16/16 [00:05<00:00,  3.16it/s]\n",
            "02/04/2022 16:29:28 - INFO - trainer -   id_test: 2.09425    time_test: 0.79080  total_loss: 2.88505\n",
            "epoch 316  id_train: 1.95356  time_train: 0.73534  total_loss: 2.68891 lr 5.313644e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:29:33 - INFO - trainer -   id_test: 2.14870    time_test: 0.79782  total_loss: 2.94651\n",
            "epoch 317  id_train: 1.94044  time_train: 0.76716  total_loss: 2.70760 lr 5.298761e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:29:39 - INFO - trainer -   id_test: 2.10131    time_test: 0.80152  total_loss: 2.90283\n",
            "epoch 318  id_train: 2.01815  time_train: 0.72797  total_loss: 2.74613 lr 5.283858e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:29:44 - INFO - trainer -   id_test: 2.11792    time_test: 0.80088  total_loss: 2.91880\n",
            "epoch 319  id_train: 1.90472  time_train: 0.75802  total_loss: 2.66275 lr 5.268934e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:29:50 - INFO - trainer -   id_test: 2.08927    time_test: 0.77469  total_loss: 2.86396\n",
            "epoch 320  id_train: 1.95136  time_train: 0.75824  total_loss: 2.70960 lr 5.253991e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:29:55 - INFO - trainer -   id_test: 2.11438    time_test: 0.79490  total_loss: 2.90928\n",
            "epoch 321  id_train: 2.04069  time_train: 0.76603  total_loss: 2.80672 lr 5.239029e-05: 100%|██████████| 16/16 [00:04<00:00,  3.55it/s]\n",
            "02/04/2022 16:30:01 - INFO - trainer -   id_test: 2.12210    time_test: 0.79940  total_loss: 2.92150\n",
            "epoch 322  id_train: 1.91106  time_train: 0.78693  total_loss: 2.69798 lr 5.224047e-05: 100%|██████████| 16/16 [00:04<00:00,  3.52it/s]\n",
            "02/04/2022 16:30:06 - INFO - trainer -   id_test: 2.06553    time_test: 0.79897  total_loss: 2.86450\n",
            "epoch 323  id_train: 1.94094  time_train: 0.77155  total_loss: 2.71249 lr 5.209047e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:30:12 - INFO - trainer -   id_test: 2.10262    time_test: 0.80217  total_loss: 2.90479\n",
            "epoch 324  id_train: 1.95300  time_train: 0.75327  total_loss: 2.70627 lr 5.194027e-05: 100%|██████████| 16/16 [00:04<00:00,  3.42it/s]\n",
            "02/04/2022 16:30:17 - INFO - trainer -   id_test: 2.08289    time_test: 0.77643  total_loss: 2.85932\n",
            "epoch 325  id_train: 1.97819  time_train: 0.76445  total_loss: 2.74264 lr 5.178989e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:30:23 - INFO - trainer -   id_test: 2.09931    time_test: 0.82486  total_loss: 2.92417\n",
            "epoch 326  id_train: 1.91771  time_train: 0.75267  total_loss: 2.67039 lr 5.163933e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:30:28 - INFO - trainer -   id_test: 2.10780    time_test: 0.79769  total_loss: 2.90550\n",
            "epoch 327  id_train: 1.96885  time_train: 0.75448  total_loss: 2.72333 lr 5.148859e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:30:34 - INFO - trainer -   id_test: 2.13206    time_test: 0.77360  total_loss: 2.90566\n",
            "epoch 328  id_train: 1.95942  time_train: 0.74354  total_loss: 2.70296 lr 5.133767e-05: 100%|██████████| 16/16 [00:04<00:00,  3.52it/s]\n",
            "02/04/2022 16:30:39 - INFO - trainer -   id_test: 2.10252    time_test: 0.79196  total_loss: 2.89448\n",
            "epoch 329  id_train: 1.95570  time_train: 0.73988  total_loss: 2.69558 lr 5.118658e-05: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n",
            "02/04/2022 16:30:45 - INFO - trainer -   id_test: 2.10650    time_test: 0.82713  total_loss: 2.93364\n",
            "epoch 330  id_train: 1.94556  time_train: 0.71722  total_loss: 2.66277 lr 5.103531e-05: 100%|██████████| 16/16 [00:04<00:00,  3.53it/s]\n",
            "02/04/2022 16:30:50 - INFO - trainer -   id_test: 2.11810    time_test: 0.80176  total_loss: 2.91985\n",
            "epoch 331  id_train: 1.96664  time_train: 0.75152  total_loss: 2.71817 lr 5.088387e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:30:56 - INFO - trainer -   id_test: 2.09724    time_test: 0.81540  total_loss: 2.91264\n",
            "epoch 332  id_train: 1.94001  time_train: 0.74171  total_loss: 2.68173 lr 5.073226e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:31:01 - INFO - trainer -   id_test: 2.11715    time_test: 0.79649  total_loss: 2.91363\n",
            "epoch 333  id_train: 2.01161  time_train: 0.77144  total_loss: 2.78306 lr 5.058048e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:31:07 - INFO - trainer -   id_test: 2.09004    time_test: 0.81835  total_loss: 2.90839\n",
            "epoch 334  id_train: 1.98348  time_train: 0.72857  total_loss: 2.71205 lr 5.042854e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:31:12 - INFO - trainer -   id_test: 2.12157    time_test: 0.79321  total_loss: 2.91477\n",
            "epoch 335  id_train: 1.93407  time_train: 0.78956  total_loss: 2.72363 lr 5.027644e-05: 100%|██████████| 16/16 [00:04<00:00,  3.42it/s]\n",
            "02/04/2022 16:31:18 - INFO - trainer -   id_test: 2.06082    time_test: 0.79548  total_loss: 2.85631\n",
            "epoch 336  id_train: 1.96420  time_train: 0.74802  total_loss: 2.71222 lr 5.012418e-05: 100%|██████████| 16/16 [00:04<00:00,  3.53it/s]\n",
            "02/04/2022 16:31:23 - INFO - trainer -   id_test: 2.09440    time_test: 0.79749  total_loss: 2.89189\n",
            "epoch 337  id_train: 1.85939  time_train: 0.74660  total_loss: 2.60598 lr 4.997177e-05: 100%|██████████| 16/16 [00:04<00:00,  3.36it/s]\n",
            "02/04/2022 16:31:29 - INFO - trainer -   id_test: 2.13487    time_test: 0.79776  total_loss: 2.93263\n",
            "epoch 338  id_train: 1.90612  time_train: 0.76352  total_loss: 2.66964 lr 4.981920e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:31:35 - INFO - trainer -   id_test: 2.13498    time_test: 0.77929  total_loss: 2.91427\n",
            "epoch 339  id_train: 1.98167  time_train: 0.77527  total_loss: 2.75694 lr 4.966647e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:31:40 - INFO - trainer -   id_test: 2.15314    time_test: 0.78750  total_loss: 2.94064\n",
            "epoch 340  id_train: 1.90344  time_train: 0.75784  total_loss: 2.66128 lr 4.951360e-05: 100%|██████████| 16/16 [00:04<00:00,  3.39it/s]\n",
            "02/04/2022 16:31:46 - INFO - trainer -   id_test: 2.12800    time_test: 0.78350  total_loss: 2.91150\n",
            "epoch 341  id_train: 1.93152  time_train: 0.71970  total_loss: 2.65122 lr 4.936058e-05: 100%|██████████| 16/16 [00:04<00:00,  3.50it/s]\n",
            "02/04/2022 16:31:51 - INFO - trainer -   id_test: 2.11696    time_test: 0.78857  total_loss: 2.90553\n",
            "epoch 342  id_train: 1.90256  time_train: 0.75347  total_loss: 2.65603 lr 4.920741e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:31:57 - INFO - trainer -   id_test: 2.09699    time_test: 0.76935  total_loss: 2.86634\n",
            "epoch 343  id_train: 1.95688  time_train: 0.78848  total_loss: 2.74536 lr 4.905411e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:32:03 - INFO - trainer -   id_test: 2.14636    time_test: 0.81536  total_loss: 2.96171\n",
            "epoch 344  id_train: 1.98194  time_train: 0.73081  total_loss: 2.71275 lr 4.890066e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:32:08 - INFO - trainer -   id_test: 2.11836    time_test: 0.81003  total_loss: 2.92838\n",
            "epoch 345  id_train: 1.83183  time_train: 0.75766  total_loss: 2.58949 lr 4.874707e-05: 100%|██████████| 16/16 [00:04<00:00,  3.50it/s]\n",
            "02/04/2022 16:32:14 - INFO - trainer -   id_test: 2.13753    time_test: 0.79013  total_loss: 2.92766\n",
            "epoch 346  id_train: 1.87765  time_train: 0.76994  total_loss: 2.64760 lr 4.859335e-05: 100%|██████████| 16/16 [00:04<00:00,  3.40it/s]\n",
            "02/04/2022 16:32:19 - INFO - trainer -   id_test: 2.13013    time_test: 0.80196  total_loss: 2.93209\n",
            "epoch 347  id_train: 1.91812  time_train: 0.74439  total_loss: 2.66251 lr 4.843949e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:32:25 - INFO - trainer -   id_test: 2.01902    time_test: 0.81115  total_loss: 2.83016\n",
            "epoch 348  id_train: 1.93306  time_train: 0.73279  total_loss: 2.66584 lr 4.828551e-05: 100%|██████████| 16/16 [00:04<00:00,  3.50it/s]\n",
            "02/04/2022 16:32:30 - INFO - trainer -   id_test: 2.09845    time_test: 0.77840  total_loss: 2.87686\n",
            "epoch 349  id_train: 1.92897  time_train: 0.73093  total_loss: 2.65990 lr 4.813139e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:32:36 - INFO - trainer -   id_test: 2.11188    time_test: 0.81598  total_loss: 2.92786\n",
            "epoch 350  id_train: 1.93401  time_train: 0.75385  total_loss: 2.68787 lr 4.797715e-05: 100%|██████████| 16/16 [00:04<00:00,  3.41it/s]\n",
            "02/04/2022 16:32:41 - INFO - trainer -   id_test: 2.09220    time_test: 0.81387  total_loss: 2.90607\n",
            "epoch 351  id_train: 1.94840  time_train: 0.72821  total_loss: 2.67661 lr 4.782279e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:32:47 - INFO - trainer -   id_test: 2.11116    time_test: 0.80895  total_loss: 2.92011\n",
            "epoch 352  id_train: 1.88142  time_train: 0.75660  total_loss: 2.63802 lr 4.766830e-05: 100%|██████████| 16/16 [00:04<00:00,  3.53it/s]\n",
            "02/04/2022 16:32:52 - INFO - trainer -   id_test: 2.09608    time_test: 0.80936  total_loss: 2.90544\n",
            "epoch 353  id_train: 1.96486  time_train: 0.76219  total_loss: 2.72706 lr 4.751370e-05: 100%|██████████| 16/16 [00:04<00:00,  3.53it/s]\n",
            "02/04/2022 16:32:58 - INFO - trainer -   id_test: 2.10886    time_test: 0.80351  total_loss: 2.91237\n",
            "epoch 354  id_train: 1.93855  time_train: 0.76855  total_loss: 2.70710 lr 4.735898e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:33:03 - INFO - trainer -   id_test: 2.11195    time_test: 0.79904  total_loss: 2.91100\n",
            "epoch 355  id_train: 1.93992  time_train: 0.72772  total_loss: 2.66764 lr 4.720414e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:33:09 - INFO - trainer -   id_test: 2.08424    time_test: 0.78171  total_loss: 2.86596\n",
            "epoch 356  id_train: 1.94337  time_train: 0.76424  total_loss: 2.70761 lr 4.704920e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:33:14 - INFO - trainer -   id_test: 2.11905    time_test: 0.78707  total_loss: 2.90612\n",
            "epoch 357  id_train: 1.95685  time_train: 0.73720  total_loss: 2.69405 lr 4.689414e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:33:20 - INFO - trainer -   id_test: 2.09902    time_test: 0.79500  total_loss: 2.89402\n",
            "epoch 358  id_train: 1.97218  time_train: 0.74261  total_loss: 2.71478 lr 4.673898e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:33:25 - INFO - trainer -   id_test: 2.10814    time_test: 0.80348  total_loss: 2.91162\n",
            "epoch 359  id_train: 1.90381  time_train: 0.75378  total_loss: 2.65759 lr 4.658371e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:33:31 - INFO - trainer -   id_test: 2.12598    time_test: 0.79297  total_loss: 2.91895\n",
            "epoch 360  id_train: 1.91301  time_train: 0.73024  total_loss: 2.64325 lr 4.642835e-05: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n",
            "02/04/2022 16:33:36 - INFO - trainer -   id_test: 2.09971    time_test: 0.81569  total_loss: 2.91540\n",
            "epoch 361  id_train: 2.00095  time_train: 0.75477  total_loss: 2.75572 lr 4.627288e-05: 100%|██████████| 16/16 [00:04<00:00,  3.53it/s]\n",
            "02/04/2022 16:33:42 - INFO - trainer -   id_test: 2.08019    time_test: 0.80992  total_loss: 2.89011\n",
            "epoch 362  id_train: 1.92155  time_train: 0.75787  total_loss: 2.67941 lr 4.611731e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:33:47 - INFO - trainer -   id_test: 2.12828    time_test: 0.81733  total_loss: 2.94561\n",
            "epoch 363  id_train: 1.91666  time_train: 0.77756  total_loss: 2.69422 lr 4.596165e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:33:53 - INFO - trainer -   id_test: 2.13781    time_test: 0.80081  total_loss: 2.93863\n",
            "epoch 364  id_train: 1.94097  time_train: 0.75412  total_loss: 2.69510 lr 4.580590e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:33:58 - INFO - trainer -   id_test: 2.13144    time_test: 0.79856  total_loss: 2.93000\n",
            "epoch 365  id_train: 1.94437  time_train: 0.76575  total_loss: 2.71011 lr 4.565006e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:34:04 - INFO - trainer -   id_test: 2.13841    time_test: 0.78411  total_loss: 2.92251\n",
            "epoch 366  id_train: 1.98637  time_train: 0.75836  total_loss: 2.74472 lr 4.549412e-05: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n",
            "02/04/2022 16:34:09 - INFO - trainer -   id_test: 2.08861    time_test: 0.80673  total_loss: 2.89534\n",
            "epoch 367  id_train: 1.87292  time_train: 0.74414  total_loss: 2.61706 lr 4.533811e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:34:15 - INFO - trainer -   id_test: 2.14856    time_test: 0.80132  total_loss: 2.94988\n",
            "epoch 368  id_train: 1.88448  time_train: 0.74962  total_loss: 2.63410 lr 4.518201e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:34:20 - INFO - trainer -   id_test: 2.11052    time_test: 0.81119  total_loss: 2.92171\n",
            "epoch 369  id_train: 1.97142  time_train: 0.74101  total_loss: 2.71243 lr 4.502583e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:34:26 - INFO - trainer -   id_test: 2.12846    time_test: 0.80326  total_loss: 2.93172\n",
            "epoch 370  id_train: 1.89679  time_train: 0.73980  total_loss: 2.63659 lr 4.486958e-05: 100%|██████████| 16/16 [00:05<00:00,  3.14it/s]\n",
            "02/04/2022 16:34:32 - INFO - trainer -   id_test: 2.14553    time_test: 0.81436  total_loss: 2.95989\n",
            "epoch 371  id_train: 1.96707  time_train: 0.75358  total_loss: 2.72065 lr 4.471324e-05: 100%|██████████| 16/16 [00:04<00:00,  3.24it/s]\n",
            "02/04/2022 16:34:38 - INFO - trainer -   id_test: 2.11211    time_test: 0.78234  total_loss: 2.89445\n",
            "epoch 372  id_train: 1.96350  time_train: 0.74682  total_loss: 2.71032 lr 4.455684e-05: 100%|██████████| 16/16 [00:04<00:00,  3.60it/s]\n",
            "02/04/2022 16:34:43 - INFO - trainer -   id_test: 2.10244    time_test: 0.79328  total_loss: 2.89572\n",
            "epoch 373  id_train: 1.90188  time_train: 0.73543  total_loss: 2.63731 lr 4.440036e-05: 100%|██████████| 16/16 [00:04<00:00,  3.61it/s]\n",
            "02/04/2022 16:34:48 - INFO - trainer -   id_test: 2.11075    time_test: 0.79500  total_loss: 2.90574\n",
            "epoch 374  id_train: 1.90945  time_train: 0.74193  total_loss: 2.65139 lr 4.424382e-05: 100%|██████████| 16/16 [00:04<00:00,  3.56it/s]\n",
            "02/04/2022 16:34:54 - INFO - trainer -   id_test: 2.07435    time_test: 0.81629  total_loss: 2.89064\n",
            "epoch 375  id_train: 1.89902  time_train: 0.77771  total_loss: 2.67673 lr 4.408721e-05: 100%|██████████| 16/16 [00:04<00:00,  3.59it/s]\n",
            "02/04/2022 16:34:59 - INFO - trainer -   id_test: 2.09939    time_test: 0.83082  total_loss: 2.93021\n",
            "epoch 376  id_train: 1.95114  time_train: 0.77064  total_loss: 2.72178 lr 4.393054e-05: 100%|██████████| 16/16 [00:04<00:00,  3.62it/s]\n",
            "02/04/2022 16:35:05 - INFO - trainer -   id_test: 2.11974    time_test: 0.80451  total_loss: 2.92425\n",
            "epoch 377  id_train: 1.91040  time_train: 0.73680  total_loss: 2.64720 lr 4.377380e-05: 100%|██████████| 16/16 [00:04<00:00,  3.58it/s]\n",
            "02/04/2022 16:35:10 - INFO - trainer -   id_test: 2.12639    time_test: 0.79050  total_loss: 2.91690\n",
            "epoch 378  id_train: 1.95113  time_train: 0.75876  total_loss: 2.70989 lr 4.361701e-05: 100%|██████████| 16/16 [00:04<00:00,  3.54it/s]\n",
            "02/04/2022 16:35:15 - INFO - trainer -   id_test: 2.12135    time_test: 0.81003  total_loss: 2.93138\n",
            "epoch 379  id_train: 1.87916  time_train: 0.74739  total_loss: 2.62655 lr 4.346016e-05: 100%|██████████| 16/16 [00:04<00:00,  3.63it/s]\n",
            "02/04/2022 16:35:21 - INFO - trainer -   id_test: 2.14318    time_test: 0.79835  total_loss: 2.94152\n",
            "epoch 380  id_train: 1.99152  time_train: 0.77191  total_loss: 2.76343 lr 4.330326e-05: 100%|██████████| 16/16 [00:04<00:00,  3.63it/s]\n",
            "02/04/2022 16:35:26 - INFO - trainer -   id_test: 2.07088    time_test: 0.82249  total_loss: 2.89337\n",
            "epoch 381  id_train: 1.92850  time_train: 0.76227  total_loss: 2.69077 lr 4.314631e-05: 100%|██████████| 16/16 [00:04<00:00,  3.67it/s]\n",
            "02/04/2022 16:35:31 - INFO - trainer -   id_test: 2.15603    time_test: 0.82157  total_loss: 2.97760\n",
            "epoch 382  id_train: 1.88070  time_train: 0.75829  total_loss: 2.63899 lr 4.298931e-05: 100%|██████████| 16/16 [00:04<00:00,  3.58it/s]\n",
            "02/04/2022 16:35:37 - INFO - trainer -   id_test: 2.11793    time_test: 0.79912  total_loss: 2.91705\n",
            "epoch 383  id_train: 1.99277  time_train: 0.72815  total_loss: 2.72092 lr 4.283226e-05: 100%|██████████| 16/16 [00:04<00:00,  3.66it/s]\n",
            "02/04/2022 16:35:42 - INFO - trainer -   id_test: 2.09541    time_test: 0.82338  total_loss: 2.91878\n",
            "epoch 384  id_train: 1.92468  time_train: 0.73810  total_loss: 2.66278 lr 4.267517e-05: 100%|██████████| 16/16 [00:04<00:00,  3.57it/s]\n",
            "02/04/2022 16:35:47 - INFO - trainer -   id_test: 2.12198    time_test: 0.83666  total_loss: 2.95864\n",
            "epoch 385  id_train: 1.83029  time_train: 0.73871  total_loss: 2.56900 lr 4.251803e-05: 100%|██████████| 16/16 [00:04<00:00,  3.58it/s]\n",
            "02/04/2022 16:35:53 - INFO - trainer -   id_test: 2.10437    time_test: 0.82296  total_loss: 2.92733\n",
            "epoch 386  id_train: 1.96079  time_train: 0.70742  total_loss: 2.66821 lr 4.236086e-05: 100%|██████████| 16/16 [00:04<00:00,  3.58it/s]\n",
            "02/04/2022 16:35:58 - INFO - trainer -   id_test: 2.14431    time_test: 0.77499  total_loss: 2.91931\n",
            "epoch 387  id_train: 1.85649  time_train: 0.75166  total_loss: 2.60816 lr 4.220365e-05: 100%|██████████| 16/16 [00:04<00:00,  3.58it/s]\n",
            "02/04/2022 16:36:03 - INFO - trainer -   id_test: 2.17947    time_test: 0.79502  total_loss: 2.97449\n",
            "epoch 388  id_train: 1.92075  time_train: 0.77502  total_loss: 2.69577 lr 4.204641e-05: 100%|██████████| 16/16 [00:04<00:00,  3.59it/s]\n",
            "02/04/2022 16:36:09 - INFO - trainer -   id_test: 2.10982    time_test: 0.82875  total_loss: 2.93857\n",
            "epoch 389  id_train: 1.91926  time_train: 0.75766  total_loss: 2.67692 lr 4.188913e-05: 100%|██████████| 16/16 [00:04<00:00,  3.50it/s]\n",
            "02/04/2022 16:36:14 - INFO - trainer -   id_test: 2.09883    time_test: 0.80048  total_loss: 2.89931\n",
            "epoch 390  id_train: 1.94880  time_train: 0.75403  total_loss: 2.70283 lr 4.173183e-05: 100%|██████████| 16/16 [00:04<00:00,  3.24it/s]\n",
            "02/04/2022 16:36:20 - INFO - trainer -   id_test: 2.09816    time_test: 0.80074  total_loss: 2.89890\n",
            "epoch 391  id_train: 1.94145  time_train: 0.72054  total_loss: 2.66199 lr 4.157450e-05: 100%|██████████| 16/16 [00:05<00:00,  3.19it/s]\n",
            "02/04/2022 16:36:26 - INFO - trainer -   id_test: 2.11039    time_test: 0.79978  total_loss: 2.91017\n",
            "epoch 392  id_train: 1.99384  time_train: 0.74280  total_loss: 2.73664 lr 4.141714e-05: 100%|██████████| 16/16 [00:04<00:00,  3.54it/s]\n",
            "02/04/2022 16:36:32 - INFO - trainer -   id_test: 2.14523    time_test: 0.78980  total_loss: 2.93503\n",
            "epoch 393  id_train: 1.92393  time_train: 0.75305  total_loss: 2.67698 lr 4.125976e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:36:37 - INFO - trainer -   id_test: 2.09330    time_test: 0.80813  total_loss: 2.90143\n",
            "epoch 394  id_train: 1.87878  time_train: 0.74758  total_loss: 2.62635 lr 4.110237e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:36:43 - INFO - trainer -   id_test: 2.12939    time_test: 0.81046  total_loss: 2.93985\n",
            "epoch 395  id_train: 1.90589  time_train: 0.74499  total_loss: 2.65088 lr 4.094495e-05: 100%|██████████| 16/16 [00:04<00:00,  3.22it/s]\n",
            "02/04/2022 16:36:49 - INFO - trainer -   id_test: 2.08414    time_test: 0.82227  total_loss: 2.90641\n",
            "epoch 396  id_train: 1.94355  time_train: 0.72505  total_loss: 2.66860 lr 4.078752e-05: 100%|██████████| 16/16 [00:04<00:00,  3.26it/s]\n",
            "02/04/2022 16:36:54 - INFO - trainer -   id_test: 2.15039    time_test: 0.78990  total_loss: 2.94029\n",
            "epoch 397  id_train: 1.88993  time_train: 0.75785  total_loss: 2.64778 lr 4.063008e-05: 100%|██████████| 16/16 [00:04<00:00,  3.52it/s]\n",
            "02/04/2022 16:37:00 - INFO - trainer -   id_test: 2.08277    time_test: 0.82075  total_loss: 2.90351\n",
            "epoch 398  id_train: 1.86802  time_train: 0.78777  total_loss: 2.65580 lr 4.047263e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:37:05 - INFO - trainer -   id_test: 2.10332    time_test: 0.79431  total_loss: 2.89763\n",
            "epoch 399  id_train: 1.90883  time_train: 0.72022  total_loss: 2.62904 lr 4.031518e-05: 100%|██████████| 16/16 [00:04<00:00,  3.52it/s]\n",
            "02/04/2022 16:37:11 - INFO - trainer -   id_test: 2.15574    time_test: 0.81129  total_loss: 2.96703\n",
            "epoch 400  id_train: 1.95457  time_train: 0.74194  total_loss: 2.69651 lr 4.015771e-05: 100%|██████████| 16/16 [00:04<00:00,  3.52it/s]\n",
            "02/04/2022 16:37:16 - INFO - trainer -   id_test: 2.15163    time_test: 0.79750  total_loss: 2.94912\n",
            "epoch 401  id_train: 1.89902  time_train: 0.72575  total_loss: 2.62477 lr 4.000025e-05: 100%|██████████| 16/16 [00:04<00:00,  3.54it/s]\n",
            "02/04/2022 16:37:22 - INFO - trainer -   id_test: 2.13535    time_test: 0.80536  total_loss: 2.94071\n",
            "epoch 402  id_train: 1.88199  time_train: 0.76418  total_loss: 2.64617 lr 3.984278e-05: 100%|██████████| 16/16 [00:04<00:00,  3.50it/s]\n",
            "02/04/2022 16:37:27 - INFO - trainer -   id_test: 2.14845    time_test: 0.80154  total_loss: 2.94999\n",
            "epoch 403  id_train: 1.94044  time_train: 0.72840  total_loss: 2.66884 lr 3.968532e-05: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n",
            "02/04/2022 16:37:33 - INFO - trainer -   id_test: 2.09103    time_test: 0.79781  total_loss: 2.88884\n",
            "epoch 404  id_train: 1.93909  time_train: 0.71637  total_loss: 2.65547 lr 3.952786e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:37:38 - INFO - trainer -   id_test: 2.18133    time_test: 0.80444  total_loss: 2.98577\n",
            "epoch 405  id_train: 1.79593  time_train: 0.77629  total_loss: 2.57222 lr 3.937041e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:37:44 - INFO - trainer -   id_test: 2.10840    time_test: 0.81199  total_loss: 2.92039\n",
            "epoch 406  id_train: 1.91085  time_train: 0.76210  total_loss: 2.67295 lr 3.921297e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:37:49 - INFO - trainer -   id_test: 2.10176    time_test: 0.85337  total_loss: 2.95513\n",
            "epoch 407  id_train: 1.81220  time_train: 0.76074  total_loss: 2.57294 lr 3.905554e-05: 100%|██████████| 16/16 [00:04<00:00,  3.52it/s]\n",
            "02/04/2022 16:37:55 - INFO - trainer -   id_test: 2.11435    time_test: 0.79880  total_loss: 2.91315\n",
            "epoch 408  id_train: 1.92371  time_train: 0.73300  total_loss: 2.65671 lr 3.889813e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:38:00 - INFO - trainer -   id_test: 2.10022    time_test: 0.80590  total_loss: 2.90612\n",
            "epoch 409  id_train: 1.90629  time_train: 0.75565  total_loss: 2.66193 lr 3.874073e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:38:06 - INFO - trainer -   id_test: 2.13039    time_test: 0.79554  total_loss: 2.92593\n",
            "epoch 410  id_train: 1.89539  time_train: 0.72389  total_loss: 2.61928 lr 3.858335e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:38:11 - INFO - trainer -   id_test: 2.11779    time_test: 0.79042  total_loss: 2.90822\n",
            "epoch 411  id_train: 1.94936  time_train: 0.76305  total_loss: 2.71241 lr 3.842600e-05: 100%|██████████| 16/16 [00:04<00:00,  3.44it/s]\n",
            "02/04/2022 16:38:17 - INFO - trainer -   id_test: 2.06661    time_test: 0.81605  total_loss: 2.88265\n",
            "epoch 412  id_train: 1.83935  time_train: 0.75133  total_loss: 2.59068 lr 3.826866e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:38:22 - INFO - trainer -   id_test: 2.14274    time_test: 0.79379  total_loss: 2.93653\n",
            "epoch 413  id_train: 1.90041  time_train: 0.75422  total_loss: 2.65464 lr 3.811136e-05: 100%|██████████| 16/16 [00:04<00:00,  3.44it/s]\n",
            "02/04/2022 16:38:28 - INFO - trainer -   id_test: 2.16840    time_test: 0.82226  total_loss: 2.99066\n",
            "epoch 414  id_train: 1.93748  time_train: 0.73586  total_loss: 2.67334 lr 3.795409e-05: 100%|██████████| 16/16 [00:04<00:00,  3.50it/s]\n",
            "02/04/2022 16:38:33 - INFO - trainer -   id_test: 2.13288    time_test: 0.79312  total_loss: 2.92600\n",
            "epoch 415  id_train: 1.88213  time_train: 0.75793  total_loss: 2.64006 lr 3.779684e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:38:39 - INFO - trainer -   id_test: 2.08780    time_test: 0.79988  total_loss: 2.88768\n",
            "epoch 416  id_train: 1.93211  time_train: 0.73297  total_loss: 2.66508 lr 3.763963e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:38:44 - INFO - trainer -   id_test: 2.10659    time_test: 0.81259  total_loss: 2.91919\n",
            "epoch 417  id_train: 1.86220  time_train: 0.76318  total_loss: 2.62538 lr 3.748246e-05: 100%|██████████| 16/16 [00:04<00:00,  3.40it/s]\n",
            "02/04/2022 16:38:50 - INFO - trainer -   id_test: 2.09978    time_test: 0.80972  total_loss: 2.90950\n",
            "epoch 418  id_train: 1.86890  time_train: 0.78753  total_loss: 2.65643 lr 3.732533e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:38:55 - INFO - trainer -   id_test: 2.11719    time_test: 0.80823  total_loss: 2.92542\n",
            "epoch 419  id_train: 1.86204  time_train: 0.75999  total_loss: 2.62203 lr 3.716823e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:39:01 - INFO - trainer -   id_test: 2.11881    time_test: 0.80717  total_loss: 2.92598\n",
            "epoch 420  id_train: 1.96969  time_train: 0.74877  total_loss: 2.71846 lr 3.701119e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:39:06 - INFO - trainer -   id_test: 2.13426    time_test: 0.79666  total_loss: 2.93093\n",
            "epoch 421  id_train: 1.89680  time_train: 0.74833  total_loss: 2.64514 lr 3.685418e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:39:12 - INFO - trainer -   id_test: 2.12460    time_test: 0.81489  total_loss: 2.93949\n",
            "epoch 422  id_train: 1.84985  time_train: 0.74545  total_loss: 2.59530 lr 3.669723e-05: 100%|██████████| 16/16 [00:04<00:00,  3.44it/s]\n",
            "02/04/2022 16:39:18 - INFO - trainer -   id_test: 2.10341    time_test: 0.83636  total_loss: 2.93977\n",
            "epoch 423  id_train: 1.94920  time_train: 0.70141  total_loss: 2.65061 lr 3.654033e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:39:23 - INFO - trainer -   id_test: 2.12743    time_test: 0.78679  total_loss: 2.91422\n",
            "epoch 424  id_train: 1.92878  time_train: 0.73652  total_loss: 2.66529 lr 3.638348e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:39:29 - INFO - trainer -   id_test: 2.14190    time_test: 0.80710  total_loss: 2.94900\n",
            "epoch 425  id_train: 1.91432  time_train: 0.73939  total_loss: 2.65371 lr 3.622669e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:39:34 - INFO - trainer -   id_test: 2.10884    time_test: 0.80604  total_loss: 2.91489\n",
            "epoch 426  id_train: 1.83613  time_train: 0.75808  total_loss: 2.59421 lr 3.606995e-05: 100%|██████████| 16/16 [00:04<00:00,  3.57it/s]\n",
            "02/04/2022 16:39:40 - INFO - trainer -   id_test: 2.11031    time_test: 0.81788  total_loss: 2.92819\n",
            "epoch 427  id_train: 1.83146  time_train: 0.76571  total_loss: 2.59717 lr 3.591328e-05: 100%|██████████| 16/16 [00:04<00:00,  3.50it/s]\n",
            "02/04/2022 16:39:45 - INFO - trainer -   id_test: 2.13639    time_test: 0.81653  total_loss: 2.95292\n",
            "epoch 428  id_train: 1.88279  time_train: 0.76161  total_loss: 2.64440 lr 3.575667e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:39:51 - INFO - trainer -   id_test: 2.09300    time_test: 0.81427  total_loss: 2.90727\n",
            "epoch 429  id_train: 1.98761  time_train: 0.72761  total_loss: 2.71523 lr 3.560013e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:39:56 - INFO - trainer -   id_test: 2.12266    time_test: 0.80868  total_loss: 2.93135\n",
            "epoch 430  id_train: 1.86710  time_train: 0.73620  total_loss: 2.60331 lr 3.544365e-05: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n",
            "02/04/2022 16:40:02 - INFO - trainer -   id_test: 2.12268    time_test: 0.80464  total_loss: 2.92732\n",
            "epoch 431  id_train: 1.86915  time_train: 0.76781  total_loss: 2.63696 lr 3.528725e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:40:07 - INFO - trainer -   id_test: 2.13666    time_test: 0.83707  total_loss: 2.97373\n",
            "epoch 432  id_train: 1.87396  time_train: 0.75417  total_loss: 2.62813 lr 3.513091e-05: 100%|██████████| 16/16 [00:04<00:00,  3.43it/s]\n",
            "02/04/2022 16:40:13 - INFO - trainer -   id_test: 2.13501    time_test: 0.81177  total_loss: 2.94678\n",
            "epoch 433  id_train: 1.84449  time_train: 0.71991  total_loss: 2.56440 lr 3.497466e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:40:18 - INFO - trainer -   id_test: 2.13208    time_test: 0.78953  total_loss: 2.92161\n",
            "epoch 434  id_train: 1.88299  time_train: 0.76460  total_loss: 2.64758 lr 3.481848e-05: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n",
            "02/04/2022 16:40:24 - INFO - trainer -   id_test: 2.11102    time_test: 0.81272  total_loss: 2.92375\n",
            "epoch 435  id_train: 1.86024  time_train: 0.74337  total_loss: 2.60361 lr 3.466238e-05: 100%|██████████| 16/16 [00:04<00:00,  3.41it/s]\n",
            "02/04/2022 16:40:29 - INFO - trainer -   id_test: 2.10145    time_test: 0.79812  total_loss: 2.89957\n",
            "epoch 436  id_train: 1.82397  time_train: 0.75468  total_loss: 2.57865 lr 3.450636e-05: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n",
            "02/04/2022 16:40:35 - INFO - trainer -   id_test: 2.13664    time_test: 0.80546  total_loss: 2.94209\n",
            "epoch 437  id_train: 1.86080  time_train: 0.73742  total_loss: 2.59822 lr 3.435043e-05: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n",
            "02/04/2022 16:40:40 - INFO - trainer -   id_test: 2.11204    time_test: 0.79027  total_loss: 2.90231\n",
            "epoch 438  id_train: 1.84622  time_train: 0.74973  total_loss: 2.59595 lr 3.419459e-05: 100%|██████████| 16/16 [00:04<00:00,  3.53it/s]\n",
            "02/04/2022 16:40:46 - INFO - trainer -   id_test: 2.10566    time_test: 0.82144  total_loss: 2.92711\n",
            "epoch 439  id_train: 1.89720  time_train: 0.74142  total_loss: 2.63863 lr 3.403884e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:40:51 - INFO - trainer -   id_test: 2.12584    time_test: 0.79727  total_loss: 2.92311\n",
            "epoch 440  id_train: 1.85528  time_train: 0.76802  total_loss: 2.62330 lr 3.388317e-05: 100%|██████████| 16/16 [00:04<00:00,  3.56it/s]\n",
            "02/04/2022 16:40:57 - INFO - trainer -   id_test: 2.13573    time_test: 0.79586  total_loss: 2.93159\n",
            "epoch 441  id_train: 1.89210  time_train: 0.77133  total_loss: 2.66344 lr 3.372761e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:41:02 - INFO - trainer -   id_test: 2.14048    time_test: 0.82054  total_loss: 2.96102\n",
            "epoch 442  id_train: 1.84678  time_train: 0.73574  total_loss: 2.58253 lr 3.357215e-05: 100%|██████████| 16/16 [00:04<00:00,  3.52it/s]\n",
            "02/04/2022 16:41:08 - INFO - trainer -   id_test: 2.07965    time_test: 0.80361  total_loss: 2.88326\n",
            "epoch 443  id_train: 1.91254  time_train: 0.77447  total_loss: 2.68701 lr 3.341677e-05: 100%|██████████| 16/16 [00:04<00:00,  3.36it/s]\n",
            "02/04/2022 16:41:13 - INFO - trainer -   id_test: 2.08618    time_test: 0.82314  total_loss: 2.90932\n",
            "epoch 444  id_train: 1.79324  time_train: 0.78818  total_loss: 2.58141 lr 3.326150e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:41:19 - INFO - trainer -   id_test: 2.17146    time_test: 0.80596  total_loss: 2.97741\n",
            "epoch 445  id_train: 1.84939  time_train: 0.74805  total_loss: 2.59743 lr 3.310634e-05: 100%|██████████| 16/16 [00:04<00:00,  3.41it/s]\n",
            "02/04/2022 16:41:24 - INFO - trainer -   id_test: 2.07641    time_test: 0.81343  total_loss: 2.88984\n",
            "epoch 446  id_train: 1.97382  time_train: 0.70317  total_loss: 2.67699 lr 3.295129e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:41:30 - INFO - trainer -   id_test: 2.15240    time_test: 0.81673  total_loss: 2.96913\n",
            "epoch 447  id_train: 1.77484  time_train: 0.74754  total_loss: 2.52238 lr 3.279634e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:41:35 - INFO - trainer -   id_test: 2.13653    time_test: 0.80513  total_loss: 2.94166\n",
            "epoch 448  id_train: 1.96702  time_train: 0.71923  total_loss: 2.68624 lr 3.264150e-05: 100%|██████████| 16/16 [00:04<00:00,  3.41it/s]\n",
            "02/04/2022 16:41:41 - INFO - trainer -   id_test: 2.14205    time_test: 0.82363  total_loss: 2.96568\n",
            "epoch 449  id_train: 1.82402  time_train: 0.75356  total_loss: 2.57758 lr 3.248679e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:41:47 - INFO - trainer -   id_test: 2.09496    time_test: 0.82664  total_loss: 2.92159\n",
            "epoch 450  id_train: 1.92681  time_train: 0.72792  total_loss: 2.65473 lr 3.233219e-05: 100%|██████████| 16/16 [00:04<00:00,  3.53it/s]\n",
            "02/04/2022 16:41:52 - INFO - trainer -   id_test: 2.12013    time_test: 0.80434  total_loss: 2.92446\n",
            "epoch 451  id_train: 1.87602  time_train: 0.71429  total_loss: 2.59032 lr 3.217770e-05: 100%|██████████| 16/16 [00:04<00:00,  3.53it/s]\n",
            "02/04/2022 16:41:57 - INFO - trainer -   id_test: 2.12392    time_test: 0.80452  total_loss: 2.92844\n",
            "epoch 452  id_train: 1.82374  time_train: 0.78718  total_loss: 2.61092 lr 3.202333e-05: 100%|██████████| 16/16 [00:04<00:00,  3.43it/s]\n",
            "02/04/2022 16:42:03 - INFO - trainer -   id_test: 2.18684    time_test: 0.79336  total_loss: 2.98020\n",
            "epoch 453  id_train: 1.84365  time_train: 0.75621  total_loss: 2.59986 lr 3.186909e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:42:09 - INFO - trainer -   id_test: 2.09978    time_test: 0.82107  total_loss: 2.92085\n",
            "epoch 454  id_train: 1.97886  time_train: 0.72290  total_loss: 2.70176 lr 3.171498e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:42:14 - INFO - trainer -   id_test: 2.13978    time_test: 0.80285  total_loss: 2.94263\n",
            "epoch 455  id_train: 1.93451  time_train: 0.74299  total_loss: 2.67750 lr 3.156099e-05: 100%|██████████| 16/16 [00:04<00:00,  3.50it/s]\n",
            "02/04/2022 16:42:20 - INFO - trainer -   id_test: 2.10858    time_test: 0.81331  total_loss: 2.92190\n",
            "epoch 456  id_train: 1.85533  time_train: 0.74339  total_loss: 2.59872 lr 3.140713e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:42:25 - INFO - trainer -   id_test: 2.10840    time_test: 0.81408  total_loss: 2.92248\n",
            "epoch 457  id_train: 1.88108  time_train: 0.74238  total_loss: 2.62346 lr 3.125341e-05: 100%|██████████| 16/16 [00:04<00:00,  3.43it/s]\n",
            "02/04/2022 16:42:31 - INFO - trainer -   id_test: 2.10843    time_test: 0.80894  total_loss: 2.91736\n",
            "epoch 458  id_train: 1.95957  time_train: 0.72599  total_loss: 2.68555 lr 3.109983e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:42:36 - INFO - trainer -   id_test: 2.16179    time_test: 0.82248  total_loss: 2.98427\n",
            "epoch 459  id_train: 1.93744  time_train: 0.72858  total_loss: 2.66603 lr 3.094638e-05: 100%|██████████| 16/16 [00:04<00:00,  3.23it/s]\n",
            "02/04/2022 16:42:42 - INFO - trainer -   id_test: 2.16341    time_test: 0.81874  total_loss: 2.98216\n",
            "epoch 460  id_train: 1.85002  time_train: 0.76533  total_loss: 2.61535 lr 3.079306e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:42:48 - INFO - trainer -   id_test: 2.13570    time_test: 0.78327  total_loss: 2.91897\n",
            "epoch 461  id_train: 1.99630  time_train: 0.74003  total_loss: 2.73633 lr 3.063990e-05: 100%|██████████| 16/16 [00:04<00:00,  3.54it/s]\n",
            "02/04/2022 16:42:53 - INFO - trainer -   id_test: 2.07380    time_test: 0.80544  total_loss: 2.87925\n",
            "epoch 462  id_train: 1.87429  time_train: 0.76669  total_loss: 2.64097 lr 3.048688e-05: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n",
            "02/04/2022 16:42:59 - INFO - trainer -   id_test: 2.16413    time_test: 0.81183  total_loss: 2.97595\n",
            "epoch 463  id_train: 1.87741  time_train: 0.76635  total_loss: 2.64376 lr 3.033401e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:43:04 - INFO - trainer -   id_test: 2.12037    time_test: 0.80692  total_loss: 2.92728\n",
            "epoch 464  id_train: 1.90774  time_train: 0.70592  total_loss: 2.61365 lr 3.018128e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:43:10 - INFO - trainer -   id_test: 2.12959    time_test: 0.79003  total_loss: 2.91963\n",
            "epoch 465  id_train: 1.91076  time_train: 0.73442  total_loss: 2.64518 lr 3.002871e-05: 100%|██████████| 16/16 [00:04<00:00,  3.44it/s]\n",
            "02/04/2022 16:43:15 - INFO - trainer -   id_test: 2.19496    time_test: 0.81634  total_loss: 3.01131\n",
            "epoch 466  id_train: 1.93782  time_train: 0.72329  total_loss: 2.66111 lr 2.987630e-05: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n",
            "02/04/2022 16:43:21 - INFO - trainer -   id_test: 2.09998    time_test: 0.80789  total_loss: 2.90787\n",
            "epoch 467  id_train: 1.96210  time_train: 0.73211  total_loss: 2.69421 lr 2.972403e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:43:26 - INFO - trainer -   id_test: 2.11566    time_test: 0.82791  total_loss: 2.94357\n",
            "epoch 468  id_train: 1.96893  time_train: 0.69828  total_loss: 2.66722 lr 2.957193e-05: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n",
            "02/04/2022 16:43:32 - INFO - trainer -   id_test: 2.12239    time_test: 0.79949  total_loss: 2.92188\n",
            "epoch 469  id_train: 1.90633  time_train: 0.75855  total_loss: 2.66488 lr 2.941999e-05: 100%|██████████| 16/16 [00:04<00:00,  3.42it/s]\n",
            "02/04/2022 16:43:37 - INFO - trainer -   id_test: 2.11968    time_test: 0.81681  total_loss: 2.93650\n",
            "epoch 470  id_train: 1.84835  time_train: 0.73380  total_loss: 2.58215 lr 2.926822e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:43:43 - INFO - trainer -   id_test: 2.08332    time_test: 0.81754  total_loss: 2.90086\n",
            "epoch 471  id_train: 1.96940  time_train: 0.73752  total_loss: 2.70692 lr 2.911661e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:43:48 - INFO - trainer -   id_test: 2.07601    time_test: 0.81915  total_loss: 2.89516\n",
            "epoch 472  id_train: 1.89958  time_train: 0.74026  total_loss: 2.63984 lr 2.896516e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:43:54 - INFO - trainer -   id_test: 2.08114    time_test: 0.82752  total_loss: 2.90866\n",
            "epoch 473  id_train: 1.91453  time_train: 0.75168  total_loss: 2.66621 lr 2.881390e-05: 100%|██████████| 16/16 [00:04<00:00,  3.40it/s]\n",
            "02/04/2022 16:44:00 - INFO - trainer -   id_test: 2.09455    time_test: 0.79885  total_loss: 2.89340\n",
            "epoch 474  id_train: 1.89829  time_train: 0.74551  total_loss: 2.64380 lr 2.866281e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:44:05 - INFO - trainer -   id_test: 2.12487    time_test: 0.81583  total_loss: 2.94070\n",
            "epoch 475  id_train: 1.94076  time_train: 0.70161  total_loss: 2.64236 lr 2.851188e-05: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n",
            "02/04/2022 16:44:11 - INFO - trainer -   id_test: 2.09223    time_test: 0.82883  total_loss: 2.92105\n",
            "epoch 476  id_train: 1.93590  time_train: 0.73492  total_loss: 2.67082 lr 2.836114e-05: 100%|██████████| 16/16 [00:05<00:00,  3.16it/s]\n",
            "02/04/2022 16:44:17 - INFO - trainer -   id_test: 2.13077    time_test: 0.85735  total_loss: 2.98813\n",
            "epoch 477  id_train: 1.89876  time_train: 0.71601  total_loss: 2.61477 lr 2.821058e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:44:22 - INFO - trainer -   id_test: 2.14228    time_test: 0.84436  total_loss: 2.98664\n",
            "epoch 478  id_train: 1.96153  time_train: 0.71293  total_loss: 2.67446 lr 2.806020e-05: 100%|██████████| 16/16 [00:04<00:00,  3.57it/s]\n",
            "02/04/2022 16:44:28 - INFO - trainer -   id_test: 2.09758    time_test: 0.79877  total_loss: 2.89635\n",
            "epoch 479  id_train: 1.95884  time_train: 0.73764  total_loss: 2.69648 lr 2.791001e-05: 100%|██████████| 16/16 [00:04<00:00,  3.53it/s]\n",
            "02/04/2022 16:44:33 - INFO - trainer -   id_test: 2.08879    time_test: 0.82929  total_loss: 2.91808\n",
            "epoch 480  id_train: 1.94234  time_train: 0.73224  total_loss: 2.67458 lr 2.775999e-05: 100%|██████████| 16/16 [00:04<00:00,  3.56it/s]\n",
            "02/04/2022 16:44:38 - INFO - trainer -   id_test: 2.10691    time_test: 0.81274  total_loss: 2.91966\n",
            "epoch 481  id_train: 1.87054  time_train: 0.77357  total_loss: 2.64411 lr 2.761018e-05: 100%|██████████| 16/16 [00:04<00:00,  3.43it/s]\n",
            "02/04/2022 16:44:44 - INFO - trainer -   id_test: 2.11816    time_test: 0.82085  total_loss: 2.93901\n",
            "epoch 482  id_train: 1.86142  time_train: 0.70114  total_loss: 2.56256 lr 2.746056e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:44:50 - INFO - trainer -   id_test: 2.12781    time_test: 0.80829  total_loss: 2.93610\n",
            "epoch 483  id_train: 1.81325  time_train: 0.74811  total_loss: 2.56136 lr 2.731113e-05: 100%|██████████| 16/16 [00:04<00:00,  3.53it/s]\n",
            "02/04/2022 16:44:55 - INFO - trainer -   id_test: 2.09407    time_test: 0.83810  total_loss: 2.93217\n",
            "epoch 484  id_train: 1.83462  time_train: 0.77459  total_loss: 2.60921 lr 2.716189e-05: 100%|██████████| 16/16 [00:04<00:00,  3.43it/s]\n",
            "02/04/2022 16:45:01 - INFO - trainer -   id_test: 2.07564    time_test: 0.82811  total_loss: 2.90375\n",
            "epoch 485  id_train: 1.85586  time_train: 0.73468  total_loss: 2.59054 lr 2.701286e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:45:06 - INFO - trainer -   id_test: 2.10788    time_test: 0.82158  total_loss: 2.92947\n",
            "epoch 486  id_train: 1.83396  time_train: 0.72385  total_loss: 2.55781 lr 2.686403e-05: 100%|██████████| 16/16 [00:04<00:00,  3.52it/s]\n",
            "02/04/2022 16:45:11 - INFO - trainer -   id_test: 2.08169    time_test: 0.82521  total_loss: 2.90690\n",
            "epoch 487  id_train: 1.88692  time_train: 0.72762  total_loss: 2.61455 lr 2.671539e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:45:17 - INFO - trainer -   id_test: 2.07558    time_test: 0.82732  total_loss: 2.90290\n",
            "epoch 488  id_train: 1.86384  time_train: 0.70241  total_loss: 2.56625 lr 2.656696e-05: 100%|██████████| 16/16 [00:04<00:00,  3.50it/s]\n",
            "02/04/2022 16:45:22 - INFO - trainer -   id_test: 2.11660    time_test: 0.80675  total_loss: 2.92335\n",
            "epoch 489  id_train: 1.88462  time_train: 0.72147  total_loss: 2.60609 lr 2.641875e-05: 100%|██████████| 16/16 [00:04<00:00,  3.44it/s]\n",
            "02/04/2022 16:45:28 - INFO - trainer -   id_test: 2.13025    time_test: 0.81134  total_loss: 2.94159\n",
            "epoch 490  id_train: 1.83942  time_train: 0.73480  total_loss: 2.57422 lr 2.627075e-05: 100%|██████████| 16/16 [00:04<00:00,  3.41it/s]\n",
            "02/04/2022 16:45:34 - INFO - trainer -   id_test: 2.12064    time_test: 0.78464  total_loss: 2.90528\n",
            "epoch 491  id_train: 1.88591  time_train: 0.74093  total_loss: 2.62685 lr 2.612295e-05: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n",
            "02/04/2022 16:45:39 - INFO - trainer -   id_test: 2.10335    time_test: 0.80558  total_loss: 2.90893\n",
            "epoch 492  id_train: 1.94871  time_train: 0.75241  total_loss: 2.70113 lr 2.597537e-05: 100%|██████████| 16/16 [00:04<00:00,  3.55it/s]\n",
            "02/04/2022 16:45:45 - INFO - trainer -   id_test: 2.11122    time_test: 0.82646  total_loss: 2.93768\n",
            "epoch 493  id_train: 2.00363  time_train: 0.73235  total_loss: 2.73599 lr 2.582801e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:45:50 - INFO - trainer -   id_test: 2.09526    time_test: 0.81442  total_loss: 2.90969\n",
            "epoch 494  id_train: 1.88275  time_train: 0.72464  total_loss: 2.60738 lr 2.568088e-05: 100%|██████████| 16/16 [00:04<00:00,  3.56it/s]\n",
            "02/04/2022 16:45:55 - INFO - trainer -   id_test: 2.10286    time_test: 0.82172  total_loss: 2.92457\n",
            "epoch 495  id_train: 1.90186  time_train: 0.75789  total_loss: 2.65975 lr 2.553395e-05: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n",
            "02/04/2022 16:46:01 - INFO - trainer -   id_test: 2.12813    time_test: 0.80594  total_loss: 2.93407\n",
            "epoch 496  id_train: 1.84674  time_train: 0.73711  total_loss: 2.58385 lr 2.538725e-05: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n",
            "02/04/2022 16:46:07 - INFO - trainer -   id_test: 2.10441    time_test: 0.83005  total_loss: 2.93446\n",
            "epoch 497  id_train: 1.88998  time_train: 0.72287  total_loss: 2.61285 lr 2.524079e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:46:12 - INFO - trainer -   id_test: 2.11002    time_test: 0.83738  total_loss: 2.94740\n",
            "epoch 498  id_train: 1.83750  time_train: 0.77329  total_loss: 2.61078 lr 2.509455e-05: 100%|██████████| 16/16 [00:04<00:00,  3.43it/s]\n",
            "02/04/2022 16:46:18 - INFO - trainer -   id_test: 2.11424    time_test: 0.81106  total_loss: 2.92530\n",
            "epoch 499  id_train: 1.88285  time_train: 0.75514  total_loss: 2.63798 lr 2.494854e-05: 100%|██████████| 16/16 [00:04<00:00,  3.44it/s]\n",
            "02/04/2022 16:46:23 - INFO - trainer -   id_test: 2.15937    time_test: 0.82163  total_loss: 2.98100\n",
            "epoch 500  id_train: 1.87718  time_train: 0.74839  total_loss: 2.62557 lr 2.480276e-05: 100%|██████████| 16/16 [00:04<00:00,  3.43it/s]\n",
            "02/04/2022 16:46:29 - INFO - trainer -   id_test: 2.14333    time_test: 0.78743  total_loss: 2.93075\n",
            "epoch 501  id_train: 1.97458  time_train: 0.76495  total_loss: 2.73953 lr 2.465723e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:46:34 - INFO - trainer -   id_test: 2.18075    time_test: 0.81607  total_loss: 2.99682\n",
            "epoch 502  id_train: 1.87883  time_train: 0.74550  total_loss: 2.62433 lr 2.451193e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:46:40 - INFO - trainer -   id_test: 2.10132    time_test: 0.82523  total_loss: 2.92656\n",
            "epoch 503  id_train: 1.78728  time_train: 0.78749  total_loss: 2.57477 lr 2.436686e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:46:45 - INFO - trainer -   id_test: 2.12489    time_test: 0.84765  total_loss: 2.97254\n",
            "epoch 504  id_train: 1.85966  time_train: 0.73796  total_loss: 2.59763 lr 2.422204e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:46:51 - INFO - trainer -   id_test: 2.06282    time_test: 0.81357  total_loss: 2.87639\n",
            "epoch 505  id_train: 1.88439  time_train: 0.74629  total_loss: 2.63068 lr 2.407747e-05: 100%|██████████| 16/16 [00:04<00:00,  3.43it/s]\n",
            "02/04/2022 16:46:56 - INFO - trainer -   id_test: 2.11492    time_test: 0.83496  total_loss: 2.94989\n",
            "epoch 506  id_train: 1.86145  time_train: 0.74496  total_loss: 2.60641 lr 2.393314e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:47:02 - INFO - trainer -   id_test: 2.14219    time_test: 0.79451  total_loss: 2.93670\n",
            "epoch 507  id_train: 1.83055  time_train: 0.75596  total_loss: 2.58651 lr 2.378906e-05: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n",
            "02/04/2022 16:47:07 - INFO - trainer -   id_test: 2.10088    time_test: 0.82956  total_loss: 2.93045\n",
            "epoch 508  id_train: 1.85166  time_train: 0.74954  total_loss: 2.60121 lr 2.364522e-05: 100%|██████████| 16/16 [00:04<00:00,  3.42it/s]\n",
            "02/04/2022 16:47:13 - INFO - trainer -   id_test: 2.11639    time_test: 0.80797  total_loss: 2.92437\n",
            "epoch 509  id_train: 1.83517  time_train: 0.72260  total_loss: 2.55776 lr 2.350165e-05: 100%|██████████| 16/16 [00:04<00:00,  3.47it/s]\n",
            "02/04/2022 16:47:19 - INFO - trainer -   id_test: 2.13575    time_test: 0.81852  total_loss: 2.95426\n",
            "epoch 510  id_train: 2.04665  time_train: 0.71625  total_loss: 2.76290 lr 2.335834e-05: 100%|██████████| 16/16 [00:04<00:00,  3.50it/s]\n",
            "02/04/2022 16:47:24 - INFO - trainer -   id_test: 2.10952    time_test: 0.83280  total_loss: 2.94232\n",
            "epoch 511  id_train: 1.92755  time_train: 0.71937  total_loss: 2.64692 lr 2.321527e-05: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n",
            "02/04/2022 16:47:30 - INFO - trainer -   id_test: 2.16689    time_test: 0.81851  total_loss: 2.98540\n",
            "epoch 512  id_train: 1.82727  time_train: 0.72718  total_loss: 2.55446 lr 2.307247e-05: 100%|██████████| 16/16 [00:04<00:00,  3.43it/s]\n",
            "02/04/2022 16:47:35 - INFO - trainer -   id_test: 2.13205    time_test: 0.80833  total_loss: 2.94038\n",
            "epoch 513  id_train: 1.93190  time_train: 0.74145  total_loss: 2.67334 lr 2.292993e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:47:41 - INFO - trainer -   id_test: 2.10010    time_test: 0.79457  total_loss: 2.89467\n",
            "epoch 514  id_train: 1.89061  time_train: 0.73602  total_loss: 2.62663 lr 2.278766e-05: 100%|██████████| 16/16 [00:04<00:00,  3.50it/s]\n",
            "02/04/2022 16:47:46 - INFO - trainer -   id_test: 2.10935    time_test: 0.81606  total_loss: 2.92541\n",
            "epoch 515  id_train: 1.86592  time_train: 0.74069  total_loss: 2.60661 lr 2.264565e-05: 100%|██████████| 16/16 [00:04<00:00,  3.29it/s]\n",
            "02/04/2022 16:47:52 - INFO - trainer -   id_test: 2.13347    time_test: 0.81163  total_loss: 2.94510\n",
            "epoch 516  id_train: 1.88565  time_train: 0.70971  total_loss: 2.59536 lr 2.250391e-05: 100%|██████████| 16/16 [00:04<00:00,  3.43it/s]\n",
            "02/04/2022 16:47:57 - INFO - trainer -   id_test: 2.14117    time_test: 0.83219  total_loss: 2.97336\n",
            "epoch 517  id_train: 1.93711  time_train: 0.72156  total_loss: 2.65867 lr 2.236244e-05: 100%|██████████| 16/16 [00:04<00:00,  3.55it/s]\n",
            "02/04/2022 16:48:03 - INFO - trainer -   id_test: 2.09095    time_test: 0.81298  total_loss: 2.90393\n",
            "epoch 518  id_train: 1.89822  time_train: 0.76193  total_loss: 2.66014 lr 2.222125e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:48:08 - INFO - trainer -   id_test: 2.14396    time_test: 0.80345  total_loss: 2.94741\n",
            "epoch 519  id_train: 1.86055  time_train: 0.71854  total_loss: 2.57909 lr 2.208033e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:48:14 - INFO - trainer -   id_test: 2.12468    time_test: 0.80304  total_loss: 2.92772\n",
            "epoch 520  id_train: 1.84636  time_train: 0.74467  total_loss: 2.59103 lr 2.193969e-05: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n",
            "02/04/2022 16:48:19 - INFO - trainer -   id_test: 2.11041    time_test: 0.83508  total_loss: 2.94550\n",
            "epoch 521  id_train: 1.85330  time_train: 0.72911  total_loss: 2.58241 lr 2.179933e-05: 100%|██████████| 16/16 [00:04<00:00,  3.46it/s]\n",
            "02/04/2022 16:48:25 - INFO - trainer -   id_test: 2.13711    time_test: 0.81234  total_loss: 2.94945\n",
            "epoch 522  id_train: 1.92462  time_train: 0.72575  total_loss: 2.65036 lr 2.165925e-05: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
            "02/04/2022 16:48:30 - INFO - trainer -   id_test: 2.13414    time_test: 0.83230  total_loss: 2.96644\n",
            "epoch 523  id_train: 1.84451  time_train: 0.72184  total_loss: 2.56635 lr 2.151945e-05: 100%|██████████| 16/16 [00:04<00:00,  3.24it/s]\n",
            "02/04/2022 16:48:36 - INFO - trainer -   id_test: 2.13487    time_test: 0.82269  total_loss: 2.95756\n",
            "epoch 524  id_train: 1.89272  time_train: 0.75118  total_loss: 2.64390 lr 2.137994e-05: 100%|██████████| 16/16 [00:04<00:00,  3.23it/s]\n",
            "02/04/2022 16:48:42 - INFO - trainer -   id_test: 2.13973    time_test: 0.81083  total_loss: 2.95056\n",
            "epoch 525  id_train: 1.87660  time_train: 0.73690  total_loss: 2.61350 lr 2.124073e-05: 100%|██████████| 16/16 [00:04<00:00,  3.26it/s]\n",
            "02/04/2022 16:48:48 - INFO - trainer -   id_test: 2.13373    time_test: 0.81637  total_loss: 2.95009\n",
            "epoch 526  id_train: 1.79536  time_train: 0.75607  total_loss: 2.55143 lr 2.110180e-05: 100%|██████████| 16/16 [00:04<00:00,  3.27it/s]\n",
            "02/04/2022 16:48:54 - INFO - trainer -   id_test: 2.13342    time_test: 0.81684  total_loss: 2.95027\n",
            "epoch 527  id_train: 1.85665  time_train: 0.73072  total_loss: 2.58737 lr 2.096316e-05: 100%|██████████| 16/16 [00:04<00:00,  3.26it/s]\n",
            "02/04/2022 16:49:00 - INFO - trainer -   id_test: 2.14936    time_test: 0.80530  total_loss: 2.95467\n",
            "epoch 528  id_train: 1.84513  time_train: 0.73867  total_loss: 2.58380 lr 2.082482e-05: 100%|██████████| 16/16 [00:04<00:00,  3.21it/s]\n",
            "02/04/2022 16:49:06 - INFO - trainer -   id_test: 2.09303    time_test: 0.81995  total_loss: 2.91298\n",
            "epoch 529  id_train: 1.89652  time_train: 0.75895  total_loss: 2.65547 lr 2.068677e-05: 100%|██████████| 16/16 [00:04<00:00,  3.26it/s]\n",
            "02/04/2022 16:49:12 - INFO - trainer -   id_test: 2.12901    time_test: 0.84178  total_loss: 2.97079\n",
            "epoch 530  id_train: 1.91335  time_train: 0.73297  total_loss: 2.64632 lr 2.054903e-05: 100%|██████████| 16/16 [00:04<00:00,  3.30it/s]\n",
            "02/04/2022 16:49:18 - INFO - trainer -   id_test: 2.11427    time_test: 0.81875  total_loss: 2.93302\n",
            "epoch 531  id_train: 1.93967  time_train: 0.70235  total_loss: 2.64202 lr 2.041159e-05: 100%|██████████| 16/16 [00:04<00:00,  3.26it/s]\n",
            "02/04/2022 16:49:24 - INFO - trainer -   id_test: 2.14863    time_test: 0.80992  total_loss: 2.95854\n",
            "epoch 532  id_train: 1.84682  time_train: 0.73778  total_loss: 2.58461 lr 2.027444e-05: 100%|██████████| 16/16 [00:04<00:00,  3.25it/s]\n",
            "02/04/2022 16:49:30 - INFO - trainer -   id_test: 2.12026    time_test: 0.81069  total_loss: 2.93095\n",
            "epoch 533  id_train: 1.92209  time_train: 0.71008  total_loss: 2.63217 lr 2.013761e-05: 100%|██████████| 16/16 [00:04<00:00,  3.29it/s]\n",
            "02/04/2022 16:49:36 - INFO - trainer -   id_test: 2.09801    time_test: 0.81380  total_loss: 2.91180\n",
            "epoch 534  id_train: 1.87210  time_train: 0.74085  total_loss: 2.61295 lr 2.000109e-05: 100%|██████████| 16/16 [00:04<00:00,  3.27it/s]\n",
            "02/04/2022 16:49:42 - INFO - trainer -   id_test: 2.14429    time_test: 0.82071  total_loss: 2.96500\n",
            "epoch 535  id_train: 1.81592  time_train: 0.75280  total_loss: 2.56872 lr 1.986487e-05: 100%|██████████| 16/16 [00:04<00:00,  3.26it/s]\n",
            "02/04/2022 16:49:47 - INFO - trainer -   id_test: 2.12374    time_test: 0.82379  total_loss: 2.94753\n",
            "epoch 536  id_train: 1.80194  time_train: 0.74999  total_loss: 2.55194 lr 1.972896e-05: 100%|██████████| 16/16 [00:04<00:00,  3.28it/s]\n",
            "02/04/2022 16:49:53 - INFO - trainer -   id_test: 2.15210    time_test: 0.82429  total_loss: 2.97639\n",
            "epoch 537  id_train: 1.93281  time_train: 0.71669  total_loss: 2.64950 lr 1.959337e-05: 100%|██████████| 16/16 [00:04<00:00,  3.32it/s]\n",
            "02/04/2022 16:49:59 - INFO - trainer -   id_test: 2.11774    time_test: 0.85265  total_loss: 2.97039\n",
            "epoch 538  id_train: 1.81813  time_train: 0.71569  total_loss: 2.53381 lr 1.945810e-05: 100%|██████████| 16/16 [00:04<00:00,  3.28it/s]\n",
            "02/04/2022 16:50:05 - INFO - trainer -   id_test: 2.14980    time_test: 0.84075  total_loss: 2.99055\n",
            "epoch 539  id_train: 1.83982  time_train: 0.75873  total_loss: 2.59855 lr 1.932314e-05: 100%|██████████| 16/16 [00:04<00:00,  3.27it/s]\n",
            "02/04/2022 16:50:11 - INFO - trainer -   id_test: 2.13320    time_test: 0.80800  total_loss: 2.94120\n",
            "epoch 540  id_train: 1.84673  time_train: 0.74736  total_loss: 2.59410 lr 1.918851e-05: 100%|██████████| 16/16 [00:04<00:00,  3.26it/s]\n",
            "02/04/2022 16:50:17 - INFO - trainer -   id_test: 2.13617    time_test: 0.81096  total_loss: 2.94714\n",
            "epoch 541  id_train: 1.81058  time_train: 0.76786  total_loss: 2.57843 lr 1.905420e-05: 100%|██████████| 16/16 [00:05<00:00,  3.06it/s]\n",
            "02/04/2022 16:50:23 - INFO - trainer -   id_test: 2.13476    time_test: 0.80556  total_loss: 2.94032\n",
            "epoch 542  id_train: 1.92494  time_train: 0.73684  total_loss: 2.66178 lr 1.892021e-05: 100%|██████████| 16/16 [00:04<00:00,  3.49it/s]\n",
            "02/04/2022 16:50:29 - INFO - trainer -   id_test: 2.14980    time_test: 0.81520  total_loss: 2.96500\n",
            "epoch 543  id_train: 1.88458  time_train: 0.74247  total_loss: 2.62705 lr 1.878655e-05: 100%|██████████| 16/16 [00:08<00:00,  1.93it/s]\n",
            "02/04/2022 16:50:38 - INFO - trainer -   id_test: 2.14427    time_test: 0.80375  total_loss: 2.94802\n",
            "epoch 544  id_train: 1.83463  time_train: 0.72061  total_loss: 2.55524 lr 1.865321e-05: 100%|██████████| 16/16 [00:08<00:00,  1.81it/s]\n",
            "02/04/2022 16:50:48 - INFO - trainer -   id_test: 2.14661    time_test: 0.81200  total_loss: 2.95860\n",
            "epoch 545  id_train: 1.94322  time_train: 0.71760  total_loss: 2.66082 lr 1.852021e-05: 100%|██████████| 16/16 [00:09<00:00,  1.76it/s]\n",
            "02/04/2022 16:50:58 - INFO - trainer -   id_test: 2.16261    time_test: 0.83809  total_loss: 3.00070\n",
            "epoch 546  id_train: 1.98675  time_train: 0.72572  total_loss: 2.71248 lr 1.838755e-05: 100%|██████████| 16/16 [00:09<00:00,  1.78it/s]\n",
            "02/04/2022 16:51:09 - INFO - trainer -   id_test: 2.15516    time_test: 0.83816  total_loss: 2.99332\n",
            "epoch 547  id_train: 1.90119  time_train: 0.73809  total_loss: 2.63927 lr 1.836750e-05:  12%|█▎        | 2/16 [00:02<00:14,  1.00s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-112-6a534bbff084>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/projects/slab/git/neuroformer/neuroformer/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mraw_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dataset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/projects/slab/git/neuroformer/neuroformer/trainer.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m    174\u001b[0m                     \u001b[0;31m#  linear warmup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# number of tokens processed this step (i.e label is not -100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarmup_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m                         \u001b[0;31m# linear warmup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0mlr_mult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarmup_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from trainer import Trainer, TrainerConfig\n",
        "# model.load_state_dict(torch.load(parent_path +  \"code/transformer_vid3/runs/models/12-01-21-14:18-e:19-b:239-l:4-h:2-ne:512-higher_order.pt\"))\n",
        "# model.load_state_dict(torch.load(parent_path +  \"code/transformer_vid3/runs/models/12-14-21-23:44-e:17-b:650-l:8-h:4-ne:256-higher_order.pt\"))\n",
        "\n",
        "\n",
        "layers = (mconf.n_state_layers, mconf.n_state_history_layers, mconf.n_stimulus_layers)\n",
        "max_epochs = 800\n",
        "batch_size = 32 * 6\n",
        "shuffle = True\n",
        "tconf = TrainerConfig(max_epochs=max_epochs, batch_size=batch_size, learning_rate=8e-5, \n",
        "                      num_workers=4, lr_decay=True, patience=3, warmup_tokens=3e5, \n",
        "                      decay_weights=True, shuffle=shuffle,\n",
        "                      final_tokens=len(train_dataset)*(id_block_size) * (max_epochs),\n",
        "                      clip_norm=8.0, grad_norm_clip=2.0,\n",
        "                      dataset='higher_order', mode='predict',\n",
        "                      block_size=train_dataset.block_size,\n",
        "                      id_block_size=train_dataset.id_block_size,\n",
        "                      show_grads=False, plot_raster=False,\n",
        "                      ckpt_path=f\"/home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:{shuffle}_perceiver_2.0_{dt}_{layers}_{mconf.n_head}_{mconf.n_embd}.pt\")\n",
        "# f\"/home/antonis/projects/slab/git/neuroformer/models/model_sim_weighted_shuffle_decay:{shuffle}_perceiver_2.0_dt:{dt}_eos_{mconf.n_layer}_{mconf.n_head}_{mconf.n_embd}.pt\")\n",
        "\n",
        "trainer = Trainer(model, train_dataset, test_dataset, tconf, mconf)\n",
        "trainer.train()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader = DataLoader(train_dataset, shuffle=False, pin_memory=False,\n",
        "                                  batch_size=1, num_workers=1)                   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "iterable = iter(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x, y = next(iterable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stoi_dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(x['interval'], x['trial'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x['id_prev']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x['id'][..., :len(x['id']) - x['pad']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y['id'][..., :len(y['id']) - x['pad']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y['dt'][..., :len(x['id']) - x['pad']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dt_int = float(x['interval'])\n",
        "train_data[(train_data['Interval'].isin([dt_int - 0.5, dt_int])) & (train_data['Trial'] == x['trial'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x['interval']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x['trial'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x['id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x['id_prev']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def return_att(att_mod):\n",
        "    return att_mod.att.shape\n",
        "\n",
        "return_att(model.neural_visual_transformer.neural_state_block[0].attn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x['pad']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(torch.sum(model.neural_visual_transformer.neural_state_block[0].attn.att, axis=1) / 8).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXZQ45pGTyMx",
        "outputId": "443d95df-c2f4-404d-a43e-8b430c9ecd0b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 729/729 [02:06<00:00,  5.75it/s]\n"
          ]
        }
      ],
      "source": [
        "\"\"\" Predict using TEST dataset \"\"\"\n",
        "\n",
        "from utils import predict_raster, predict_raster_resnet, predict_raster_enc_dec, predict_raster_recursive, predict_beam_search, predict_raster_recursive_time, predict_raster_recursive_time_auto, predict_beam_search_time, predict_raster_hungarian\n",
        "from utils import set_plot_params\n",
        "set_plot_params()\n",
        "%matplotlib inline\n",
        "\n",
        "loader = DataLoader(test_dataset, shuffle=False, pin_memory=False,\n",
        "                                  batch_size=1, num_workers=1)\n",
        "\n",
        "# device = torch.cuda.current_device()\n",
        "# model = model.to(device)\n",
        "model.load_state_dict(torch.load(\"/home/antonis/projects/slab/git/neuroformer/models/neuroformer_norm_weighted:dt_:True_perceiver_2.0_0.05_(6, 6, 6)_8_256.pt\"))\n",
        "\n",
        "\"\"\" \n",
        "\n",
        "To predict only neurons we pass <frame_end> so we see predictions only for Neurons \n",
        "If you want to also see frame_tokens, just pass <frame_end=0>\n",
        "\n",
        "NOTE: 512 ID is the <end-of-sequence-id>. Right now, makes no difference if I include\n",
        "it in loss, here it is included in loss and predictions.\n",
        "\n",
        "\"\"\"\n",
        "# true, predicted, true_timing, predicted_timing = predict_time_raster(model, loader, \n",
        "#                                                                     f_block_sz=frame_block_size, id_block_sz=frame_block_size, \n",
        "#                                                                     get_dt=True)\n",
        "\n",
        "# true, predicted, true_timing, predicted_timing = predict_time_raster(model, loader, \n",
        "#                                                                     f_block_sz=frame_block_size, id_block_sz=frame_block_size,\n",
        "#                                                                     get_dt=True)\n",
        "\n",
        "# true, predicted = predict_raster(model, loader)\n",
        "\n",
        "# true, predicted = predict_beam_search(model, loader, stoi, frame_end=0)\n",
        "# true, predicted, true_timing = predict_raster_recursive(model, loader, stoi, sample=True, top_p=0.95, gpu=True, frame_end=0)\n",
        "results = predict_raster_recursive_time_auto(model, loader, window, stoi, itos_dt, sample=True, top_p=0.85, top_p_t=0.85, temp=1.0, temp_t=1, frame_end=0, get_dt=True, gpu=False)\n",
        "\n",
        "# true, predicted = predict_raster_hungarian(model, loader)\n",
        "# true, predicted = predict_raster(model, loader, gpu=True)\n",
        "\n",
        "# true_df = pd.DataFrame(true.numpy())\n",
        "# predicted_df = pd.DataFrame(predicted.numpy())\n",
        "# print(len(true_df[true_df[0] == 512]), len(predicted_df[predicted_df[0] == 512])) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
        "# results = predict_raster_recursive_time_auto(model, loader, window, stoi, itos_dt, sample=True, top_p=0.95, top_p_t=0.95, frame_end=0, get_dt=True, gpu=False)\n",
        "\n",
        "pred_keys = ['pred', 'time_pred', 'trial', 'interval']\n",
        "predicted_dict = {k: results[k] for k in results if k in pred_keys}\n",
        "df_pred = pd.DataFrame(predicted_dict)\n",
        "df_pred.rename({'pred':'ID', 'time_pred':'dt', 'trial':'Trial'}, axis=1, inplace=True)\n",
        "df_pred['Time'] = df_pred['dt'] + df_pred['interval'] - 0.5\n",
        "# df_true['time'] = df_true['dt'] + df_true['interval'] - 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [],
      "source": [
        "true_keys = ['true', 'time']\n",
        "true_dict = {k: results[k] for k in results if k in true_keys}\n",
        "df_true = pd.DataFrame(true_dict)\n",
        "df_true.rename({'true':'ID', 'time':'dt'}, axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "kejTrZ1_2u8T"
      },
      "outputs": [],
      "source": [
        "# # predicted_timing = [itos_dt[int(dt)] for dt in predicted_timing]\n",
        "# # df_pred = pd.DataFrame({'True':true, 'Predicted':predicted, 'Time':true_timing, 'Predicted_Time':predicted_timing})    # Time':test_data['Time']})\n",
        "# df_pred.to_csv(f\"/content/drive/MyDrive/slab/predictions/OneCombo3/model_weighted_shuffle:{shuffle}_perceiver_2.0_dt:{dt}_eos_{mconf.n_layer}_{mconf.n_head}_{mconf.n_embd}.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_sim = pd.read_csv(\"/home/antonis/projects/slab/git/neuroformer/data/full_sim__model_weighted_shuffle_decay_True_perceiver_2.0_dt_0.05_eos_8_8_256.csv\").iloc[:,1:].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5893 5972\n"
          ]
        }
      ],
      "source": [
        "print(len(df_true), len(df_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>dt</th>\n",
              "      <th>Trial</th>\n",
              "      <th>interval</th>\n",
              "      <th>Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>24</td>\n",
              "      <td>0.05</td>\n",
              "      <td>2</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>26</td>\n",
              "      <td>0.15</td>\n",
              "      <td>2</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84</td>\n",
              "      <td>0.50</td>\n",
              "      <td>2</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>166</td>\n",
              "      <td>0.50</td>\n",
              "      <td>2</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>13</td>\n",
              "      <td>0.05</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>162</td>\n",
              "      <td>0.05</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>12</td>\n",
              "      <td>0.20</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>154</td>\n",
              "      <td>0.25</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>49</td>\n",
              "      <td>0.25</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>12</td>\n",
              "      <td>0.50</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>166</td>\n",
              "      <td>0.50</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>149</td>\n",
              "      <td>0.05</td>\n",
              "      <td>2</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>95</td>\n",
              "      <td>0.05</td>\n",
              "      <td>2</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>125</td>\n",
              "      <td>0.50</td>\n",
              "      <td>2</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>109</td>\n",
              "      <td>0.50</td>\n",
              "      <td>2</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>166</td>\n",
              "      <td>0.50</td>\n",
              "      <td>2</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>157</td>\n",
              "      <td>0.05</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>114</td>\n",
              "      <td>0.35</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>30</td>\n",
              "      <td>0.50</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>26</td>\n",
              "      <td>0.50</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>0.50</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>80</td>\n",
              "      <td>0.50</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>94</td>\n",
              "      <td>0.50</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>166</td>\n",
              "      <td>0.50</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>104</td>\n",
              "      <td>0.05</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>82</td>\n",
              "      <td>0.25</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>115</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>13</td>\n",
              "      <td>0.35</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>104</td>\n",
              "      <td>0.40</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>40</td>\n",
              "      <td>0.45</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>109</td>\n",
              "      <td>0.50</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>166</td>\n",
              "      <td>0.50</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>26</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>49</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>166</td>\n",
              "      <td>0.05</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>119</td>\n",
              "      <td>0.05</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>111</td>\n",
              "      <td>0.05</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>135</td>\n",
              "      <td>0.05</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>16</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>13</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>162</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>128</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>104</td>\n",
              "      <td>0.05</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>49</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>48</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>149</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>84</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>128</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     ID    dt  Trial  interval  Time\n",
              "0     0  0.00      0       0.0 -0.50\n",
              "1    24  0.05      2       0.5  0.05\n",
              "2    26  0.15      2       0.5  0.15\n",
              "3    84  0.50      2       0.5  0.50\n",
              "4   166  0.50      2       0.5  0.50\n",
              "5    13  0.05      2       1.0  0.55\n",
              "6   162  0.05      2       1.0  0.55\n",
              "7    12  0.20      2       1.0  0.70\n",
              "8   154  0.25      2       1.0  0.75\n",
              "9    49  0.25      2       1.0  0.75\n",
              "10   12  0.50      2       1.0  1.00\n",
              "11  166  0.50      2       1.0  1.00\n",
              "12  149  0.05      2       1.5  1.05\n",
              "13   95  0.05      2       1.5  1.05\n",
              "14  125  0.50      2       1.5  1.50\n",
              "15  109  0.50      2       1.5  1.50\n",
              "16  166  0.50      2       1.5  1.50\n",
              "17  157  0.05      2       2.0  1.55\n",
              "18   75  0.25      2       2.0  1.75\n",
              "19  114  0.35      2       2.0  1.85\n",
              "20   30  0.50      2       2.0  2.00\n",
              "21   26  0.50      2       2.0  2.00\n",
              "22   22  0.50      2       2.0  2.00\n",
              "23   80  0.50      2       2.0  2.00\n",
              "24   94  0.50      2       2.0  2.00\n",
              "25  166  0.50      2       2.0  2.00\n",
              "26  104  0.05      2       3.0  2.55\n",
              "27   82  0.25      2       3.0  2.75\n",
              "28  115  0.30      2       3.0  2.80\n",
              "29   13  0.35      2       3.0  2.85\n",
              "30  104  0.40      2       3.0  2.90\n",
              "31   40  0.45      2       3.0  2.95\n",
              "32  109  0.50      2       3.0  3.00\n",
              "33  166  0.50      2       3.0  3.00\n",
              "34   26  0.00      2       3.5  3.00\n",
              "35   49  0.00      2       3.5  3.00\n",
              "36  166  0.05      2       3.5  3.05\n",
              "37  119  0.05      2       3.5  3.05\n",
              "38  111  0.05      2       3.5  3.05\n",
              "39  135  0.05      2       3.5  3.05\n",
              "40   16  0.10      2       3.5  3.10\n",
              "41   13  0.10      2       3.5  3.10\n",
              "42  162  0.10      2       3.5  3.10\n",
              "43  128  0.10      2       3.5  3.10\n",
              "44  104  0.05      2       3.5  3.05\n",
              "45   49  0.10      2       3.5  3.10\n",
              "46   48  0.10      2       3.5  3.10\n",
              "47  149  0.10      2       3.5  3.10\n",
              "48   84  0.10      2       3.5  3.10\n",
              "49  128  0.10      2       3.5  3.10"
            ]
          },
          "execution_count": 152,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_pred[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>dt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>150</td>\n",
              "      <td>0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>47</td>\n",
              "      <td>0.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>57</td>\n",
              "      <td>0.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>23</td>\n",
              "      <td>0.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5888</th>\n",
              "      <td>93</td>\n",
              "      <td>0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5889</th>\n",
              "      <td>4</td>\n",
              "      <td>0.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5890</th>\n",
              "      <td>93</td>\n",
              "      <td>0.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5891</th>\n",
              "      <td>85</td>\n",
              "      <td>0.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5892</th>\n",
              "      <td>166</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5893 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       ID    dt\n",
              "0       0  0.00\n",
              "1     150  0.05\n",
              "2      47  0.15\n",
              "3      57  0.15\n",
              "4      23  0.20\n",
              "...   ...   ...\n",
              "5888   93  0.10\n",
              "5889    4  0.15\n",
              "5890   93  0.15\n",
              "5891   85  0.30\n",
              "5892  166  0.50\n",
              "\n",
              "[5893 rows x 2 columns]"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>ID</th>\n",
              "      <th>Trial</th>\n",
              "      <th>Interval</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.031</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.055</td>\n",
              "      <td>156</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.143</td>\n",
              "      <td>105</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.161</td>\n",
              "      <td>109</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.198</td>\n",
              "      <td>72</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.204</td>\n",
              "      <td>162</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.213</td>\n",
              "      <td>71</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.222</td>\n",
              "      <td>66</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.240</td>\n",
              "      <td>71</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.288</td>\n",
              "      <td>44</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.303</td>\n",
              "      <td>156</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.318</td>\n",
              "      <td>93</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.364</td>\n",
              "      <td>57</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.384</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.386</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.400</td>\n",
              "      <td>115</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.451</td>\n",
              "      <td>149</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.453</td>\n",
              "      <td>101</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.525</td>\n",
              "      <td>154</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.563</td>\n",
              "      <td>157</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.579</td>\n",
              "      <td>26</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.606</td>\n",
              "      <td>74</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.613</td>\n",
              "      <td>156</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.648</td>\n",
              "      <td>109</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.666</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.671</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.691</td>\n",
              "      <td>120</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.734</td>\n",
              "      <td>149</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.775</td>\n",
              "      <td>72</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.859</td>\n",
              "      <td>30</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Time   ID  Trial  Interval\n",
              "0   0.031   16      1       0.5\n",
              "1   0.055  156      1       0.5\n",
              "2   0.143  105      1       0.5\n",
              "3   0.161  109      1       0.5\n",
              "4   0.198   72      1       0.5\n",
              "5   0.204  162      1       0.5\n",
              "6   0.213   71      1       0.5\n",
              "7   0.222   66      1       0.5\n",
              "8   0.240   71      1       0.5\n",
              "9   0.288   44      1       0.5\n",
              "10  0.303  156      1       0.5\n",
              "11  0.318   93      1       0.5\n",
              "12  0.364   57      1       0.5\n",
              "13  0.384   12      1       0.5\n",
              "14  0.386   13      1       0.5\n",
              "15  0.400  115      1       0.5\n",
              "16  0.451  149      1       0.5\n",
              "17  0.453  101      1       0.5\n",
              "18  0.525  154      1       1.0\n",
              "19  0.563  157      1       1.0\n",
              "20  0.579   26      1       1.0\n",
              "21  0.606   74      1       1.0\n",
              "22  0.613  156      1       1.0\n",
              "23  0.648  109      1       1.0\n",
              "24  0.666    8      1       1.0\n",
              "25  0.671   23      1       1.0\n",
              "26  0.691  120      1       1.0\n",
              "27  0.734  149      1       1.0\n",
              "28  0.775   72      1       1.0\n",
              "29  0.859   30      1       1.0"
            ]
          },
          "execution_count": 154,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[:30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>dt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>166</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>166</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>150</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>166</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>166</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5875</th>\n",
              "      <td>166</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5883</th>\n",
              "      <td>93</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5884</th>\n",
              "      <td>166</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5886</th>\n",
              "      <td>166</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5892</th>\n",
              "      <td>166</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>997 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       ID   dt\n",
              "11    166  0.5\n",
              "26    166  0.5\n",
              "31    150  0.5\n",
              "32    166  0.5\n",
              "38    166  0.5\n",
              "...   ...  ...\n",
              "5875  166  0.5\n",
              "5883   93  0.5\n",
              "5884  166  0.5\n",
              "5886  166  0.5\n",
              "5892  166  0.5\n",
              "\n",
              "[997 rows x 2 columns]"
            ]
          },
          "execution_count": 155,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_true[df_true['dt'] == 0.5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:xlabel='dt'>"
            ]
          },
          "execution_count": 156,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "dark"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "df_pred[df_pred['ID'] == 166][:30].groupby('dt').size().plot.bar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>ID</th>\n",
              "      <th>Trial</th>\n",
              "      <th>Interval</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.074</td>\n",
              "      <td>150</td>\n",
              "      <td>2</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.138</td>\n",
              "      <td>47</td>\n",
              "      <td>2</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.153</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.212</td>\n",
              "      <td>23</td>\n",
              "      <td>2</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.231</td>\n",
              "      <td>149</td>\n",
              "      <td>2</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.324</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.345</td>\n",
              "      <td>157</td>\n",
              "      <td>2</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.389</td>\n",
              "      <td>156</td>\n",
              "      <td>2</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.393</td>\n",
              "      <td>112</td>\n",
              "      <td>2</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.429</td>\n",
              "      <td>26</td>\n",
              "      <td>2</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.503</td>\n",
              "      <td>106</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.527</td>\n",
              "      <td>71</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.545</td>\n",
              "      <td>37</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.559</td>\n",
              "      <td>124</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.588</td>\n",
              "      <td>40</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.623</td>\n",
              "      <td>75</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.658</td>\n",
              "      <td>53</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.663</td>\n",
              "      <td>83</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.679</td>\n",
              "      <td>101</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.696</td>\n",
              "      <td>95</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.781</td>\n",
              "      <td>123</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.867</td>\n",
              "      <td>71</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.868</td>\n",
              "      <td>105</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.909</td>\n",
              "      <td>71</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1.069</td>\n",
              "      <td>40</td>\n",
              "      <td>2</td>\n",
              "      <td>1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1.098</td>\n",
              "      <td>150</td>\n",
              "      <td>2</td>\n",
              "      <td>1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1.184</td>\n",
              "      <td>101</td>\n",
              "      <td>2</td>\n",
              "      <td>1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1.267</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1.481</td>\n",
              "      <td>150</td>\n",
              "      <td>2</td>\n",
              "      <td>1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1.591</td>\n",
              "      <td>109</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1.659</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>1.785</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>1.801</td>\n",
              "      <td>71</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>1.829</td>\n",
              "      <td>150</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>2.899</td>\n",
              "      <td>16</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>2.981</td>\n",
              "      <td>75</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>3.018</td>\n",
              "      <td>73</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>3.060</td>\n",
              "      <td>34</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>3.064</td>\n",
              "      <td>23</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>3.081</td>\n",
              "      <td>101</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Time   ID  Trial  Interval\n",
              "0   0.074  150      2       0.5\n",
              "1   0.138   47      2       0.5\n",
              "2   0.153   57      2       0.5\n",
              "3   0.212   23      2       0.5\n",
              "4   0.231  149      2       0.5\n",
              "5   0.324   13      2       0.5\n",
              "6   0.345  157      2       0.5\n",
              "7   0.389  156      2       0.5\n",
              "8   0.393  112      2       0.5\n",
              "9   0.429   26      2       0.5\n",
              "10  0.503  106      2       1.0\n",
              "11  0.527   71      2       1.0\n",
              "12  0.545   37      2       1.0\n",
              "13  0.559  124      2       1.0\n",
              "14  0.588   40      2       1.0\n",
              "15  0.623   75      2       1.0\n",
              "16  0.658   53      2       1.0\n",
              "17  0.663   83      2       1.0\n",
              "18  0.679  101      2       1.0\n",
              "19  0.696   95      2       1.0\n",
              "20  0.781  123      2       1.0\n",
              "21  0.867   71      2       1.0\n",
              "22  0.868  105      2       1.0\n",
              "23  0.909   71      2       1.0\n",
              "24  1.069   40      2       1.5\n",
              "25  1.098  150      2       1.5\n",
              "26  1.184  101      2       1.5\n",
              "27  1.267   13      2       1.5\n",
              "28  1.481  150      2       1.5\n",
              "29  1.591  109      2       2.0\n",
              "30  1.659    9      2       2.0\n",
              "31  1.785   13      2       2.0\n",
              "32  1.801   71      2       2.0\n",
              "33  1.829  150      2       2.0\n",
              "34  2.899   16      2       3.0\n",
              "35  2.981   75      2       3.0\n",
              "36  3.018   73      2       3.5\n",
              "37  3.060   34      2       3.5\n",
              "38  3.064   23      2       3.5\n",
              "39  3.081  101      2       3.5"
            ]
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data[:40]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [],
      "source": [
        "freq_dt_true = df_true.groupby(['dt']).size()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>dt</th>\n",
              "      <th>Trial</th>\n",
              "      <th>interval</th>\n",
              "      <th>Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>104</td>\n",
              "      <td>0.40</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>40</td>\n",
              "      <td>0.45</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>109</td>\n",
              "      <td>0.50</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>166</td>\n",
              "      <td>0.50</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>26</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>49</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>166</td>\n",
              "      <td>0.05</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>119</td>\n",
              "      <td>0.05</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>111</td>\n",
              "      <td>0.05</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>135</td>\n",
              "      <td>0.05</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>16</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>13</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>162</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>128</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>104</td>\n",
              "      <td>0.05</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>49</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>48</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>149</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>84</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>128</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>104</td>\n",
              "      <td>0.15</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>111</td>\n",
              "      <td>0.15</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>96</td>\n",
              "      <td>0.15</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>23</td>\n",
              "      <td>0.15</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>117</td>\n",
              "      <td>0.20</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>93</td>\n",
              "      <td>0.25</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>35</td>\n",
              "      <td>0.20</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>149</td>\n",
              "      <td>0.25</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>149</td>\n",
              "      <td>0.20</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>157</td>\n",
              "      <td>0.20</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     ID    dt  Trial  interval  Time\n",
              "30  104  0.40      2       3.0  2.90\n",
              "31   40  0.45      2       3.0  2.95\n",
              "32  109  0.50      2       3.0  3.00\n",
              "33  166  0.50      2       3.0  3.00\n",
              "34   26  0.00      2       3.5  3.00\n",
              "35   49  0.00      2       3.5  3.00\n",
              "36  166  0.05      2       3.5  3.05\n",
              "37  119  0.05      2       3.5  3.05\n",
              "38  111  0.05      2       3.5  3.05\n",
              "39  135  0.05      2       3.5  3.05\n",
              "40   16  0.10      2       3.5  3.10\n",
              "41   13  0.10      2       3.5  3.10\n",
              "42  162  0.10      2       3.5  3.10\n",
              "43  128  0.10      2       3.5  3.10\n",
              "44  104  0.05      2       3.5  3.05\n",
              "45   49  0.10      2       3.5  3.10\n",
              "46   48  0.10      2       3.5  3.10\n",
              "47  149  0.10      2       3.5  3.10\n",
              "48   84  0.10      2       3.5  3.10\n",
              "49  128  0.10      2       3.5  3.10\n",
              "50  104  0.15      2       3.5  3.15\n",
              "51  111  0.15      2       3.5  3.15\n",
              "52   96  0.15      2       3.5  3.15\n",
              "53   23  0.15      2       3.5  3.15\n",
              "54  117  0.20      2       3.5  3.20\n",
              "55   93  0.25      2       3.5  3.25\n",
              "56   35  0.20      2       3.5  3.20\n",
              "57  149  0.25      2       3.5  3.25\n",
              "58  149  0.20      2       3.5  3.20\n",
              "59  157  0.20      2       3.5  3.20"
            ]
          },
          "execution_count": 159,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_pred[30:2 * 30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>dt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>150</td>\n",
              "      <td>0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>47</td>\n",
              "      <td>0.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>57</td>\n",
              "      <td>0.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>23</td>\n",
              "      <td>0.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>149</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>13</td>\n",
              "      <td>0.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>157</td>\n",
              "      <td>0.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>156</td>\n",
              "      <td>0.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>112</td>\n",
              "      <td>0.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>26</td>\n",
              "      <td>0.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>166</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>106</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>71</td>\n",
              "      <td>0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>37</td>\n",
              "      <td>0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>124</td>\n",
              "      <td>0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>40</td>\n",
              "      <td>0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>75</td>\n",
              "      <td>0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>53</td>\n",
              "      <td>0.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>83</td>\n",
              "      <td>0.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>101</td>\n",
              "      <td>0.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>95</td>\n",
              "      <td>0.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>123</td>\n",
              "      <td>0.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>71</td>\n",
              "      <td>0.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>105</td>\n",
              "      <td>0.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>71</td>\n",
              "      <td>0.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>166</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>40</td>\n",
              "      <td>0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>150</td>\n",
              "      <td>0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>101</td>\n",
              "      <td>0.20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     ID    dt\n",
              "0     0  0.00\n",
              "1   150  0.05\n",
              "2    47  0.15\n",
              "3    57  0.15\n",
              "4    23  0.20\n",
              "5   149  0.25\n",
              "6    13  0.30\n",
              "7   157  0.35\n",
              "8   156  0.40\n",
              "9   112  0.40\n",
              "10   26  0.45\n",
              "11  166  0.50\n",
              "12  106  0.00\n",
              "13   71  0.05\n",
              "14   37  0.05\n",
              "15  124  0.05\n",
              "16   40  0.10\n",
              "17   75  0.10\n",
              "18   53  0.15\n",
              "19   83  0.15\n",
              "20  101  0.20\n",
              "21   95  0.20\n",
              "22  123  0.30\n",
              "23   71  0.35\n",
              "24  105  0.35\n",
              "25   71  0.40\n",
              "26  166  0.50\n",
              "27   40  0.05\n",
              "28  150  0.10\n",
              "29  101  0.20"
            ]
          },
          "execution_count": 160,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_true[:30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 864x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "dark"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# df_dt = [y['dt'][:len(y['dt']) - (x['pad'])] for x, y in test_dataset]\n",
        "plt.figure(figsize=(12,10))\n",
        "\n",
        "# df_dt = df_true['time']\n",
        "freq_dt_true = df_true.groupby(['dt']).size()\n",
        "# freq_dt_true = df_sim.groupby(['dt']).size()\n",
        "plt.bar(np.arange(len(freq_dt_true.index)), freq_dt_true, alpha=0.5, label='True')\n",
        "\n",
        "\n",
        "# df_dt_pred = df_pred['time_pred']\n",
        "freq_dt_pred = df_pred.groupby(['dt']).size()\n",
        "plt.bar(np.arange(len(freq_dt_pred.index)), freq_dt_pred, alpha=0.5, label='Predicted')\n",
        "\n",
        "\n",
        "# plt.xticks(ticks=pd.to_numeric(freq_dt_pred.index)labels=pd.to_numeric(freq_dt_pred.index))\n",
        "plt.title('dt Interval Groups')\n",
        "plt.xlabel('dt Group (n x 0.05s)')\n",
        "plt.ylabel('Count (N)')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "# plt.savefig(\"dt_interval_dist.png\", dpi=300)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5972 5893\n"
          ]
        }
      ],
      "source": [
        "print(len(df_pred), len(df_true))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2160x1440 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "dark"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "def plot_distribution(df_1, df_2):\n",
        "    plt.figure(figsize=(30,20))\n",
        "    n_min = 166\n",
        "    freq_true = df_1[df_1['ID'] < n_min].groupby(['ID']).size()\n",
        "    freq_pred = df_2[df_2['ID'] < n_min].groupby(['ID']).size()\n",
        "    plt.bar(freq_pred.index, freq_pred, label='predicted', alpha=0.5)\n",
        "    plt.bar(freq_true.index, freq_true, label='true', alpha=0.5)\n",
        "    plt.title('Neuron Firing Distribution', fontsize=40)\n",
        "    plt.legend(fontsize=30)\n",
        "    plt.xlabel('Neuron ID (n)', fontsize=30)\n",
        "    plt.ylabel('Count (N)', fontsize=30)\n",
        "    plt.xticks(fontsize=30)\n",
        "    plt.yticks(fontsize=30)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_distribution(df_true, df_pred)\n",
        "# plt.savefig(\"id_interval_dist.png\", dpi=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len_pred = len(df_true)\n",
        "# len_pred = 1000\n",
        "plt.figure(figsize=(40,40))\n",
        "plt.title('Pixel / Spike Raster', size=50)\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Neuron ID')\n",
        "plt.scatter(test_data['Time'], test_data['ID'], alpha=0.6, label='true', marker='o')\n",
        "plt.scatter(df_pred['Time'], df_pred['ID'], alpha=0.6, label='predicted', marker='x')\n",
        "plt.legend(fontsize=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "transformer_vid3_dt.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
