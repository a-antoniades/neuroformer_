{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !CUDA_VISIBLE_DEVICES=4,5,6,7\n",
    "!CUDA_VISIBLE_DEVICES=7,5,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_702416/1676056690.py:41: DeprecationWarning: Please use `gaussian_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
      "  from scipy.ndimage.filters import gaussian_filter, uniform_filter\n",
      "/tmp/ipykernel_702416/1676056690.py:41: DeprecationWarning: Please use `uniform_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
      "  from scipy.ndimage.filters import gaussian_filter, uniform_filter\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import collections\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "import glob\n",
    "from pathlib import Path, PurePath\n",
    "path = Path.cwd()\n",
    "parent_path = path.parents[1]\n",
    "sys.path.append('neuroformer')\n",
    "sys.path.append('.')\n",
    "sys.path.append('../')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from attentionVis import AttentionVis\n",
    "from trainer import Trainer, TrainerConfig\n",
    "from utils import set_seed\n",
    "\n",
    "\n",
    "from scipy import io as scipyio\n",
    "from scipy.special import softmax\n",
    "import skimage\n",
    "import skvideo.io\n",
    "from utils import print_full\n",
    "from scipy.ndimage.filters import gaussian_filter, uniform_filter\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "from visualize import *\n",
    "set_plot_params()\n",
    "%matplotlib inline\n",
    "parent_path = os.path.dirname(os.path.dirname(os.getcwd())) + \"/\"\n",
    "\n",
    "\n",
    "# for i in {1..10}; do python3 -m gather_atts.py; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "\n",
    "-- DATA --\n",
    "neuroformer/data/OneCombo3_V1AL/\n",
    "df = response\n",
    "video_stack = stimulus\n",
    "DOWNLOAD DATA URL = https://drive.google.com/drive/folders/1jNvA4f-epdpRmeG9s2E-2Sfo-pwYbjeY?usp=sharing\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "RESPONSE_PATH = \"data/SimNeu3D/NaturalMovie/response/NaturalStim_all.csv\"\n",
    "STIMULUS_PATH = \"data/SimNeu3D/NaturalMovie/stimulus/docuMovie\"\n",
    "\n",
    "# download data \n",
    "\n",
    "if not os.path.exists('/data'):\n",
    "    import gdown\n",
    "    id = 'https://drive.google.com/drive/folders/1Axmr0jNG-IxlQ3g-UOKApO-9LhCZgXL4?usp=sharing'\n",
    "    gdown.download_folder(id=id, quiet=True, use_cookies=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R3D: (3 x T x H x W)\n",
    "\n",
    "from SpikeVidUtils import image_dataset\n",
    "\n",
    "# def nearest(n, x):\n",
    "#   u = n % x > x // 2\n",
    "#   return n + (-1)**(1 - u) * abs(x * u - n % x)\n",
    "\n",
    "# vid_paths = sorted(glob.glob(STIMULUS_PATH + '/*.tif'))\n",
    "# vid_list = [skimage.io.imread(vid)[::3] for vid in vid_paths]\n",
    "# video_stack = [torch.nan_to_num(image_dataset(vid)).transpose(1, 0) for vid in vid_list]\n",
    "# torch.save({k:v for k, v in enumerate(video_stack)}, '/data5/antonis/projects/neuroformer/data/SimNeu3D/NaturalMovie/stimulus/docuMovie.pt')\n",
    "\n",
    "vs = torch.load(\"data/SimNeu3D/NaturalMovie/stimulus/docuMovie.pt\")\n",
    "video_stack = [vs[i] for i in range(len(vs))]\n",
    "\n",
    "# plt.imshow(video_stack[0][0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SR7Vs-Pn_3oo"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(RESPONSE_PATH).iloc[:, 1:]\n",
    "frame_window = 1\n",
    "window = 0.1\n",
    "window_prev = 0.1\n",
    "dt = 0.05\n",
    "\n",
    "from SpikeVidUtils import make_intervals\n",
    "\n",
    "df['Interval'] = make_intervals(df, window)\n",
    "# df['Interval_2'] = make_intervals(df, window_prev)\n",
    "# df['Interval_dt'] = make_intervals(df, dt)\n",
    "# df['Interval_dt'] = (df['Interval_dt'] - df['Interval'] + window).round(3)\n",
    "df = df.reset_index(drop=True)\n",
    "df.to_csv(f\"data/SimNeu3D/NaturalMovie/response/NaturalStim_all_{window}.csv\", index=False)\n",
    "\n",
    "# from SpikeVidUtils import neuron_dict\n",
    "# data_dict = neuron_dict(df)\n",
    "\n",
    "# save_path = \"data/SimNeu3D/NaturalMovie/response\"\n",
    "# with open(os.path.join(save_path, 'neuron_dict.pkl'), 'wb') as handle:\n",
    "#     pickle.dump(data_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "data_dict = pickle.load(open(\"data/SimNeu3D/NaturalMovie/response/neuron_dict.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwyX2t-z_44J"
   },
   "outputs": [],
   "source": [
    "# n_dt = sorted((df['Interval_dt'].unique()).round(2)) \n",
    "max_window = max(window, window_prev)\n",
    "dt_range = math.ceil(max_window / dt) + 1  # add first / last interval for SOS / EOS'\n",
    "n_dt = [round(dt * n, 2) for n in range(dt_range)]\n",
    "df['Time'] = df['Time'].round(4)\n",
    "n_unique = len(df.groupby(['Interval', 'Trial']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fgNYdciA_5xo",
    "outputId": "9be97414-3f42-49e8-ebbf-b887cf2d221b"
   },
   "outputs": [],
   "source": [
    "# df.groupby(['Interval', 'Trial']).size().plot.bar()\n",
    "# df.groupby(['Interval', 'Trial']).agg(['nunique'])model_path\n",
    "# n_unique = len(df.groupby(['Interval', 'Trial']).size())\n",
    "# df.groupby(['Interval', 'Trial']).size().nlargest(int(0.2 * n_unique))\n",
    "# df.groupby(['Interval_2', 'Trial']).size().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vd3P4ASE_7gy",
    "outputId": "3f834a76-ced6-4023-bd1b-8a420f7d0e2d"
   },
   "outputs": [],
   "source": [
    "from SpikeVidUtils import SpikeTimeVidData2\n",
    "\n",
    "## qv-vae feats\n",
    "# frames = torch.load(parent_path + \"code/data/SImNew3D/stimulus/vq-vae_code_feats-24-05-4x4x4.pt\").numpy() + 2\n",
    "# frame_feats = torch.load(parent_path + \"code/data/SImNew3D/stimulus/vq-vae_embed_feats-24-05-4x4x4.pt\").numpy()\n",
    "# frame_block_size = frames.shape[-1] - 1\n",
    "\n",
    "## resnet3d feats\n",
    "kernel_size = (5, 8, 8)\n",
    "n_embd = 256\n",
    "n_embd_frames = 64\n",
    "frame_feats = video_stack\n",
    "\n",
    "frame_block_size = ((20 // kernel_size[0] * 64 * 112) // (n_embd_frames))\n",
    "# frame_block_size = 560\n",
    "prev_id_block_size = 30\n",
    "id_block_size = 30   # 95\n",
    "block_size = frame_block_size + id_block_size + prev_id_block_size # frame_block_size * 2  # small window for faster training\n",
    "frame_memory = 20   # how many frames back does model see\n",
    "window = window\n",
    "\n",
    "neurons = sorted(list(set(df['ID'])))\n",
    "id_stoi = { ch:i for i,ch in enumerate(neurons) }\n",
    "id_itos = { i:ch for i,ch in enumerate(neurons) }\n",
    "\n",
    "# translate neural embeddings to separate them from ID embeddings\n",
    "# frames = frames + [*id_stoi.keys()][-1] \n",
    "neurons = [i for i in range(df['ID'].min(), df['ID'].max() + 1)]\n",
    "# pixels = sorted(np.unique(frames).tolist())\n",
    "feat_encodings = neurons + ['SOS'] + ['EOS'] + ['PAD']  # + pixels \n",
    "stoi = { ch:i for i,ch in enumerate(feat_encodings) }\n",
    "itos = { i:ch for i,ch in enumerate(feat_encodings) }\n",
    "stoi_dt = { ch:i for i,ch in enumerate(n_dt) }\n",
    "itos_dt = { i:ch for i,ch in enumerate(n_dt) }\n",
    "max(list(itos_dt.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O1ZJuUWTDpwk"
   },
   "outputs": [],
   "source": [
    "n = []\n",
    "for n_stim in range(df['Trial'].max() // 200):\n",
    "    n_trial = [i for i in range(200 // 20)]\n",
    "    for n_trial in n_trial:\n",
    "        trial = (n_stim + 1) * 20 - n_trial\n",
    "        n.append(trial)\n",
    "train_data = df[~df['Trial'].isin(n)].reset_index(drop=True)\n",
    "test_data = df[df['Trial'].isin(n)].reset_index(drop=True)\n",
    "small_data = df[df['Trial'].isin([5])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "luwjZJiBDuJV",
    "outputId": "46a1af29-90f1-4d64-c785-f35316d319cd"
   },
   "outputs": [],
   "source": [
    "from SpikeVidUtils import SpikeTimeVidData2\n",
    "\n",
    "# train_dat1aset = spikeTimeData(spikes, block_size, dt, stoi, itos)\n",
    "\n",
    "train_dataset = SpikeTimeVidData2(train_data, None, block_size, id_block_size, frame_block_size, prev_id_block_size, window, dt, frame_memory, stoi, itos, neurons, stoi_dt, itos_dt, frame_feats, pred=False, window_prev=window_prev, frame_window=frame_window, data_dict=data_dict)\n",
    "test_dataset = SpikeTimeVidData2(test_data, None, block_size, id_block_size, frame_block_size, prev_id_block_size, window, dt, frame_memory, stoi, itos, neurons, stoi_dt, itos_dt, frame_feats, pred=False, window_prev=window_prev, frame_window=frame_window, data_dict=data_dict)\n",
    "# dataset = SpikeTimeVidData(df, frames, frame_feats, block_size, frame_block_size, prev_id_block_size, window, frame_memory, stoi, itos)\n",
    "# single_batch = SpikeTimeVidData(df[df['Trial'].isin([5])], None, block_size, frame_block_size, prev_id_block_size, window, frame_memory, stoi, itos, neurons, stoi_dt, itos_dt, frame_feats)\n",
    "small_dataset = SpikeTimeVidData2(small_data, None, block_size, id_block_size, frame_block_size, prev_id_block_size, window, dt, frame_memory, stoi, itos, neurons, stoi_dt, itos_dt, frame_feats, pred=False, window_prev=window_prev, frame_window=frame_window, data_dict=data_dict)\n",
    "\n",
    "\n",
    "print(f'train: {len(train_dataset)}, test: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Load Model\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from model_neuroformer import GPT\n",
    "\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "n_seed = 25\n",
    "models = []\n",
    "# for n in [0, 25, 50]:\n",
    "# set_seed(25)\n",
    "\n",
    "from model_neuroformer import GPT, GPTConfig, neuralGPTConfig, Decoder\n",
    "# initialize config class and model (holds hyperparameters)\n",
    "# for is_conv in [True, False]:    \n",
    "conv_layer = True\n",
    "mconf = GPTConfig(train_dataset.population_size, block_size,    # frame_block_size\n",
    "                        id_vocab_size=train_dataset.id_population_size,\n",
    "                        frame_block_size=frame_block_size,\n",
    "                        id_block_size=id_block_size,  # frame_block_size\n",
    "                        prev_id_block_size=prev_id_block_size,\n",
    "                        sparse_mask=False, p_sparse=0.25, sparse_topk_frame=None, sparse_topk_id=None,\n",
    "                        n_dt=len(n_dt),\n",
    "                        data_size=train_dataset.size,\n",
    "                        class_weights=None,\n",
    "                        pretrain=False,\n",
    "                        n_state_layers=4, n_state_history_layers=0, n_stimulus_layers=8, self_att_layers=4,\n",
    "                        n_layer=10, n_head=2, n_embd=n_embd, n_embd_frames=n_embd_frames, \n",
    "                        contrastive=True, clip_emb=1024, clip_temp=0.5,\n",
    "                        conv_layer=conv_layer, kernel_size=kernel_size,\n",
    "                        temp_emb=True, pos_emb=False,\n",
    "                        id_drop=0.35, im_drop=0.35,\n",
    "                        window=window, window_prev=window_prev, frame_window=frame_window, dt=dt,\n",
    "                        neurons=neurons, stoi_dt=stoi_dt, itos_dt=itos_dt)  # 0.35\n",
    "model = GPT(mconf)\n",
    "# model.load_state_dict(torch.load(\"/home/antonis/projects/slab/git/neuroformer/models/tensorboard/V1_AL_cont/cont0+conv_emask_Cont:True_0.50.05_sparseTrue_conv_True_shuffle:True_batch:224_sparse_(None_None)_pos_emb:False_temp_emb:True_drop:0.2_dt:True_2.0_0.5_max0.05_(4, 4, 6)_2_256_nembframe64.pt\", map_location='cpu'))\n",
    "\n",
    "from trainer import Trainer, TrainerConfig\n",
    "# model.load_state_dict(torch.load(parent_path +  \"code/transformer_vid3/runs/models/12-01-21-14:18-e:19-b:239-l:4-h:2-ne:512-higher_order.pt\"))\n",
    "\n",
    "\n",
    "layers = (mconf.n_state_layers, mconf.n_state_history_layers, mconf.n_stimulus_layers)\n",
    "max_epochs = 250\n",
    "batch_size = 50\n",
    "shuffle = True\n",
    "model_path = f\"models/tensorboard/natmovie/w:{window}_wp:{window_prev}/{6}_Cont:{mconf.contrastive}_window:{window}_f_window:{frame_window}_df:{dt}_blocksize:{id_block_size}_sparse{mconf.sparse_mask}_conv_{conv_layer}_shuffle:{shuffle}_batch:{batch_size}_sparse_({mconf.sparse_topk_frame}_{mconf.sparse_topk_id})_blocksz{block_size}_pos_emb:{mconf.pos_emb}_temp_emb:{mconf.temp_emb}_drop:{mconf.id_drop}_dt:{shuffle}_2.0_{max(n_dt)}_max{dt}_{layers}_{mconf.n_head}_{mconf.n_embd}_nembframe{mconf.n_embd_frames}_{kernel_size}.pt\"\n",
    "# model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "# model.load_state_dict(torch.load(\"/Users/antonis/Downloads/[16, 17, 18, 19]_Cont_True_0.50.05_sparseFalse_conv_True_shuffle_True_batch_224_sparse_(None_None)_pos_emb_False_temp_emb_True_drop_0.2_dt_True_2.0_0.5_max0.05_(4, 0, 6)_2_256_nembframe64-2.pt\", map_location='cpu'))\n",
    "\n",
    "tconf = TrainerConfig(max_epochs=max_epochs, batch_size=batch_size, learning_rate=3e-4, \n",
    "                    num_workers=4, lr_decay=True, patience=3, warmup_tokens=8e6, \n",
    "                    decay_weights=True, weight_decay=0.1, shuffle=shuffle,\n",
    "                    final_tokens=len(train_dataset)*(id_block_size) * (max_epochs),\n",
    "                    clip_norm=1.0, grad_norm_clip=1.0,\n",
    "                    dataset='higher_order', mode='predict',\n",
    "                    block_size=train_dataset.block_size,\n",
    "                    id_block_size=train_dataset.id_block_size,\n",
    "                    show_grads=False, plot_raster=False,\n",
    "                    ckpt_path=model_path, no_pbar=False)\n",
    "# f\"/home/antonis/projects/slab/git/neuroformer/models/model_sim_weighted_shuffle_decay:{shuffle}_perceiver_2.0_dt:{dt}_eos_{mconf.n_layer}_{mconf.n_head}_{mconf.n_embd}.pt\")\n",
    "\n",
    "\n",
    "trainer = Trainer(model, train_dataset, test_dataset, tconf, mconf)\n",
    "trainer.train()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "iterable = iter(loader)\n",
    "x, y = next(iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(test_dataset, batch_size=150, shuffle=False, num_workers=0)\n",
    "iterable = iter(loader)\n",
    "x, y = next(iterable)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "model = model.to(device)\n",
    "for key in x.keys():\n",
    "    x[key] = x[key].to(device)\n",
    "for key in y.keys():\n",
    "    y[key] = y[key].to(device)\n",
    "_, _, _, = model(x, y)\n",
    "\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "RUN SIMULATION\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from IPython.utils import io\n",
    "# top_p=0.25, top_p_t=0.9, temp=2.\n",
    "\n",
    "# for idx in range(len(df_pred['ID'])):\n",
    "#     df_pred['ID'][idx] = itos[df_pred['ID'][idx]]\n",
    "\n",
    "# model_path = \"/home/antonis/projects/slab/git/neuroformer/models/tensorboard/V1_AL_modalities/cont_weighted/25_Cont:True_0.50.1_sparseFalse_conv_True_shuffle:True_batch:224_sparse_(None_None)_pos_emb:False_temp_emb:True_drop:0.2_dt:True_2.0_0.5_max0.1_(4, 4, 6)_2_256_nembframe64.pt\"\n",
    "# model_path = \"/Users/antonis/Downloads/[16, 17, 18, 19]_Cont_True_0.50.05_sparseFalse_conv_True_shuffle_True_batch_224_sparse_(None_None)_pos_emb_False_temp_emb_True_drop_0.2_dt_True_2.0_0.5_max0.05_(4, 0, 6)_2_256_nembframe64-2.pt\"\n",
    "# model_path = \"/Users/antonis/projects/slab/neuroformer/neuroformer/models/tensorboard/V1_AL/sos_clip/25_Cont:True_0.50.1_sparseFalse_conv_True_shuffle:True_batch:96_sparse_(200_4)_pos_emb:False_temp_emb:True_drop:0.2_dt:True_2.0_0.5_max0.1_(2, 2, 2)_2_256_nembframe64_(20, 8, 8).pt\"\n",
    "model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "\n",
    "# trials = np.random.choice(train_data['Trial'].unique(), size=12)\n",
    "# trials = test_data['Trial'].unique()\n",
    "trials = train_data['Trial'].unique()[::4]\n",
    "# trials = train_data['Trial'].unique()\n",
    "# trials = train_data['Trial'].unique()[::4]\n",
    "\n",
    "# trials = [start + (i * 20) for i in range(3)]\n",
    "# trials = df['Trial'].unique()[0::20] \n",
    "results_dict = dict()\n",
    "# for n in range(2, 20):\n",
    "df_pred = None\n",
    "df_true = None\n",
    "n_p = 0.3   # (n + 1) * 0.05\n",
    "temp = 2\n",
    "# stoi['SOS'] = 2000\n",
    "for trial in trials:    # test_data['Trial'].unique():\n",
    "    # with io.capture_output() as captured:\n",
    "        print(f\"Trial: {trial}\")\n",
    "        df_trial = df[df['Trial'] == trial]\n",
    "        trial_dataset = SpikeTimeVidData2(df_trial, None, block_size, id_block_size, frame_block_size, prev_id_block_size, \n",
    "                                          window, dt, frame_memory, stoi, itos, neurons, stoi_dt, itos_dt, frame_feats, \n",
    "                                          pred=False, window_prev=window_prev, frame_window=frame_window)\n",
    "        trial_loader = DataLoader(trial_dataset, shuffle=False, pin_memory=False)\n",
    "        results_trial = predict_raster_recursive_time_auto(model, trial_loader, window, window_prev, stoi, itos_dt, itos=itos, sample=True, top_p=0.75, top_p_t=0.75, temp=1.3, temp_t=1.3, frame_end=0, get_dt=True, gpu=False, pred_dt=True)\n",
    "        # results_trial = predict_raster_hungarian(model, loader, itos_dt, top_p=0.75, temp=1)\n",
    "        # print(f\"MAX ID ---- {sorted(results_trial['ID'].unique()[-10])}\")\n",
    "        df_trial_pred, df_trial_true = process_predictions(results_trial, stoi, itos, window)\n",
    "        print(f\"pred: {df_trial_pred.shape}, true: {df_trial_true.shape}\" )\n",
    "        if df_pred is None:\n",
    "            df_pred = df_trial_pred\n",
    "            df_true = df_trial_true\n",
    "        else:\n",
    "            df_pred = pd.concat([df_pred, df_trial_pred])\n",
    "            df_true = pd.concat([df_true, df_trial_true])\n",
    "\n",
    "# df_preds[n] = df_pred\n",
    "# print(f\"--- n: {n}, n_p: {n_p}, temp: {temp} ---\")\n",
    "scores = compute_scores(df[df['Trial'].isin(trials)], df_pred)\n",
    "print(scores)\n",
    "print(f\"pred: {len(df_pred)}, true: {len(df_true)}\" )\n",
    "# results_dict[n] = (scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Split data into full-stimulus trials\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "t_1, t_2 = 35, 36\n",
    "trial_data_1 = df[df['Trial'] == t_1]\n",
    "trial_dataset_1 = SpikeTimeVidData2(train_data, None, block_size, id_block_size, frame_block_size, prev_id_block_size, window, dt, frame_memory, stoi, itos, neurons, stoi_dt, itos_dt, frame_feats, pred=False, window_prev=window_prev)\n",
    "trial_loader_1 = DataLoader(trial_dataset_1, shuffle=False, pin_memory=False)\n",
    "\n",
    "def loader_trial(df, n_trial):\n",
    "    trial_data = df[df['Trial'] == n_trial]\n",
    "    trial_dataset = SpikeTimeVidData2(trial_data, None, block_size, id_block_size, frame_block_size, prev_id_block_size, window, dt, frame_memory, stoi, itos, neurons, stoi_dt, itos_dt, frame_feats, pred=False, window_prev=window_prev)\n",
    "    trial_loader = DataLoader(trial_dataset, shuffle=False, pin_memory=False)\n",
    "    return trial_loader\n",
    "\n",
    "trial_data_1 = loader_trial(df, t_1)\n",
    "trial_data_2 = loader_trial(df, t_2)\n",
    "\n",
    "iterable1 = iter(trial_data_1)\n",
    "iterable2 = iter(trial_data_2)\n",
    "\n",
    "# train_len = round(len(df)*(4/5))\n",
    "# test_len = round(len(df) - train_len)\n",
    "\n",
    "# train_data = df[:train_len]\n",
    "# test_data = df[train_len:train_len + test_len].reset_index().drop(['index'], axis=1)\n",
    "n_trial = [2, 8, 14, 19]\n",
    "\n",
    "n_1 = []\n",
    "for n_stim in range(3): # range(df['Trial'].max() // 20):\n",
    "    for n_t in n_trial:\n",
    "        trial = (n_stim + 1) * 20 - (n_t - 2)\n",
    "        n_1.append(trial)\n",
    "test2_data = df[df['Trial'].isin(n_1)].reset_index(drop=True)\n",
    "small_data = df[df['Trial'].isin([5])].reset_index(drop=True)\n",
    "\n",
    "\n",
    "n_2 = []\n",
    "for n_stim in range(3): # range(df['Trial'].max() // 20):\n",
    "    for n_t in n_trial:\n",
    "        trial = (n_stim + 1) * 20 - (n_t - 1)\n",
    "        n_2.append(trial)\n",
    "test3_data = df[df['Trial'].isin(n_2)].reset_index(drop=True)\n",
    "# small_data = df[df['Trial'].isin([5])].reset_index(drop=True)\n",
    "\n",
    "print(f\"trials: {test2_data['Trial'].unique()}\")\n",
    "print(f\"trials: {test3_data['Trial'].unique()}\")\n",
    "\n",
    "from analysis import *\n",
    "from utils import *\n",
    "from SpikeVidUtils import create_full_trial\n",
    "\n",
    "# df_1 = df_pred[df_pred['ID'] < stoi['SO`S']].reset_index(drop=True)\n",
    "# df_2 = test_data[test_data['Trial'].isin(trials)].reset_index(drop=True)\n",
    "\n",
    "# df_1 = df[df['Trial'].isin(range(10))]\n",
    "# df_2 = df[df['Trial'].isin(range(11, 20))]\n",
    "\n",
    "df_pred_full = create_full_trial(df_pred)\n",
    "df_1 = create_full_trial(df, trials)\n",
    "df_2 = create_full_trial(df, n_1)\n",
    "df_3 = create_full_trial(df, n_2)\n",
    "\n",
    "# df_2['Interval'] += 0.5\n",
    "# df_pred_full['Interval'] += 0.5\n",
    "\n",
    "# df_1 = create_full_trial(df, t_trial=t_trial, n_start=df_pred['Trial'].min(), n_stim=3, n_step=20, n_trials=10)\n",
    "# df_2 = create_full_trial(df, t_trial=t_trial, n_start=df_pred['Trial'].min() + 1, n_stim=3, n_step=20, n_trials=10)\n",
    "# df_3 = create_full_trial(df, t_trial=t_trial, n_start=df_pred['Trial'].min() + 2, n_stim=3, n_step=20, n_trials=10)\n",
    "\n",
    "# df_1 = df_1[(df_1['Interval'].isin(df_pred_full['Interval'].unique()))].reset_index(drop=True)\n",
    "# df_2 = df_2[(df_2['Interval'].isin(df_pred_full['Interval'].unique()))].reset_index(drop=True)\n",
    "# df_3 = df_3[(df_3['Interval'].isin(df_pred_full['Interval'].unique()))].reset_index(drop=True)\n",
    "\n",
    "window_pred = None\n",
    "\n",
    "df_list = [df_pred_full, df_1, df_2, df_3]\n",
    "\n",
    "for df_ in df_list:\n",
    "    window_pred = 0.5\n",
    "    df_['Interval'] = make_intervals(df_, window_pred)\n",
    "\n",
    "window_pred = window if window_pred is None else window_pred\n",
    "intervals = np.array(sorted(set(df['Interval'].unique()) & set(df['Interval'].unique())))\n",
    "labels = np.array([round(window_pred + window_pred*n, 2) for n in range(0, int(max(df_pred_full['Interval']) / window_pred))])\n",
    "ids = sorted(set(df['ID'].unique()) & set(df['ID'].unique()))\n",
    "\n",
    "\n",
    "# labels = sorted(set(df_pred_full['Interval'].unique()))\n",
    "rates_pred = get_rates_trial(df_pred_full, labels)\n",
    "rates_1 = get_rates_trial(df_1, labels)\n",
    "rates_2 = get_rates_trial(df_2, labels)\n",
    "rates_3 = get_rates_trial(df_3, labels)\n",
    "\n",
    "top_corr_pred = calc_corr_psth(rates_pred, rates_1)\n",
    "top_corr_real = calc_corr_psth(rates_1, rates_2)\n",
    "top_corr_real_2 = calc_corr_psth(rates_1, rates_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Evaluate results\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "from visualize import *\n",
    "\n",
    "# df_2['Trial'] -= 2\n",
    "id_pred, id_true_1, id_true_2 = len(df_pred_full['ID'].unique()), len(df_1['ID'].unique()), len(df_2['ID'].unique())\n",
    "print(f\"id_pred: {id_pred}, id_true_1: {id_true_1}, id_true_2: {id_true_2}\")\n",
    "\n",
    "len_pred, len_true = len(df_pred_full), len(df_1)\n",
    "print(f\"len_pred: {len_pred}, len_true: {len_true}\")\n",
    "\n",
    "accuracy = get_accuracy(df_pred, df_2)\n",
    "\n",
    "scores = compute_scores(df_1, df_2)\n",
    "pred_scores = compute_scores(df_1, df_pred_full)\n",
    "print(f\"real: {scores}\")\n",
    "print(f\"pred: {pred_scores}\")\n",
    "\n",
    "set_plot_white()\n",
    "plt.figure(figsize=(10, 10), facecolor='white')\n",
    "plt.title('PSTH Correlations (V1 + AL)', fontsize=25)\n",
    "plt.ylabel('Count (n)', fontsize=25)\n",
    "plt.xlabel('Pearson r', fontsize=25)\n",
    "plt.hist(top_corr_real, label='real - real2', alpha=0.6)\n",
    "# plt.hist(top_corr_real_2, label='real - real3', alpha=0.6)\n",
    "plt.hist(top_corr_pred, label='real - simulated', alpha=0.6)\n",
    "plt.legend(fontsize=20)\n",
    "\n",
    "dir_name = os.path.dirname(model_path)\n",
    "plt.savefig(os.path.join(dir_name, 'psth_corr.svg'))\n",
    "\n",
    "plot_distribution(df_1, df_pred)\n",
    "\n",
    "total_scores = dict()\n",
    "total_scores['real'] = scores\n",
    "total_scores['pred'] = pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('neuroformer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "787a8a7742a4832f0fd0db3e1615915b3401a416515140940799f74370bf26af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
