{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter notebook\n",
      " // CONTRASTIVE: True //\n",
      " // VISUAL: True //\n",
      " // PAST_STATE: True //\n",
      " // PREDICT_BEHAVIOR: False //\n",
      " // BEHAVIOR: True //\n",
      " // FUSE_STIM_BEHAVIOR: False //\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pathlib\n",
    "import glob\n",
    "import os\n",
    "import collections\n",
    "import json\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "import glob\n",
    "from pathlib import Path, PurePath\n",
    "path = Path.cwd()\n",
    "parent_path = path.parents[1]\n",
    "sys.path.append(str(PurePath(parent_path, 'neuroformer')))\n",
    "sys.path.append('neuroformer')\n",
    "sys.path.append('.')\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from neuroformer.model_neuroformer import GPT, GPTConfig, neuralGPTConfig\n",
    "from neuroformer.trainer import Trainer, TrainerConfig\n",
    "from neuroformer.utils import set_seed, update_object, check_common_attrs\n",
    "from neuroformer.visualize import set_plot_params\n",
    "from neuroformer.SpikeVidUtils import round_n, set_intervals\n",
    "set_plot_params()\n",
    "\n",
    "from scipy import io as scipyio\n",
    "from scipy.special import softmax\n",
    "import skimage\n",
    "import skvideo.io\n",
    "from scipy.ndimage import gaussian_filter, uniform_filter\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "parent_path = os.path.dirname(os.path.dirname(os.getcwd())) + \"/\"\n",
    "import argparse\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument(\"--infer\", action=\"store_true\", help=\"Inference mode\")\n",
    "    parser.add_argument(\"--train\", action=\"store_true\", default=False, help=\"Train mode\")\n",
    "    parser.add_argument(\"--infer\", action=\"store_true\", default=False, help=\"Inference mode\")    \n",
    "    parser.add_argument(\"--finetune\", action=\"store_true\", default=False, help=\"Finetune\")\n",
    "    parser.add_argument(\"--pdata\", type=float, default=None, help=\"Proportion of data to finetune on\")\n",
    "    parser.add_argument(\"--dataset\", type=str, default=\"HippocampusPos\", help=\"Dataset\")\n",
    "    parser.add_argument(\"--dist\", action=\"store_true\", default=False, help=\"Distrinuted training\")\n",
    "    parser.add_argument(\"--resume\", type=str, default=None, help=\"Resume from checkpoint\")\n",
    "    parser.add_argument(\"--rand_perm\", action=\"store_true\", default=False, help=\"Randomly permute the ID column\")\n",
    "    parser.add_argument(\"--mconf\", type=str, default=None, help=\"Path to model config file\")\n",
    "    parser.add_argument(\"--downstream\", action=\"store_true\", default=False, help=\"Downstream task\")\n",
    "    parser.add_argument(\"--freeze_model\", action=\"store_true\", default=False, help=\"Freeze model\")\n",
    "    parser.add_argument(\"--title\", type=str, default=None)\n",
    "    parser.add_argument(\"--seed\", type=int, default=25)\n",
    "    parser.add_argument(\"--behavior\", action=\"store_true\", default=False, help=\"Behavior task\")\n",
    "    parser.add_argument(\"--predict_behavior\", action=\"store_true\", default=False, help=\"Predict behavior\")\n",
    "    # parser.add_argument(\"--behavior_vars\", type=str, default=None, help=\"Behavior variables\")\n",
    "    parser.add_argument(\"--behavior_vars\", nargs='+', default=None, help=\"Behavior variables\")\n",
    "    parser.add_argument(\"--round_vars\", action=\"store_true\", default=False, help=\"Round variables\")\n",
    "    parser.add_argument(\"--past_state\", action=\"store_true\", default=False, help=\"Input past state\")\n",
    "    parser.add_argument(\"--visual\", action=\"store_true\", default=False, help=\"Visualize\")\n",
    "    parser.add_argument(\"--contrastive\", action=\"store_true\", default=False, help=\"Contrastive\")\n",
    "    parser.add_argument(\"--clip_vars\", nargs='+', default=None, help=\"Clip variables\")\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=0)\n",
    "    parser.add_argument(\"--fuse_stim_behavior\", action=\"store_true\", default=False, help=\"Fuse stimulus and behavior\")\n",
    "    parser.add_argument(\"--mlp_only\", action=\"store_true\", default=False, help=\"MLP only\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     args = parse_args()\n",
    "#     INFERENCE = not args.train\n",
    "# else:\n",
    "#     INFERENCE = True\n",
    "\n",
    "# check if jupyter notebook\n",
    "\n",
    "try:\n",
    "    shell = get_ipython().__class__.__name__\n",
    "    print(\"Running in Jupyter notebook\")\n",
    "    TRAIN = False\n",
    "    INFERENCE = True\n",
    "    DATASET = \"HippocampusPosCEBRA\"  #\"HippocampusPos\"\n",
    "    DIST = False\n",
    "    DOWNSTREAM = False\n",
    "    RAND_PERM = False\n",
    "    RESUME = \"././models/tensorboard/HippocampusPosCEBRA/behavior_pred_exp/classification/ablations_1/finetuning_None_resumeFalse/behavior_before_stim_RESUMEFalse_paststateTrue_method_behavior_True_['Position', 'Dir_Right', 'Dir_Left']_predictbehaviorFalse_roundedFalsevisualFalse_contrastiveTrue_['id', 'behavior_mean']/sparse_f:None_id:None/w:0.025_wp:0.1/6_Cont:False_window:0.025_f_window:None_df:0.025_blocksize:100_conv_True_shuffle:True_batch:224_sparse_(None_None)_blocksz400_pos_emb:False_temp_emb:True_drop:0.35_dt:True_2.0_6_max0.025_(8, 8, 0)_8_256.pt\"\n",
    "    MCONF = \"./configs/cebra/HippocampusPos/mconf.yaml\"\n",
    "    FREEZE_MODEL = False\n",
    "    TITLE = None\n",
    "    SEED = 25\n",
    "    BEHAVIOR = True\n",
    "    PREDICT_BEHAVIOR = False\n",
    "    BEHAVIOR_VARS = ['Position', 'Dir_Right', 'Dir_Left']\n",
    "    ROUND_VARS = False\n",
    "    PAST_STATE = True\n",
    "    VISUAL = True\n",
    "    CONTRASTIVE = True\n",
    "    CLIP_VARS = ['id', 'behavior_mean']\n",
    "    FUSE_STIM_BEHAVIOR = False\n",
    "    FINETUNE = False\n",
    "    PDATA = 0.1\n",
    "    MLP_ONLY = False\n",
    "    PARALLEL = False\n",
    "except:\n",
    "    print(\"Running in terminal\")\n",
    "    args = parse_args()\n",
    "    TRAIN = args.train\n",
    "    INFERENCE = args.infer\n",
    "    DATASET = args.dataset\n",
    "    DIST = args.dist\n",
    "    DOWNSTREAM = args.downstream\n",
    "    RESUME = args.resume\n",
    "    RAND_PERM = args.rand_perm\n",
    "    MCONF = args.mconf\n",
    "    FREEZE_MODEL = args.freeze_model\n",
    "    TITLE = args.title\n",
    "    SEED = args.seed\n",
    "    BEHAVIOR = args.behavior\n",
    "    PREDICT_BEHAVIOR = args.predict_behavior\n",
    "    BEHAVIOR_VARS = args.behavior_vars\n",
    "    ROUND_VARS = args.round_vars\n",
    "    PAST_STATE = args.past_state\n",
    "    VISUAL = args.visual\n",
    "    CONTRASTIVE = args.contrastive\n",
    "    CLIP_VARS = args.clip_vars\n",
    "    FUSE_STIM_BEHAVIOR = args.fuse_stim_behavior\n",
    "    FINETUNE = args.finetune\n",
    "    PDATA = args.pdata\n",
    "    MLP_ONLY = args.mlp_only\n",
    "    PARALLEL = True\n",
    "    \n",
    "set_seed(25)\n",
    "\n",
    "print(f\" // CONTRASTIVE: {CONTRASTIVE} //\")\n",
    "print(f\" // VISUAL: {VISUAL} //\")\n",
    "print(f\" // PAST_STATE: {PAST_STATE} //\")\n",
    "print(f\" // PREDICT_BEHAVIOR: {PREDICT_BEHAVIOR} //\")\n",
    "print(f\" // BEHAVIOR: {BEHAVIOR} //\")\n",
    "print(f\" // FUSE_STIM_BEHAVIOR: {FUSE_STIM_BEHAVIOR} //\")\n",
    "\n",
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroformer.prepare_data import DataLinks\n",
    "\n",
    "\n",
    "DATA_POINTERS = getattr(DataLinks, DATASET)\n",
    "\n",
    "# if not os.path.exists(data_dir):\n",
    "#     print(\"Downloading data...\")\n",
    "#     import gdown\n",
    "#     url = DATA_POINTERS['url']\n",
    "#     gdown.download_folder(id=url, quiet=False, use_cookies=False, output=DATA_POINTERS['DIRECTORY'])\n",
    "\n",
    "df = pd.read_csv(DATA_POINTERS['RESPONSE_PATH'])\n",
    "df_behavior = pd.read_csv(DATA_POINTERS['BEHAVIOR_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config files\n",
    "import yaml\n",
    "\n",
    "# base_path = \"configs/visnav/predict_behavior\"\n",
    "base_path = \"./models/tensorboard/visnav_medial\" if MCONF is None else os.path.dirname(MCONF)\n",
    "\n",
    "with open(os.path.join(base_path, 'mconf.yaml'), 'r') as stream:\n",
    "    mconf = yaml.full_load(stream)\n",
    "\n",
    "with open(os.path.join(base_path, 'tconf.yaml'), 'r') as stream:\n",
    "    tconf = yaml.full_load(stream)\n",
    "\n",
    "with open(os.path.join(base_path, 'dconf.yaml'), 'r') as stream:\n",
    "    dconf = yaml.full_load(stream)\n",
    "\n",
    "# open yaml as omegacong\n",
    "mconf = OmegaConf.create(mconf)\n",
    "tconf = OmegaConf.create(tconf)\n",
    "dconf = OmegaConf.create(dconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common attributes: {'dt_vars': (0.025, [{'_is_protocol': False, 'block_size': 446, 'data_dict': None, 'dataset': 'visnav', 'dt_speed': 0.2, 'dt_vars': 0.025, 'idx': 0, 'interval': 0, 'intervals': None}])}\n"
     ]
    }
   ],
   "source": [
    "if INFERENCE or DOWNSTREAM or FINETUNE or TRAIN: \n",
    "    window = mconf.window\n",
    "    window_prev = mconf.window_prev\n",
    "    frame_window = mconf.frame_window if hasattr(mconf, 'frame_window') else None\n",
    "    window_behavior = mconf.window_behavior if hasattr(mconf, 'window_behavior') else None\n",
    "    dt = mconf.dt\n",
    "    dt_frames = mconf.dt_frames if hasattr(mconf, 'dt_frames') else 0.05\n",
    "    dt_vars = mconf.dt_vars if hasattr(mconf, 'dt_vars') else 0.05\n",
    "    dt_speed = mconf.dt_speed if hasattr(mconf, 'dt_speed') else 0.2\n",
    "    intervals = None\n",
    "else:\n",
    "    window = 0.025\n",
    "    window_prev = 0.1\n",
    "    frame_window = window + window_prev\n",
    "    window_behavior = window\n",
    "    dt = 0.0001\n",
    "    dt_frames = 0.05\n",
    "    dt_vars = 0.05\n",
    "    dt_speed = 0.2\n",
    "    intervals = None\n",
    "\n",
    "# set attrs that are not equal\n",
    "common_attrs = check_common_attrs(mconf, tconf, dconf)\n",
    "print(f\"Common attributes: {common_attrs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " // using behavior vars: ['Position', 'Dir_Right', 'Dir_Left'] //\n"
     ]
    }
   ],
   "source": [
    "## choose modalities ##\n",
    "\n",
    "# behavior\n",
    "behavior = BEHAVIOR\n",
    "# behavior_vars = ['t', 'eyerad', 'phi', 'speed', 'th']\n",
    "behavior_vars = ['locations', 'locations_left', 'locations_right'] if BEHAVIOR_VARS is None else BEHAVIOR_VARS\n",
    "n_behavior = len(behavior_vars)\n",
    "predict_behavior = PREDICT_BEHAVIOR\n",
    "# stimulus\n",
    "visual_stim = None\n",
    "\n",
    "print(f\" // using behavior vars: {BEHAVIOR_VARS} //\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior = BEHAVIOR\n",
    "# df_behavior = None\n",
    "# behavior_vars = None\n",
    "# behavior_block_size = 0\n",
    "samples_per_behavior = 1\n",
    "stoi_speed = None\n",
    "itos_speed = None\n",
    "dt_range_speed = None\n",
    "n_behavior = None\n",
    "stoi_phi = None\n",
    "itos_phi = None\n",
    "dt_range_phi = None\n",
    "stoi_th = None\n",
    "itos_th = None\n",
    "dt_range_th = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.175 in df_behavior['Interval'].unique()\n",
    "# 1.175 in df['Interval'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroformer.SpikeVidUtils import make_intervals, round_n\n",
    "\n",
    "if behavior:\n",
    "    df_behavior['Interval'] = make_intervals(df_behavior, window=window)\n",
    "\n",
    "df['Interval'] = make_intervals(df, window, col='Time')\n",
    "df['Interval_2'] = make_intervals(df, window_prev, col='Time')\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df = df[df['Interval'].isin(df_behavior['Interval'].unique())]\n",
    "\n",
    "max_window = max(window, window_prev)\n",
    "dt_range = math.ceil(max_window / dt) + 1  # add first / last interval for SOS / EOS'\n",
    "n_dt = [round_n(dt * n, dt) for n in range(dt_range)] + ['EOS'] + ['PAD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique groups: 8926\n",
      "Top 0.05 groups: Interval  Trial\n",
      "48.925    1        30\n",
      "6.375     1        28\n",
      "16.925    1        28\n",
      "150.975   1        28\n",
      "8.600     1        27\n",
      "                   ..\n",
      "69.625    1        13\n",
      "70.075    1        13\n",
      "71.825    1        13\n",
      "72.125    1        13\n",
      "76.825    1        13\n",
      "Length: 446, dtype: int64\n",
      "Mean groups: 5.990477257450146\n"
     ]
    }
   ],
   "source": [
    "var_group = 'Interval'\n",
    "groups = df.groupby([var_group, 'Trial']).size()\n",
    "n_unique = len(groups)\n",
    "top_p = 0.05\n",
    "top_k_groups = groups.nlargest(int(top_p * n_unique))\n",
    "mean_groups = groups.mean()\n",
    "\n",
    "print(f\"Unique groups: {n_unique}\")\n",
    "print(f\"Top {top_p} groups: {top_k_groups}\")\n",
    "print(f\"Mean groups: {mean_groups}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique groups: 2544\n",
      "Top 0.2 groups: Interval_2  Trial\n",
      "67.2        1        70\n",
      "76.2        1        56\n",
      "17.0        1        55\n",
      "107.8       1        55\n",
      "85.7        1        54\n",
      "                     ..\n",
      "85.6        1        28\n",
      "88.0        1        28\n",
      "88.7        1        28\n",
      "89.5        1        28\n",
      "91.4        1        28\n",
      "Length: 508, dtype: int64\n",
      "Mean groups: 21.018474842767297\n"
     ]
    }
   ],
   "source": [
    "var_group = 'Interval_2'\n",
    "groups = df.groupby([var_group, 'Trial']).size()\n",
    "n_unique = len(groups)\n",
    "top_p = 0.2\n",
    "top_k_groups = groups.nlargest(int(top_p * n_unique))\n",
    "mean_groups = groups.mean()\n",
    "\n",
    "print(f\"Unique groups: {n_unique}\")\n",
    "print(f\"Top {top_p} groups: {top_k_groups}\")\n",
    "print(f\"Mean groups: {mean_groups}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroformer.SpikeVidUtils import SpikeTimeVidData2\n",
    "\n",
    "## resnet3d feats\n",
    "n_frames = None\n",
    "kernel_size = (n_frames, 5, 5)\n",
    "n_embd = 256\n",
    "n_embd_frames = 64\n",
    "frame_feats = None\n",
    "frame_block_size = 0\n",
    "conv_layer = True\n",
    "\n",
    "prev_id_block_size = 300\n",
    "id_block_size = 100   #\n",
    "behavior_block_size = len(behavior_vars)\n",
    "block_size = frame_block_size + id_block_size + prev_id_block_size\n",
    "frame_memory = None\n",
    "window = window\n",
    "\n",
    "neurons = sorted(list(set(df['ID'])))\n",
    "id_stoi = { ch:i for i,ch in enumerate(neurons) }\n",
    "id_itos = { i:ch for i,ch in enumerate(neurons) }\n",
    "\n",
    "neurons = sorted(list(set(df['ID'].unique())))\n",
    "trial_tokens = [f\"Trial {n}\" for n in df['Trial'].unique()]\n",
    "feat_encodings = neurons + ['SOS'] + ['EOS'] + ['PAD']  # + pixels \n",
    "stoi = { ch:i for i,ch in enumerate(feat_encodings) }\n",
    "itos = { i:ch for i,ch in enumerate(feat_encodings) }\n",
    "stoi_dt = { ch:i for i,ch in enumerate(n_dt) }\n",
    "itos_dt = { i:ch for i,ch in enumerate(n_dt) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "r_split = 0.8\n",
    "# all_trials = sorted(df['Trial'].unique())\n",
    "# train_trials = random.sample(all_trials, int(len(all_trials) * r_split))\n",
    "\n",
    "# train_data = df[df['Trial'].isin(train_trials)]\n",
    "# test_data = df[~df['Trial'].isin(train_trials)]\n",
    "\n",
    "len_train = int(len(df) * r_split)\n",
    "train_data = df\n",
    "# train_data = df.iloc[:len_train].reset_index(drop=True)\n",
    "# test_data = df.iloc[len_train:].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population Size:  122\n",
      "ID Population Size:  122\n",
      "DT Population Size:  7\n",
      "Min Interval: 0.125\n",
      "train: 8921\n"
     ]
    }
   ],
   "source": [
    "from neuroformer.SpikeVidUtils import SpikeTimeVidData2\n",
    "\n",
    "\n",
    "train_dataset = SpikeTimeVidData2(train_data, None, block_size, id_block_size, frame_block_size, prev_id_block_size, \n",
    "                                  window, dt, frame_memory, stoi, itos, neurons, stoi_dt, itos_dt, frame_feats,\n",
    "                                  pred=False, window_prev=window_prev, frame_window=frame_window,\n",
    "                                  dt_frames=dt_frames, intervals=None, dataset=DATASET,\n",
    "                                  behavior=df_behavior, behavior_vars=behavior_vars, dt_vars=dt_vars,\n",
    "                                  behavior_block_size=behavior_block_size, samples_per_behavior=samples_per_behavior,\n",
    "                                  window_behavior=window_behavior, predict_behavior=predict_behavior,\n",
    "                                  stoi_speed=stoi_speed, itos_speed=itos_speed, dt_speed=dt_speed) # stoi_phi=stoi_phi, itos_phi=itos_phi, \n",
    "                                  # dt_phi=dt_phi, stoi_th=stoi_th, itos_th=itos_th, dt_th=dt_th)\n",
    "\n",
    "test_dataset = None\n",
    "\n",
    "print(f'train: {len(train_dataset)}')\n",
    "if test_dataset is not None:\n",
    "    print(f'test: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(train_dataset, batch_size=2, shuffle=False, num_workers=4, pin_memory=True)\n",
    "iterable = iter(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8921"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Time</th>\n",
       "      <th>Trial</th>\n",
       "      <th>Interval</th>\n",
       "      <th>Interval_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>0.175</td>\n",
       "      <td>1</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>0.200</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>0.225</td>\n",
       "      <td>1</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103</td>\n",
       "      <td>0.275</td>\n",
       "      <td>1</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8916</th>\n",
       "      <td>16</td>\n",
       "      <td>254.325</td>\n",
       "      <td>1</td>\n",
       "      <td>254.325</td>\n",
       "      <td>254.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8917</th>\n",
       "      <td>16</td>\n",
       "      <td>254.350</td>\n",
       "      <td>1</td>\n",
       "      <td>254.350</td>\n",
       "      <td>254.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8918</th>\n",
       "      <td>3</td>\n",
       "      <td>254.375</td>\n",
       "      <td>1</td>\n",
       "      <td>254.375</td>\n",
       "      <td>254.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8919</th>\n",
       "      <td>19</td>\n",
       "      <td>254.400</td>\n",
       "      <td>1</td>\n",
       "      <td>254.400</td>\n",
       "      <td>254.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8920</th>\n",
       "      <td>16</td>\n",
       "      <td>254.425</td>\n",
       "      <td>1</td>\n",
       "      <td>254.425</td>\n",
       "      <td>254.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8921 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID     Time  Trial  Interval  Interval_2\n",
       "0      44    0.125      1     0.125         0.2\n",
       "1      47    0.175      1     0.175         0.2\n",
       "2      23    0.200      1     0.200         0.2\n",
       "3      15    0.225      1     0.225         0.3\n",
       "4     103    0.275      1     0.275         0.3\n",
       "...   ...      ...    ...       ...         ...\n",
       "8916   16  254.325      1   254.325       254.4\n",
       "8917   16  254.350      1   254.350       254.4\n",
       "8918    3  254.375      1   254.375       254.4\n",
       "8919   19  254.400      1   254.400       254.4\n",
       "8920   16  254.425      1   254.425       254.5\n",
       "\n",
       "[8921 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_prev torch.Size([2, 300])\n",
      "dt_prev torch.Size([2, 300])\n",
      "pad_prev torch.Size([2])\n",
      "id torch.Size([2, 100])\n",
      "dt torch.Size([2, 100])\n",
      "pad torch.Size([2])\n",
      "interval torch.Size([2])\n",
      "trial torch.Size([2])\n",
      "behavior torch.Size([2, 3, 1, 1])\n",
      "behavior_dt torch.Size([2, 1])\n",
      "cid torch.Size([2, 2])\n",
      "pid torch.Size([2, 2])\n",
      "y: id, torch.Size([2, 100])\n",
      "y: dt, torch.Size([2, 100])\n",
      "tensor([[0.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iterable)\n",
    "# print(x['behavior'].shape, x['behavior_dt'].shape)\n",
    "for k in x.keys():\n",
    "    print(k, x[k].shape)\n",
    "for k in y.keys():\n",
    "    print(f\"y: {k}, {y[k].shape}\")\n",
    "print(x['behavior_dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// -- updating model conf -- //\n",
      "// -- contrastive objective -- //\n",
      "// -- CLIP VARS: ['id', 'behavior_mean'] -- //\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/08/2023 14:17:33 - INFO - neuroformer.model_neuroformer -   number of parameters: 2.696858e+07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 7\n",
      "// -- Resuming model -- //\n"
     ]
    }
   ],
   "source": [
    "layers = (mconf.n_state_layers, mconf.n_state_history_layers, mconf.n_stimulus_layers)   \n",
    "max_epochs = 750\n",
    "batch_size = round((32 * 7))\n",
    "shuffle = True\n",
    "\n",
    "model_conf = GPTConfig(train_dataset.population_size, block_size,    # frame_block_size\n",
    "                        id_vocab_size=train_dataset.id_population_size,\n",
    "                        frame_block_size=frame_block_size,\n",
    "                        id_block_size=id_block_size,  # frame_block_size\n",
    "                        prev_id_block_size=prev_id_block_size,\n",
    "                        behavior_block_size=behavior_block_size,\n",
    "                        sparse_mask=False, p_sparse=None, \n",
    "                        sparse_topk_frame=None, sparse_topk_id=None, sparse_topk_prev_id=None,\n",
    "                        n_dt=len(n_dt),\n",
    "                        class_weights=None,\n",
    "                        pretrain=False,\n",
    "                        n_state_layers=mconf.n_state_layers, n_state_history_layers=mconf.n_state_history_layers,\n",
    "                        n_stimulus_layers=mconf.n_stimulus_layers, self_att_layers=mconf.self_att_layers,\n",
    "                        n_behavior_layers=mconf.n_behavior_layers, predict_behavior=predict_behavior, n_behavior=n_behavior,\n",
    "                        n_head=mconf.n_head, n_embd=mconf.n_embd, \n",
    "                        contrastive=mconf.contrastive, clip_emb=mconf.clip_emb, clip_temp=mconf.clip_temp, clip_loss=False,\n",
    "                        conv_layer=conv_layer, kernel_size=kernel_size,\n",
    "                        temp_emb=mconf.temp_emb, pos_emb=False, wave_emb=True,\n",
    "                        id_drop=0.35, im_drop=0.35, b_drop=0.45,\n",
    "                        window=window, window_prev=window_prev, frame_window=frame_window, dt=dt,\n",
    "                        neurons=neurons, stoi_dt=stoi_dt, itos_dt=itos_dt, n_embd_frames=n_embd_frames,\n",
    "                        ignore_index_id=stoi['PAD'], ignore_index_dt=stoi_dt['PAD'],\n",
    "                        fuse_stim_behavior=FUSE_STIM_BEHAVIOR, mlp_only=MLP_ONLY)  # 0.35\n",
    "\n",
    "# update_object(model_conf, mconf)\n",
    "model_conf.contrastive_vars = ['id', 'behavior_mean']\n",
    "\n",
    "if INFERENCE or MCONF is not None:\n",
    "    update_object(model_conf, mconf)\n",
    "\n",
    "if TRAIN or FINETUNE or INFERENCE:\n",
    "    if MCONF is not None:\n",
    "        print(f\"// -- updating model conf -- //\")\n",
    "        update_object(model_conf, mconf)\n",
    "\n",
    "    if BEHAVIOR and not PREDICT_BEHAVIOR:\n",
    "        model_conf.contrastive_vars += ['behavior_mean']\n",
    "        \n",
    "    if PREDICT_BEHAVIOR is True:\n",
    "        print(f\"// Predict behavior: n_behavior_layers = 0 //\")\n",
    "        model_conf.n_behavior_layers = 0\n",
    "        model_conf.predict_behavior = True\n",
    "\n",
    "    if PAST_STATE is False:\n",
    "        print(f\"// -- No past state, layers=0 -- //\")\n",
    "        model_conf.n_state_history_layers = 0\n",
    "\n",
    "    if CONTRASTIVE is True:\n",
    "        print(f\"// -- contrastive objective -- //\")\n",
    "        model_conf.contrastive = True\n",
    "    else:\n",
    "        print(f\"// -- NOOO contrastive objective -- //\")\n",
    "        model_conf.contrastive = False\n",
    "\n",
    "    if VISUAL is False:\n",
    "        print(f\"// -- No visual, layers=0 -- //\")\n",
    "        model_conf.n_stimulus_layers = 0\n",
    "\n",
    "    if CLIP_VARS is not None:\n",
    "        print(f\"// -- CLIP VARS: {CLIP_VARS} -- //\")\n",
    "        model_conf.contrastive_vars = CLIP_VARS\n",
    "    \n",
    "\n",
    "\n",
    "model = GPT(model_conf)\n",
    "\n",
    "if RESUME:\n",
    "    print(f\"// -- Resuming model -- //\")\n",
    "    model.load_state_dict(torch.load(RESUME, map_location='cpu'), strict=False)\n",
    "\n",
    "n = 1\n",
    "title =  f'ablations_1/finetuning_{PDATA}_resume{RESUME != None}/behavior_before_stim_RESUME{RESUME != None}_paststate{PAST_STATE}_method_behavior_{behavior}_{behavior_vars}_predictbehavior{PREDICT_BEHAVIOR}_rounded{ROUND_VARS}visual{VISUAL}_contrastive{model_conf.contrastive}_{model_conf.contrastive_vars}'\n",
    "\n",
    "# count number of files at the same level as this one\n",
    "if not INFERENCE:\n",
    "    while os.path.exists(f'./models/tensorboard/visnav_medial/{title}'):\n",
    "        n += 1\n",
    "        title = f'cebra/cebra_data_no_dt/RESUME{RESUME != None}_paststate{PAST_STATE}_method_behavior_{behavior}_{behavior_vars}_predictbehavior{PREDICT_BEHAVIOR}_visual{VISUAL}_contrastive{model_conf.contrastive}_{model_conf.contrastive_vars}'\n",
    "if FINETUNE:        \n",
    "    title = os.path.join(title, f'_{args.title}_{PDATA}')\n",
    "if TITLE is not None:\n",
    "    title = title + f'_{TITLE}'\n",
    "# model_path = f\"\"\"./models/tensorboard/visnav_medial/{title}/sparse_f:{mconf.sparse_topk_frame}_id:{mconf.sparse_topk_id}/w:{window}_wp:{window_prev}/{6}_Cont:{mconf.contrastive}_window:{window}_f_window:{frame_window}_df:{dt}_blocksize:{id_block_size}_conv_{conv_layer}_shuffle:{shuffle}_batch:{batch_size}_sparse_({mconf.sparse_topk_frame}_{mconf.sparse_topk_id})_blocksz{block_size}_pos_emb:{mconf.pos_emb}_temp_emb:{mconf.temp_emb}_drop:{mconf.id_drop}_dt:{shuffle}_2.0_{max(stoi_dt.values())}_max{dt}_{layers}_{mconf.n_head}_{mconf.n_embd}.pt\"\"\"\"\n",
    "model_path = f\"\"\"./models/tensorboard/{DATASET}/behavior_pred_exp/classification/{title}/sparse_f:{mconf.sparse_topk_frame}_id:{mconf.sparse_topk_id}/w:{window}_wp:{window_prev}/{6}_Cont:{mconf.contrastive}_window:{window}_f_window:{frame_window}_df:{dt}_blocksize:{id_block_size}_conv_{conv_layer}_shuffle:{shuffle}_batch:{batch_size}_sparse_({mconf.sparse_topk_frame}_{mconf.sparse_topk_id})_blocksz{block_size}_pos_emb:{mconf.pos_emb}_temp_emb:{mconf.temp_emb}_drop:{mconf.id_drop}_dt:{shuffle}_2.0_{max(stoi_dt.values())}_max{dt}_{layers}_{mconf.n_head}_{mconf.n_embd}.pt\"\"\"\n",
    "\n",
    "# # %%\n",
    "# model.cpu()\n",
    "# preds, features, loss = model(x, y)\n",
    "# for key in loss.keys():\n",
    "#     print(key, loss[key])\n",
    "\n",
    "\n",
    "preds, features, loss = model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIST: False\n",
      "Loading model from ././models/tensorboard/HippocampusPosCEBRA/behavior_pred_exp/classification/ablations_1/finetuning_None_resumeFalse/behavior_before_stim_RESUMEFalse_paststateTrue_method_behavior_True_['Position', 'Dir_Right', 'Dir_Left']_predictbehaviorFalse_roundedFalsevisualFalse_contrastiveTrue_['id', 'behavior_mean']/sparse_f:None_id:None/w:0.025_wp:0.1/6_Cont:False_window:0.025_f_window:None_df:0.025_blocksize:100_conv_True_shuffle:True_batch:224_sparse_(None_None)_blocksz400_pos_emb:False_temp_emb:True_drop:0.35_dt:True_2.0_6_max0.025_(8, 8, 0)_8_256.pt\n"
     ]
    }
   ],
   "source": [
    "print(F\"DIST: {DIST}\")\n",
    "tconf = TrainerConfig(max_epochs=max_epochs, batch_size=batch_size, learning_rate=2e-4, \n",
    "                    num_workers=4, lr_decay=True, patience=3, warmup_tokens=8e7, \n",
    "                    decay_weights=True, weight_decay=1.0, shuffle=shuffle,\n",
    "                    final_tokens=len(train_dataset)*(id_block_size) * (max_epochs),\n",
    "                    clip_norm=1.0, grad_norm_clip=1.0,\n",
    "                    dataset='higher_order', mode='predict',\n",
    "                    block_size=train_dataset.block_size,\n",
    "                    id_block_size=train_dataset.id_block_size,\n",
    "                    show_grads=False, plot_raster=False,\n",
    "                    ckpt_path=model_path, no_pbar=False, \n",
    "                    dist=DIST, save_every=0)\n",
    "\n",
    "if TRAIN:\n",
    "    # trainer = Trainer(model, train_dataset, test_dataset, tconf, model_conf)\n",
    "    # if DOWNSTREAM:\n",
    "    #     mconf.__setattr__('freeze_model', FREEZE_MODEL)\n",
    "    #     trainer.config.__setattr__('warmup_tokens', 100)\n",
    "    #     N_CLASSES = 2\n",
    "    #     classifier = ClassifierWrapper(model, mconf, N_CLASSES)\n",
    "    #     train_model = classifier\n",
    "\n",
    "    # else:\n",
    "    #     train_model = model\n",
    "    train_model = model\n",
    "    trainer = Trainer(train_model, train_dataset, test_dataset, tconf, model_conf)\n",
    "    trainer.train()\n",
    "elif FINETUNE:\n",
    "    assert PDATA is not None, \"Must provide path to data to finetune\"\n",
    "    if PDATA < 1:\n",
    "        batch_size = 32 * 7\n",
    "        setattr(tconf, 'batch_size', batch_size)\n",
    "        # max_epochs = 200\n",
    "    # assert RESUME is not None, \"Must provide path to model to finetune\"\n",
    "    loss_bprop = ['behavior']\n",
    "    r_split_ft = PDATA\n",
    "    n_finetune_trials = round(len(train_data['Trial'].unique()) * r_split_ft)\n",
    "    finetune_trials = train_data['Trial'].unique()[:n_finetune_trials]\n",
    "    finetune_data = train_data[train_data['Trial'].isin(finetune_trials)]\n",
    "    finetune_dataset = train_dataset.copy(finetune_data)\n",
    "    \n",
    "    setattr(tconf, 'loss_bprop', loss_bprop)\n",
    "    setattr(tconf, 'finetune', True)\n",
    "    setattr(tconf, 'max_epochs', 1000)\n",
    "    print(f\"// loss to backprop: {loss_bprop} //\")\n",
    "    print(f\"// -- Finetuning model -- //\")\n",
    "    if RESUME is not None:\n",
    "        print(f\"// -- Loading model from {RESUME} -- //\")\n",
    "        model.load_state_dict(torch.load(RESUME, map_location='cpu'), strict=False)\n",
    "    trainer = Trainer(model, finetune_dataset, test_dataset, tconf, model_conf)\n",
    "    trainer.train()\n",
    "\n",
    "else:\n",
    "    if RESUME is not None:\n",
    "        model_path = RESUME\n",
    "    else:\n",
    "        model_path = glob.glob(os.path.join(base_path, '**.pt'), recursive=True)[0]\n",
    "    print(f\"Loading model from {model_path}\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location='cpu'), strict=True)\n",
    "\n",
    "# loader = DataLoader(train_dataset, batch_size=2, shuffle=False, num_workers=4, pin_memory=True)\n",
    "# iterable = iter(loader)\n",
    "# x, y = next(iterable)\n",
    "\n",
    "# load best model after training\n",
    "if TRAIN or FINETUNE:\n",
    "    # load best model\n",
    "    model.load_state_dict(torch.load(tconf.ckpt_path, map_location='cpu'), strict=True)\n",
    "dir_name = os.path.dirname(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get correlation between predicted and true speed\n",
    "# if PREDICT_BEHAVIOR:\n",
    "#     from neuroformer.utils import predict_behavior\n",
    "#     chosen_trials = test_data['Trial'].unique()\n",
    "#     trial_data = test_data[test_data['Trial'].isin(chosen_trials)]\n",
    "#     trial_dataset = train_dataset.copy(trial_data)\n",
    "#     sample_behavior = False\n",
    "#     top_p = 0.75 if sample_behavior else 0\n",
    "#     behavior_preds = predict_behavior(model, trial_dataset, itos_speed, \n",
    "#                                       sample=sample_behavior, top_p=top_p)\n",
    "\n",
    "#     from scipy.stats import pearsonr\n",
    "#     r, p = pearsonr(behavior_preds['behavior'], behavior_preds['true'])\n",
    "#     print(f\"r: {r}, p: {p}\")\n",
    "#     behavior_preds.to_csv(os.path.join(dir_name, f'behavior_pred_sample_{sample_behavior}_{top_p}.csv'), index=False)\n",
    "#     # plot hexplot of predicted vs true speed\n",
    "#     # import seaborn as sns\n",
    "#     # import matplotlib.pyplot as plt\n",
    "#     # sns.set_theme(style=\"white\", color_codes=True)\n",
    "#     # g = sns.jointplot(x=\"behavior\", y=\"true\", data=behavior_preds, kind=\"hex\")\n",
    "#     # g.ax_joint.plot([0, 1], [0, 1], 'k--')\n",
    "#     # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [10:47<00:00,  9.25s/it]\n"
     ]
    }
   ],
   "source": [
    "# Extract latents ##\n",
    "\n",
    "from neuroformer.utils import extract_latents\n",
    "\n",
    "# latent_trials = np.random.choice(train_data['Trial'].unique())\n",
    "latent_trials = train_data['Trial'].unique()\n",
    "latent_data = train_data[train_data['Trial'].isin(latent_trials)]\n",
    "latent_dataset = train_dataset.copy(latent_data)\n",
    "\n",
    "feats, latents = extract_latents(model, latent_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents.keys()\n",
    "\n",
    "for key in latents.keys():\n",
    "    print(f\"key: {len(latents[key])}\")\n",
    "for key in feats.keys():\n",
    "    print(f\"key: {len(feats[key])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroformer.utils import all_device\n",
    "device = torch.cuda.current_device()\n",
    "x = all_device(x, device)\n",
    "y = all_device(y, device)\n",
    "preds, features, loss = model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['clip']['id'].shape\n",
    "features['clip']['behavior_mean'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.stack(feats['behavior'])[:, :, 0, 0]\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.stack(latents['id']).detach().cpu().numpy()\n",
    "labels = np.stack(feats['behavior'])[:, :, 0, 0]\n",
    "print(labels.shape)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_latents(dataset, save_file):\n",
    "    feats, latents = extract_latents(model, latent_dataset, res=dt_vars)\n",
    "    embeddings = torch.stack(latents['id']).detach().cpu().numpy()\n",
    "    labels = np.stack(feats['behavior'])[:, :, 0, 0]\n",
    "    np.save(os.path.join(save_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroformer.visualize import set_plot_white\n",
    "set_plot_white()\n",
    "\n",
    "fig = plt.figure(figsize=(50,50))\n",
    "\n",
    "ax1 = plt.subplot(141, projection='3d')\n",
    "ax1 = plot_hippocampus(ax1, embeddings, labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = df_behavior['Position'].values\n",
    "\n",
    "# plot distribution of positions\n",
    "plt.hist(position, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(train_dataset, batch_size=32*7, shuffle=True, num_workers=4, pin_memory=True)\n",
    "iterable = iter(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions_sample.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions_sample = x['behavior'][:, 0].flatten().cpu().numpy()\n",
    "plt.hist(positions_sample, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_to_spherical(cartesian_coords):\n",
    "    if cartesian_coords.shape[1] == 2:  # Handle 2D input\n",
    "        x, y = cartesian_coords.T\n",
    "        r = np.sqrt(x**2 + y**2)\n",
    "        theta = np.arctan2(y, x)\n",
    "        return np.stack((r, theta), axis=-1)  # Shape will be (N, 2)\n",
    "    elif cartesian_coords.shape[1] == 3:  # Handle 3D input\n",
    "        x, y, z = cartesian_coords.T\n",
    "        r = np.sqrt(x**2 + y**2 + z**2)\n",
    "        theta = np.arctan2(y, x)\n",
    "        phi = np.arccos(z / r)\n",
    "        return np.stack((r, theta, phi), axis=-1)  # Shape will be (N, 3)\n",
    "    else:\n",
    "        raise ValueError(\"Input shape is not supported. It should be (N, 2) or (N, 3).\")\n",
    "    \n",
    "\n",
    "from neuroformer.visualize import set_plot_white\n",
    "set_plot_white()\n",
    "\n",
    "fig = plt.figure(figsize=(50,50))\n",
    "\n",
    "ax1 = plt.subplot(141, projection='3d')\n",
    "ax1 = plot_hippocampus(ax1, cartesian_to_spherical(embeddings), labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from neuroformer.visualize import set_plot_white\n",
    "\n",
    "IS_3D = False\n",
    "\n",
    "def plot_data(data, is_3d=True):\n",
    "    set_plot_white()\n",
    "\n",
    "    # Creating a color map based on the keys of the data\n",
    "    cmap = cm.get_cmap('viridis')  # Can choose different colormap here\n",
    "    norm = plt.Normalize(min(data.keys()), max(data.keys()))  # Normalize speed values\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    if is_3d:\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.set_zlabel('Z')\n",
    "    else:\n",
    "        ax = fig.add_subplot(111)\n",
    "    \n",
    "    # Iterating through the dictionary\n",
    "    for speed in data.keys():\n",
    "        speed_data = torch.stack(data[speed], dim=0)\n",
    "        # Convert list of vectors to a numpy array\n",
    "        vectors = np.array(speed_data)\n",
    "        \n",
    "        if is_3d:\n",
    "            ax.scatter(vectors[:, 0], vectors[:, 1], vectors[:, 2], color=cmap(norm(speed)))\n",
    "        else:\n",
    "            ax.scatter(vectors[:, 0], vectors[:, 1], color=cmap(norm(speed)))\n",
    "\n",
    "    # Setting labels\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "\n",
    "    # show colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    plt.colorbar(sm)\n",
    "    plt.title('Latent Space, Alignment (Color = Speed)', fontsize=20)\n",
    "\n",
    "\n",
    "plot_data(latents['id'], is_3d=IS_3D)\n",
    "save_pth = \"./plots/dim_reduction\"\n",
    "# plt.savefig(os.path.join(save_pth, '2D_speed_neurons.png'))\n",
    "# plt.savefig(os.path.join(save_pth, '2D_speed_neurons.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_to_spherical(cartesian_coords):\n",
    "    if cartesian_coords.shape[1] == 2:  # Handle 2D input\n",
    "        x, y = cartesian_coords.T\n",
    "        r = np.sqrt(x**2 + y**2)\n",
    "        theta = np.arctan2(y, x)\n",
    "        return np.stack((r, theta), axis=-1)  # Shape will be (N, 2)\n",
    "    elif cartesian_coords.shape[1] == 3:  # Handle 3D input\n",
    "        z, y, x = cartesian_coords.T\n",
    "        r = np.sqrt(x**2 + y**2 + z**2)\n",
    "        theta = np.arctan2(y, x)\n",
    "        phi = np.arccos(z / r)\n",
    "        return np.stack((r, theta, phi), axis=-1)  # Shape will be (N, 3)\n",
    "    else:\n",
    "        raise ValueError(\"Input shape is not supported. It should be (N, 2) or (N, 3).\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from neuroformer.visualize import set_plot_white\n",
    "\n",
    "set_plot_white()\n",
    "\n",
    "# Your dictionary. This is a simplified example\n",
    "data = latents['behavior_mean']\n",
    "\n",
    "# Creating a color map based on the keys of the data\n",
    "cmap = cm.get_cmap('viridis')  # Can choose different colormap here\n",
    "norm = plt.Normalize(min(data.keys()), max(data.keys()))  # Normalize speed values\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "if IS_3D:\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.set_zlabel('Z')\n",
    "else:\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "# Iterating through the dictionary\n",
    "for speed in data.keys():\n",
    "    speed_data = torch.stack(data[speed], dim=0)\n",
    "    speed_data = cartesian_to_spherical(speed_data)\n",
    "    # Convert list of vectors to a numpy array\n",
    "    vectors = np.array(speed_data)\n",
    "    if vectors.shape[1] == 2:\n",
    "        ax.scatter(vectors[:, 0], vectors[:, 1], color=cmap(norm(speed)))\n",
    "    elif vectors.shape[1] == 3:\n",
    "        ax.scatter(vectors[:, 0], vectors[:, 1], vectors[:, 2], color=cmap(norm(speed)))\n",
    "\n",
    "# Show colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "plt.colorbar(sm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroformer.utils import predict_raster_recursive_time_auto, process_predictions\n",
    "\n",
    "PARALLEL = PARALLEL\n",
    "df_pred_paths = list(pathlib.Path(base_path).glob('*df_pred_.csv*.csv'))\n",
    "df_pred = pd.read_csv(df_pred_paths[0]) if len(df_pred_paths) > 0 else None \n",
    "# df_pred = \"./models/tensorboard/visnav_lateral/behavior_pred_exp/classification/ablations_1/behavior_before_stim_RESUMEFalse_paststateTrue_method_behavior_True_['phi', 'th']_predictbehaviorFalse_roundedFalsevisualTrue_contrastiveFalse_['id', 'frames', 'behavior_mean', 'behavior_mean']/sparse_f:None_id:None/w:0.05_wp:0.25/df_true__top_p0_top_p_t0_9_temp1_0_temp_t1_0_.csv\"\n",
    "results_dict = dict()\n",
    "\n",
    "top_p = 0.76\n",
    "top_p_t = 0.8\n",
    "temp = 1.03\n",
    "temp_t = 1.1\n",
    "\n",
    "test_trials = test_data['Trial'].unique()\n",
    "# pick 8 trials at random from test\n",
    "trials = np.random.choice(test_trials, 8, replace=False)\n",
    "\n",
    "if df_pred is None:\n",
    "    from joblib import Parallel, delayed\n",
    "    # Define a function to process each trial\n",
    "    def process_trial(model, train_dataset, df, stoi, itos_dt, itos, window, window_prev, top_p, top_p_t, temp, temp_t, trial):\n",
    "        print(f\"-- Trial: {trial} --\")\n",
    "        df_trial = df[df['Trial'] == trial]\n",
    "        trial_dataset = train_dataset.copy(df_trial)\n",
    "        results_trial = predict_raster_recursive_time_auto(model, trial_dataset, window, window_prev, stoi, itos_dt, itos=itos, \n",
    "                                                        sample=True, top_p=top_p, top_p_t=top_p_t, temp=temp, temp_t=temp_t, \n",
    "                                                        frame_end=0, get_dt=True, gpu=False, pred_dt=True, plot_probs=True)\n",
    "        df_trial_pred, df_trial_true = process_predictions(results_trial, stoi, itos, window)\n",
    "        print(f\"pred: {df_trial_pred.shape}, true: {df_trial_true.shape}\" )\n",
    "        return df_trial_pred, df_trial_true\n",
    "\n",
    "    if PARALLEL:\n",
    "        # Process each trial in parallel\n",
    "        results = Parallel(n_jobs=-1)(delayed(process_trial)(model, train_dataset, df, stoi, itos_dt, \n",
    "                                                            itos, window, window_prev, top_p, top_p_t, \n",
    "                                                            temp, temp_t, trial) for trial in trials)\n",
    "    else:\n",
    "        # Process each trial sequentially\n",
    "        results = []\n",
    "        for trial in trials:\n",
    "            results.append(process_trial(model, train_dataset, df, stoi, itos_dt, \n",
    "                                            itos, window, window_prev, top_p, top_p_t, \n",
    "                                            temp, temp_t, trial))\n",
    "    # Combine the results from each trial\n",
    "    for n, (df_trial_pred, df_trial_true) in enumerate(results):   \n",
    "        print(f\"-- No. {n} Trial --\")\n",
    "        if df_pred is None:\n",
    "            df_pred = df_trial_pred\n",
    "            df_true = df_trial_true\n",
    "        else:\n",
    "            df_pred = pd.concat([df_pred, df_trial_pred])\n",
    "            df_true = pd.concat([df_true, df_trial_true])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis import get_rates_trial, calc_corr_psth\n",
    "\n",
    "df_1 = df[df['Trial'].isin(trials)]\n",
    "df_pred_full = df_pred\n",
    "\n",
    "window_pred = 2.5\n",
    "window_pred = window if window_pred is None else window_pred\n",
    "df_pred_full = set_intervals(df_pred_full, window, window_prev, window_pred)\n",
    "df_1 = set_intervals(df_1, window, window_prev, window_pred)\n",
    "\n",
    "from neuroformer.analysis import compute_scores, compute_scores_scikit\n",
    "df_true = df[df['Trial'].isin(trials)]\n",
    "scores = compute_scores(df_1, df_pred)\n",
    "scores_scikit = compute_scores_scikit(df_1, df_pred)\n",
    "print(scores)\n",
    "print(f\"len predL: {len(df_pred)}, len true: {len(df_true)}\")\n",
    "\n",
    "model_name = os.path.basename(model_path)\n",
    "df_pred.to_csv(os.path.join(dir_name, F'df_pred_.csv'))\n",
    "\n",
    "intervals = np.array(sorted(set(df_pred_full['Interval'].unique()) & set(df_pred_full['Interval'].unique())))\n",
    "labels = np.array([round(window_pred + window_pred*n, 2) for n in range(0, int(max(df_pred_full['Interval']) / window_pred))])\n",
    "ids = sorted(set(df['ID'].unique()) & set(df['ID'].unique()))\n",
    "\n",
    "rates_pred = get_rates_trial(df_pred_full, labels)\n",
    "rates_1 = get_rates_trial(df_1, labels)\n",
    "\n",
    "top_corr_pred = calc_corr_psth(rates_pred, rates_1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Evaluate results\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from neuroformer.visualize import *\n",
    "from neuroformer.analysis import get_accuracy\n",
    "\n",
    "\n",
    "len_pred, len_true = len(df_pred_full), len(df_1)\n",
    "print(f\"len_pred: {len_pred}, len_true: {len_true}\")\n",
    "\n",
    "accuracy = get_accuracy(df_pred, df_1)\n",
    "pred_scores = compute_scores(df_1, df_pred_full)\n",
    "\n",
    "print(f\"pred: {pred_scores}\")\n",
    "\n",
    "n_bins = 30\n",
    "set_plot_white()\n",
    "plt.figure(figsize=(10, 10), facecolor='white')\n",
    "plt.title(f'PSTH Correlations (V1 + AL) {title}', fontsize=25)\n",
    "plt.ylabel('Count (n)', fontsize=25)\n",
    "plt.xlabel('Pearson r', fontsize=25)\n",
    "# plt.hist(top_corr_real_2, label='real - real3', alpha=0.6)\n",
    "plt.hist(top_corr_pred, label='real - simulated', alpha=0.6, bins=30)\n",
    "plt.legend(fontsize=20)\n",
    "\n",
    "model_name = os.path.basename(model_path)\n",
    "\n",
    "top_p = 0\n",
    "save_title = f'_top_p{str(top_p)}_top_p_t{str(top_p_t)}_temp{str(temp)}_temp_t{str(temp_t)}'.replace('.', '_')\n",
    "plt.savefig(os.path.join(dir_name, F'psth_corr_{save_title}_.svg'))\n",
    "df_pred.to_csv(os.path.join(dir_name, F'df_pred_{save_title}_.csv'))\n",
    "df_true.to_csv(os.path.join(dir_name, F'df_true_{save_title}_.csv'))\n",
    "\n",
    "plot_distribution(df_1, df_pred, save_path=os.path.join(dir_name, F'psth_dist_.svg'))\n",
    "# save scores to json}}\n",
    "# check if files already exists\n",
    "scores_path = os.path.join(dir_name, F'scores_{save_title}_.json')\n",
    "\n",
    "# with open(os.path.join(scores_path), 'w') as fp:\n",
    "#     json.dump(pred_scores, fp)\n",
    "\n",
    "# save scikit scores to json\n",
    "scores_path_scikit = os.path.join(dir_name, F'scores_scikit_{save_title}.json')\n",
    "with open(scores_path_scikit, 'w') as fp:\n",
    "    json.dump(scores_scikit, fp)\n",
    "\n",
    "total_scores = dict()\n",
    "total_scores['pred'] = pred_scores\n",
    "\n",
    "print(f\"model: {title}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterable = iter(test_dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while x['trial'] < 4:\n",
    "\n",
    "x, y = next(iterable)\n",
    "\n",
    "T = len(x['id'])\n",
    "P = x['pad'] - 1\n",
    "T_prev = len(x['id_prev'])\n",
    "P_prev = x['pad_prev'] - 4\n",
    "\n",
    "iv = float(x['interval'])\n",
    "\n",
    "xid = x['id'][: T - P]\n",
    "xid = [itos[int(i)] for i in xid]\n",
    "\n",
    "xid_prev = x['id_prev'][: T_prev - P_prev]\n",
    "xid_prev = [itos[int(i)] for i in xid_prev]\n",
    "\n",
    "print(f\"iv: {iv}, ix+window: {iv + window} pid: {x['pid']} cid: {x['cid']}\")\n",
    "print(f\"x: {xid}\")\n",
    "print(f\"xid_prev: {xid_prev}\")\n",
    "\n",
    "if 'behavior' in y:\n",
    "    t_var = 'Interval' # 'Interval'\n",
    "    tdiff = 0\n",
    "    int_var = 'cid'\n",
    "    y_behavior = y['behavior']\n",
    "    y_behavior = [itos_speed[int(i)] for i in y_behavior]\n",
    "    print(f\"y_behavior: {y_behavior}\")\n",
    "    true_behavior = print(df_behavior[(df_behavior[t_var] > round(float(x[int_var][0]), 2) - tdiff) & (df_behavior[t_var] <= round(float(x[int_var][1]), 2)) & (df_behavior['Trial'] == int(x['trial']))])\n",
    "    print(f\"true_behavior: {true_behavior}\")\n",
    "\n",
    "tdiff = 0\n",
    "t_var = 'Time' # 'Interval'\n",
    "int_var = 'cid'\n",
    "# df[(df[t_var] >= iv - tdiff) & (df[t_var] <= iv + (window + tdiff)) & (df['Trial'] == int(x['trial']))]\n",
    "# df[(df[t_var] >= float(x[int_var][0]) - tdiff) & (df[t_var] <= float(x[int_var][1] + tdiff)) & (df['Trial'] == int(x['trial']))]\n",
    "df[(df[t_var] > float(x[int_var][0]) - tdiff) & (df[t_var] <= float(x['cid'][1] + tdiff)) & (df['Trial'] == int(x['trial']))]\n",
    "\n",
    "t_var = 'Time' # 'Interval'\n",
    "int_var = 'cid'\n",
    "df[(df[t_var] > round(float(x[int_var][0]), 2) - tdiff) & (df[t_var] <= round(float(x[int_var][1]), 2)) & (df['Trial'] == int(x['trial']))]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
