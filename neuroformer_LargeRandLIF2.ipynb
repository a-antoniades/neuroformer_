{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","\n","import glob\n","import collections\n","\n","import pickle\n","import sys\n","import glob\n","from pathlib import Path, PurePath\n","path = Path.cwd()\n","parent_path = path.parents[1]\n","sys.path.append(str(PurePath(parent_path, 'neuroformer')))\n","sys.path.append('neuroformer')\n","sys.path.append('.')\n","sys.path.append('../')\n","\n","\n","import pandas as pd\n","import numpy as np\n","from einops import rearrange\n","\n","from tqdm import tqdm\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","import pandas as pd\n","from torch.utils.data.dataloader import DataLoader\n","\n","import math\n","from torch.utils.data import Dataset\n","\n","from trainer import Trainer, TrainerConfig\n","from utils import set_seed\n","\n","\n","from scipy import io as scipyio\n","from scipy.special import softmax\n","import skimage\n","import skvideo.io\n","from utils import print_full\n","from scipy.ndimage import gaussian_filter, uniform_filter\n","\n","\n","import matplotlib.pyplot as plt\n","from utils import *\n","from visualize import *\n","set_plot_params()\n","parent_path = os.path.dirname(os.path.dirname(os.getcwd())) + \"/\"\n","\n","\n","from model_neuroformer import GPT, GPTConfig, neuralGPTConfig, Decoder\n","from trainer import Trainer, TrainerConfig\n","\n","\n","import json\n","# for i in {1..10}; do python3 -m gather_atts.py; done\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# stim_path = \"data/LargeRandLIF2/LargeRandNet2_PoissonRate.csv\"\n","# response_path = \"data/LargeRandLIF2/LargeRandNet2_SpikeTime.csv\"\n","\n","# if not os.path.exists(response_path):\n","#     print(\"Downloading data...\")\n","#     import gdown\n","#     url = \"https://drive.google.com/drive/folders/1dkU4GjyKt5ror5DuyfM135ISsnE69Jp5?usp=sharing\"\n","#     gdown.download_folder(id=url, quiet=False, use_cookies=False, output=\"data/\")\n","\n","stim_path = \"data/LargeRandLIF2-2/LargeRandNet2_PoissonRate.csv\"\n","response_path = \"data/LargeRandLIF2-2/LargeRandNet2_SpikeTime.csv\"\n","\n","if not os.path.exists(response_path):\n","    print(\"Downloading data...\")\n","    import gdown\n","    url = \"https://drive.google.com/drive/folders/1yDWde9rJ_9nOYN5Ic-_JoAYaW2a2jYOY?usp=sharing\"\n","    gdown.download_folder(id=url, quiet=False, use_cookies=False, output=\"data/\")\n","\n","\n","# Load Data\n","stimulus = np.transpose(np.loadtxt(stim_path, delimiter=','), (1, 0))\n","df = pd.read_csv(response_path, names=['Time', 'ID'])\n","dt_res = 10000\n","df['Time'] = df['Time'].round(4)\n","df['Trial'] = df['Time'].apply(lambda x: x // dt_res + 1).astype(int)\n","df['Time'] = df['Time'].apply(lambda x: x - ((x // dt_res) * dt_res)).round(2)\n","df['ID'] = df['ID'].astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# set up logging\n","import logging\n","logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO,\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from utils import set_seed\n","n_seed = 25\n","set_seed(n_seed)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(stimulus[0, :,])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# df = pd.read_csv(parent_path + \"code/data/OneCombo3/Combo3_all_stim.csv\")\n","frame_window = 20\n","window = 0.5\n","window_prev = 20 - window\n","dt = 0.1\n","dt_frames = 20\n","start_interval = max(window, window_prev)\n","\n","from SpikeVidUtils import make_intervals\n","\n","df['Interval'] = make_intervals(df, window)\n","df['Interval_2'] = make_intervals(df, window_prev)\n","df = df.reset_index(drop=True)\n","\n","# n_dt = sorted((df['Interval_dt'].unique()).round(2)) \n","max_window = max(window, window_prev)\n","dt_range = math.ceil(max_window / dt) + 1  # add first / last interval for SOS / EOS'\n","n_dt = [round(dt * n, 2) for n in range(dt_range)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from utils import df_to_dict\n","\n","dict_path = \"data/LargeRandLIF2-2/LargeRandNet2_SpikeTime_dict.pkl\"\n","\n","if not os.path.exists(dict_path):\n","    print(\"Creating dictionary...\")\n","    df_dict = df_to_dict(df)\n","    with open(dict_path, 'wb') as f:\n","        pickle.dump(df_dict, f)\n","else:\n","    print(\"Loading dictionary...\")\n","    with open(dict_path, 'rb') as f:\n","        df_dict = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# df.groupby(['Interval', 'Trial']).size().nlargest(int(len(df['Trial'].unique()) * 0.2))\n","# df.groupby(['Interval_2', 'Trial']).size().nlargest(int(len(df['Trial'].unique()) * 0.2))\n","# df.groupby(['Interval', 'Trial']).size().plot.bar()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# df.groupby(['Interval_2', 'Trial']).size().nlargest(int(len(df['Trial'].unique()) * 0.2))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from SpikeVidUtils import SpikeTimeVidData2\n","\n","## resnet3d feats\n","n_embd = 200\n","frame_feats = torch.tensor(stimulus, dtype=torch.float32)\n","frame_block_size = 1000  # math.ceil(frame_feats.shape[-1] * frame_window)\n","n_embd_frames = 10000\n","\n","prev_id_block_size = 800\n","id_block_size = 85   # 95\n","block_size = frame_block_size + id_block_size + prev_id_block_size # frame_block_size * 2  # small window for faster training\n","frame_memory = 20   # how many frames back does model see\n","window = window\n","\n","neurons = sorted(list(set(df['ID'])))\n","id_stoi = { ch:i for i,ch in enumerate(neurons) }\n","id_itos = { i:ch for i,ch in enumerate(neurons) }\n","\n","# translate neural embeddings to separate them from ID embeddings\n","neurons = sorted(list(set(df['ID'].unique())))\n","trial_tokens = [f\"Trial {n}\" for n in df['Trial'].unique()]\n","feat_encodings = neurons + ['SOS'] + ['EOS'] + ['PAD']  # + pixels \n","stoi = { ch:i for i,ch in enumerate(feat_encodings) }\n","itos = { i:ch for i,ch in enumerate(feat_encodings) }\n","stoi_dt = { ch:i for i,ch in enumerate(n_dt) }\n","itos_dt = { i:ch for i,ch in enumerate(n_dt) }\n","max(list(itos_dt.values()))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["r_split = 0.8\n","train_trials = sorted(df['Trial'].unique())[:int(len(df['Trial'].unique()) * r_split)]\n","train_data = df[df['Trial'].isin(train_trials)]\n","test_data = df[~df['Trial'].isin(train_trials)]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from SpikeVidUtils import SpikeTimeVidData2\n","\n","train_dataset = SpikeTimeVidData2(train_data, None, block_size, id_block_size, frame_block_size, prev_id_block_size, \n","                                  window, dt, frame_memory, stoi, itos, neurons, stoi_dt, itos_dt, frame_feats, \n","                                  pred=False, window_prev=window_prev, frame_window=frame_window, start_interval=20,\n","                                  dt_frames=dt_frames, data_dict=df_dict, dataset='LIF2')\n","test_dataset = SpikeTimeVidData2(test_data, None, block_size, id_block_size, frame_block_size, prev_id_block_size, \n","                                  window, dt, frame_memory, stoi, itos, neurons, stoi_dt, itos_dt, frame_feats, \n","                                  pred=False, window_prev=window_prev, frame_window=frame_window, start_interval=20,\n","                                  dt_frames=dt_frames, data_dict=df_dict, dataset='LIF2')\n","\n","print(f'train: {len(train_dataset)}, test: {len(test_dataset)}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from utils import get_class_weights\n","# class_weights = get_class_weights(train_dataset, stoi, stoi_dt)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from model_neuroformer_LRN import GPT, GPTConfig, neuralGPTConfig, Decoder\n","# initialize config class and model (holds hyperparameters)\n","# for is_conv in [True, False]:    \n","conv_layer = False\n","mconf = GPTConfig(train_dataset.population_size, block_size,    # frame_block_size\n","                        id_vocab_size=train_dataset.id_population_size,\n","                        frame_block_size=frame_block_size,\n","                        id_block_size=id_block_size,  # frame_block_size\n","                        prev_id_block_size=prev_id_block_size,\n","                        sparse_mask=False, p_sparse=0.25, sparse_topk_frame=None, sparse_topk_id=None,\n","                        n_dt=len(n_dt),\n","                        data_size=train_dataset.size,\n","                        class_weights=None,\n","                        pretrain=False,\n","                        n_state_layers=6, n_state_history_layers=4, n_stimulus_layers=4, self_att_layers=8,\n","                        n_layer=10, n_head=8, n_embd=n_embd, n_embd_frames=n_embd_frames,\n","                        contrastive=True, clip_emb=1024, clip_temp=0.5,\n","                        temp_emb=True, pos_emb=False,\n","                        id_drop=0.35, im_drop=0.35,\n","                        window=window, window_prev=window_prev, frame_window=frame_window, dt=dt,\n","                        neurons=neurons, stoi_dt=stoi_dt, itos_dt=itos_dt, dataset='LIF2')  # 0.35\n","model = GPT(mconf)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["layers = (mconf.n_state_layers, mconf.n_state_history_layers, mconf.n_stimulus_layers)\n","max_epochs = 2\n","batch_size = 32 * 12\n","shuffle = True\n","\n","## first run\n","# model_path = f\"../models/tensorboard/LRN/w:{window}_wp:{window_prev}/{6}_Cont:{mconf.contrastive}_window:{window}_f_window:{frame_window}_df:{dt}_blocksize:{id_block_size}_sparse{mconf.sparse_mask}_conv_{conv_layer}_shuffle:{shuffle}_batch:{batch_size}_sparse_({mconf.sparse_topk_frame}_{mconf.sparse_topk_id})_blocksz{block_size}_pos_emb:{mconf.pos_emb}_temp_emb:{mconf.temp_emb}_drop:{mconf.id_drop}_dt:{shuffle}_2.0_{max(n_dt)}_max{dt}_{layers}_{mconf.n_head}_{mconf.n_embd}.pt\"\n","# model_path = \"/local/home/antonis/neuroformer/models/tensorboard/LRN/w:10_wp:10/6_Cont:True_window:10_f_window:20_df:0.1_blocksize:30_sparseFalse_conv_True_shuffle:True_batch:128_sparse_(200_200)_blocksz1060_pos_emb:False_temp_emb:True_drop:0.35_dt:True_2.0_10.0_max0.1_(6, 4, 10)_2_200.pt\"\n","## weighted\n","weighted = True if mconf.class_weights is not None else False\n","title = '0.5_window_CORRECTCONTRASTIVE_startinterval'\n","model_path = f\"\"\"./models/tensorboard/LRL2/weighted_{weighted}/{title}/sparse_f:{mconf.sparse_topk_frame}_id:{mconf.sparse_topk_id}/w:{window}_wp:{window_prev}/{6}_Cont:{mconf.contrastive}_window:{window}_f_window:{frame_window}_df:{dt}_blocksize:{id_block_size}_conv_{conv_layer}_shuffle:{shuffle}_batch:{batch_size}_sparse_({mconf.sparse_topk_frame}_{mconf.sparse_topk_id})_blocksz{block_size}_pos_emb:{mconf.pos_emb}_temp_emb:{mconf.temp_emb}_drop:{mconf.id_drop}_dt:{shuffle}_2.0_{max(n_dt)}_max{dt}_{layers}_{mconf.n_head}_{mconf.n_embd}.pt\"\"\"\n","\n","if os.path.exists(model_path):\n","    model.load_state_dict(torch.load(model_path))\n","    print(f\"-- loaded model from {model_path} --\")\n","\n","\n","tconf = TrainerConfig(max_epochs=max_epochs, batch_size=batch_size, learning_rate=1e-4, \n","                    num_workers=4, lr_decay=False, patience=3, warmup_tokens=8e0, \n","                    decay_weights=True, weight_decay=0.1, shuffle=shuffle,\n","                    final_tokens=len(train_dataset)*(id_block_size) * (max_epochs),\n","                    clip_norm=1.0, grad_norm_clip=1.0,\n","                    dataset='higher_order', mode='predict',\n","                    block_size=train_dataset.block_size,\n","                    id_block_size=train_dataset.id_block_size,\n","                    show_grads=False, plot_raster=False,\n","                    ckpt_path=model_path, no_pbar=False)\n","# f\"/home/antonis/projects/slab/git/neuroformer/models/model_sim_weighted_shuffle_decay:{shuffle}_perceiver_2.0_dt:{dt}_eos_{mconf.n_layer}_{mconf.n_head}_{mconf.n_embd}.pt\")\n","\n","\n","trainer = Trainer(model, train_dataset, test_dataset, tconf, mconf)\n","# trainer.train()"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["loader = DataLoader(train_dataset, batch_size=32*5, shuffle=True, num_workers=0, pin_memory=True)\n","iterable = iter(loader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","\n","RUN SIMULATION\n","\n","\"\"\"\n","\n","\n","from utils import *\n","from IPython.utils import io\n","# top_p=0.25, top_p_t=0.9, temp=2.\n","\n","\n","model.load_state_dict(torch.load(model_path, map_location='cpu'))\n","\n","# trials = np.random.choice(train_data['Trial'].unique(), size=12)\n","trials = test_data['Trial'].unique()[:4]\n","results_dict = dict()\n","# for n in range(2, 20):\n","df_pred = None\n","df_true = None\n","n_p = 0.3   # (n + 1) * 0.05\n","temp = 2\n","# stoi['SOS'] = 2000\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for trial in trials:    # test_data['Trial'].unique():\n","    # with io.capture_output() as captured:\n","        print(f\"Trial: {trial}\")\n","        df_trial = df[df['Trial'] == trial]\n","        trial_dataset = SpikeTimeVidData2(df_trial, None, block_size, id_block_size, frame_block_size, prev_id_block_size, \n","                                  window, dt, frame_memory, stoi, itos, neurons, stoi_dt, itos_dt, frame_feats, \n","                                  pred=False, window_prev=window_prev, frame_window=frame_window, start_interval=20,\n","                                  dt_frames=dt_frames, data_dict=df_dict, dataset='LIF2')\n","        trial_loader = DataLoader(trial_dataset, shuffle=False, pin_memory=False)\n","        results_trial = predict_raster_recursive_time_auto(model, trial_loader, window, window_prev, stoi, itos_dt, itos=itos, \n","                                                           sample=True, top_p=0.65, top_p_t=0.65, temp=1.0, temp_t=1., frame_end=0, get_dt=True, gpu=False, pred_dt=True)\n","        # results_trial = predict_raster_hungarian(model, loader, itos_dt, top_p=0.75, temp=1)\n","        # print(f\"MAX ID ---- {sorted(results_trial['ID'].unique()[-10])}\")\n","        df_trial_pred, df_trial_true = process_predictions(results_trial, stoi, itos, window)\n","        print(f\"pred: {df_trial_pred.shape}, true: {df_trial_true.shape}\" )\n","        if df_pred is None:\n","            df_pred = df_trial_pred\n","            df_true = df_trial_true\n","        else:\n","            df_pred = pd.concat([df_pred, df_trial_pred])\n","            df_true = pd.concat([df_true, df_trial_true])\n","\n","# df_preds[n] = df_pred\n","# print(f\"--- n: {n}, n_p: {n_p}, temp: {temp} ---\")\n","scores = compute_scores(df[df['Trial'].isin(trials)], df_pred)\n","print(scores)\n","print(f\"pred: {len(df_pred)}, true: {len(df_true)}\" )\n","# results_dict[n] = (scores)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","df_1 = df[df['Trial'].isin(trials)]\n","df_2 = df[df['Trial'].isin(trials + 1)]\n","df_3 = df[df['Trial'].isin(trials + 2)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[df['Trial'] == 1]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_pred_full = df_pred\n","window_pred = 25\n","\n","df_list = [df_pred_full, df_1, df_2, df_3]\n","\n","for df_ in df_list:\n","    df_['Interval'] = make_intervals(df_, window_pred)\n","    df_ = df_[df_['Interval'] > window_prev]\n","\n","window_pred = window if window_pred is None else window_pred\n","intervals = np.array(sorted(set(df['Interval'].unique()) & set(df['Interval'].unique())))\n","labels = np.array([round(window_pred + window_pred*n, 2) for n in range(0, int(max(df_pred_full['Interval']) / window_pred))])\n","ids = sorted(set(df['ID'].unique()) & set(df['ID'].unique()))\n","\n","\n","# labels = sorted(set(df_pred_full['Interval'].unique()))\n","rates_pred = get_rates_trial(df_pred_full, labels)\n","rates_1 = get_rates_trial(df_1, labels)\n","rates_2 = get_rates_trial(df_2, labels)\n","rates_3 = get_rates_trial(df_3, labels)\n","\n","top_corr_pred = calc_corr_psth(rates_pred, rates_1)\n","top_corr_real = calc_corr_psth(rates_1, rates_2)\n","top_corr_real_2 = calc_corr_psth(rates_1, rates_3)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\"\"\"\n","\n","Evaluate results\n","\n","\"\"\"\n","\n","from visualize import *\n","\n","# df_2['Trial'] -= 2\n","id_pred, id_true_1, id_true_2 = len(df_pred_full['ID'].unique()), len(df_1['ID'].unique()), len(df_2['ID'].unique())\n","print(f\"id_pred: {id_pred}, id_true_1: {id_true_1}, id_true_2: {id_true_2}\")\n","\n","len_pred, len_true = len(df_pred_full), len(df_1)\n","print(f\"len_pred: {len_pred}, len_true: {len_true}\")\n","\n","accuracy = get_accuracy(df_pred, df_2)\n","\n","scores = compute_scores(df_1, df_2)\n","pred_scores = compute_scores(df_1, df_pred_full)\n","print(f\"real: {scores}\")\n","print(f\"pred: {pred_scores}\")\n","\n","set_plot_white()\n","plt.figure(figsize=(10, 10), facecolor='white')\n","plt.title(f'PSTH Correlations (V1 + AL) bin {window_pred}', fontsize=25)\n","plt.ylabel('Count (n)', fontsize=25)\n","plt.xlabel('Pearson r', fontsize=25)\n","plt.hist(top_corr_real, label='real - real2', alpha=0.6)\n","# plt.hist(top_corr_real_2, label='real - real3', alpha=0.6)\n","plt.hist(top_corr_pred, label='real - simulated', alpha=0.6)\n","plt.legend(fontsize=20)\n","plt.show()\n","\n","dir_name = os.path.dirname(model_path)\n","model_name = os.path.basename(model_path)\n","plt.savefig(os.path.join(dir_name, F'psth_corr_{title}.svg'))\n","df_pred.to_csv(os.path.join(dir_name, F'df_pred_{title}.csv'))\n","\n","plot_distribution(df_1, df_pred, save_path=os.path.join(dir_name, F'psth_dist_{title}.svg'))\n","\n","total_scores = dict()\n","total_scores['real'] = scores\n","total_scores['pred'] = pred_scores\n","\n","print(f\"model: {title}\")"]}],"metadata":{"kernelspec":{"display_name":"neuroformer","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"d081bc067bf79be810ce3d13e0a50750de639b750af8f2c90615b4772e48a538"}}},"nbformat":4,"nbformat_minor":2}
