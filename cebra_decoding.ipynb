{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import sys\n",
    "import glob\n",
    "from pathlib import Path, PurePath\n",
    "path = Path.cwd()\n",
    "parent_path = path.parents[1]\n",
    "sys.path.append(str(PurePath(parent_path, 'neuroformer')))\n",
    "sys.path.append('neuroformer')\n",
    "sys.path.append('.')\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import math\n",
    "\n",
    "from neuroformer.model_neuroformer import GPT, GPTConfig\n",
    "from neuroformer.trainer import Trainer, TrainerConfig\n",
    "from neuroformer.utils_2 import (set_seed, update_object, \n",
    "                                 check_common_attrs, running_jupyter, \n",
    "                                 all_device, load_config, update_config, \n",
    "                                 dict_to_object, object_to_dict, recursive_print)\n",
    "from neuroformer.visualize import set_plot_params\n",
    "from neuroformer.SpikeVidUtils import make_intervals, round_n, SpikeTimeVidData2\n",
    "import gdown\n",
    "\n",
    "parent_path = os.path.dirname(os.path.dirname(os.getcwd())) + \"/\"\n",
    "\n",
    "import argparse\n",
    "from neuroformer.SpikeVidUtils import round_n\n",
    "\n",
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"medial\"\n",
    "DT = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spikes: (32614, 1906), speed: (32614,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "\n",
    "-- DATA --\n",
    "neuroformer/data/OneCombo3_V1AL/\n",
    "df = response\n",
    "video_stack = stimulus\n",
    "DOWNLOAD DATA URL = https://drive.google.com/drive/folders/1jNvA4f-epdpRmeG9s2E-2Sfo-pwYbjeY?usp=sharing\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from neuroformer.prepare_data import DataLinks\n",
    "from neuroformer.DataUtils import round_n, get_frame_idx\n",
    "from neuroformer.DataUtils import resample_spikes\n",
    "\n",
    "\n",
    "if DATASET in [\"first\", \"visnav\"]:\n",
    "    data_path = \"./data/VisNav_VR_Expt\"\n",
    "elif DATASET == \"medial\":\n",
    "    data_path = \"./data/VisNav_VR_Expt/MedialVRDataset/\"\n",
    "elif DATASET == \"lateral\":\n",
    "    data_path = \"./data/VisNav_VR_Expt/LateralVRDataset\"\n",
    "\n",
    "\n",
    "train_data = pd.read_csv(os.path.join(data_path, \"train_data.csv\"))\n",
    "\n",
    "spikes_path = f\"{data_path}/NF_1.5/spikerates_dt_0.01.npy\"\n",
    "speed_path = f\"{data_path}/NF_1.5/behavior_speed_dt_0.05.npy\"\n",
    "stim_path = f\"{data_path}/NF_1.5/stimulus.npy\"\n",
    "\n",
    "spikes = resample_spikes(np.load(spikes_path), 0.01, DT).transpose()\n",
    "speed = np.round(np.load(speed_path), 3).transpose()\n",
    "stimulus = np.load(stim_path)\n",
    "\n",
    "frame_feats = None\n",
    "print(f\"spikes: {spikes.shape}, speed: {speed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spikes_train: (293, 1906), spikes_test: (32321, 1906)\n"
     ]
    }
   ],
   "source": [
    "train_indexes = set([get_frame_idx(value, DT) for value in train_data['Interval']])\n",
    "test_indexes = set(range(len(speed))) - train_indexes\n",
    "len_test = len(test_indexes)\n",
    "\n",
    "spikes_train = spikes[list(train_indexes)]\n",
    "spikes_test = spikes[list(test_indexes)][:len_test]\n",
    "\n",
    "speed_train = speed[list(train_indexes)]\n",
    "speed_test = speed[list(test_indexes)][:len_test]\n",
    "\n",
    "print(f\"spikes_train: {spikes_train.shape}, spikes_test: {spikes_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iterations: [ 100 1000 1900 2800 3700 4600 5500 6400 7300 8200 9100], output_dimension: [2, 3, 8, 16, 32]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_iterations = np.arange(100, 10000, 900) #default is 5000.\n",
    "output_dimension = [2, 3, 8, 16, 32] #here, we set as a variable for hypothesis testing below.\n",
    "\n",
    "print(f\"max_iterations: {max_iterations}, output_dimension: {output_dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iterations: 5000, output_dimension: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pos:  0.0722 neg:  2.6367 total:  2.7089 temperature:  1.0000: 100%|██████████| 5000/5000 [00:42<00:00, 118.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training\n",
      "finished embedding\n",
      "finished embedding test\n",
      "corr: 0.3563641032748791\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import cebra\n",
    "from cebra import CEBRA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "max_iterations = np.arange(100, 10000, 1900) #default is 5000.\n",
    "output_dimension = [2, 3, 8, 16, 32] #here, we set as a variable for hypothesis testing below.\n",
    "OFFSET = 1\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "MAX_CORR = 0\n",
    "\n",
    "max_iter = 5000\n",
    "out_dim = 16\n",
    "\n",
    "print(f\"max_iterations: {max_iter}, output_dimension: {out_dim}\")\n",
    "\n",
    "cebra_model = CEBRA(model_architecture=f'offset{OFFSET}-model',\n",
    "                    batch_size=16,\n",
    "                    learning_rate=e-4,\n",
    "                    temperature=1,\n",
    "                    output_dimension=out_dim,\n",
    "                    max_iterations=max_iter,\n",
    "                    distance='cosine',\n",
    "                    device='cuda_if_available',\n",
    "                    verbose=True)\n",
    "\n",
    "# 1. Train a CEBRA-Time model on the whole dataset\n",
    "cebra_model.fit(spikes_train, speed_train)\n",
    "\n",
    "print(\"finished training\")\n",
    "embedding = cebra_model.transform(spikes_train)\n",
    "print(\"finished embedding\")\n",
    "embedding_test = cebra_model.transform(spikes_test)\n",
    "print(\"finished embedding test\")\n",
    "\n",
    "# 3. Train the decoder on the training set\n",
    "decoder = cebra.KNNDecoder()\n",
    "# decoder = cebra.L1LinearRegressor()\n",
    "decoder.fit(embedding, speed_train)\n",
    "\n",
    "# 4. Test the decoder on the test set\n",
    "speed_pred = decoder.predict(embedding_test)\n",
    "\n",
    "# 5. Compute the correlation between the predicted and true speed\n",
    "corr = pearsonr(speed_pred, speed_test)[0]\n",
    "\n",
    "print(f\"corr: {corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
