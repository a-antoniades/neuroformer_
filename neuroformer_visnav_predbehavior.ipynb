{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter notebook\n",
      " // CONTRASTIVE: True //\n",
      " // VISUAL: True //\n",
      " // PAST_STATE: True //\n",
      " // PREDICT_BEHAVIOR: True //\n",
      " // BEHAVIOR: True //\n",
      " // FUSE_STIM_BEHAVIOR: False //\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pathlib\n",
    "import glob\n",
    "import os\n",
    "import collections\n",
    "import json\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "import glob\n",
    "from pathlib import Path, PurePath\n",
    "path = Path.cwd()\n",
    "parent_path = path.parents[1]\n",
    "sys.path.append(str(PurePath(parent_path, 'neuroformer')))\n",
    "sys.path.append('neuroformer')\n",
    "sys.path.append('.')\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from neuroformer.model_neuroformer import GPT, GPTConfig, neuralGPTConfig\n",
    "from neuroformer.trainer import Trainer, TrainerConfig\n",
    "from neuroformer.utils import set_seed, update_object, check_common_attrs\n",
    "from neuroformer.visualize import set_plot_params\n",
    "from neuroformer.SpikeVidUtils import round_n, set_intervals\n",
    "set_plot_params()\n",
    "\n",
    "from scipy import io as scipyio\n",
    "from scipy.special import softmax\n",
    "import skimage\n",
    "import skvideo.io\n",
    "from scipy.ndimage import gaussian_filter, uniform_filter\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "parent_path = os.path.dirname(os.path.dirname(os.getcwd())) + \"/\"\n",
    "import argparse\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument(\"--infer\", action=\"store_true\", help=\"Inference mode\")\n",
    "    parser.add_argument(\"--train\", action=\"store_true\", default=False, help=\"Train mode\")\n",
    "    parser.add_argument(\"--infer\", action=\"store_true\", default=False, help=\"Inference mode\")    \n",
    "    parser.add_argument(\"--finetune\", action=\"store_true\", default=False, help=\"Finetune\")\n",
    "    parser.add_argument(\"--pdata\", type=float, default=None, help=\"Proportion of data to finetune on\")\n",
    "    parser.add_argument(\"--dataset\", type=str, default=\"first\", help=\"Dataset\")\n",
    "    parser.add_argument(\"--dist\", action=\"store_true\", default=False, help=\"Distrinuted training\")\n",
    "    parser.add_argument(\"--resume\", type=str, default=None, help=\"Resume from checkpoint\")\n",
    "    parser.add_argument(\"--rand_perm\", action=\"store_true\", default=False, help=\"Randomly permute the ID column\")\n",
    "    parser.add_argument(\"--mconf\", type=str, default=None, help=\"Path to model config file\")\n",
    "    parser.add_argument(\"--downstream\", action=\"store_true\", default=False, help=\"Downstream task\")\n",
    "    parser.add_argument(\"--freeze_model\", action=\"store_true\", default=False, help=\"Freeze model\")\n",
    "    parser.add_argument(\"--title\", type=str, default=None)\n",
    "    parser.add_argument(\"--seed\", type=int, default=25)\n",
    "    parser.add_argument(\"--behavior\", action=\"store_true\", default=False, help=\"Behavior task\")\n",
    "    parser.add_argument(\"--predict_behavior\", action=\"store_true\", default=False, help=\"Predict behavior\")\n",
    "    # parser.add_argument(\"--behavior_vars\", type=str, default=None, help=\"Behavior variables\")\n",
    "    parser.add_argument(\"--behavior_vars\", nargs='+', default=None, help=\"Behavior variables\")\n",
    "    parser.add_argument(\"--round_vars\", action=\"store_true\", default=False, help=\"Round variables\")\n",
    "    parser.add_argument(\"--past_state\", action=\"store_true\", default=False, help=\"Input past state\")\n",
    "    parser.add_argument(\"--visual\", action=\"store_true\", default=False, help=\"Visualize\")\n",
    "    parser.add_argument(\"--contrastive\", action=\"store_true\", default=False, help=\"Contrastive\")\n",
    "    parser.add_argument(\"--clip_vars\", type=list, default=None, help=\"Clip variables\")\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=0)\n",
    "    parser.add_argument(\"--fuse_stim_behavior\", action=\"store_true\", default=False, help=\"Fuse stimulus and behavior\")\n",
    "    parser.add_argument(\"--mlp_only\", action=\"store_true\", default=False, help=\"MLP only\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     args = parse_args()\n",
    "#     INFERENCE = not args.train\n",
    "# else:\n",
    "#     INFERENCE = True\n",
    "\n",
    "# check if jupyter notebook\n",
    "\n",
    "try:\n",
    "    shell = get_ipython().__class__.__name__\n",
    "    print(\"Running in Jupyter notebook\")\n",
    "    TRAIN = False\n",
    "    INFERENCE = True\n",
    "    DATASET = \"lateral\"\n",
    "    DIST = False\n",
    "    DOWNSTREAM = False\n",
    "    RESUME = \"./models/tensorboard/visnav_medial/behavior_pred_exp/classification/ablations_1/behavior_before_stim_RESUMETrue_paststateTrue_method_behavior_True_['speed']_predictbehaviorTrue_roundedFalsevisualTrue_contrastiveFalse_['id', 'frames', 'behavior_mean']/_finetuning_behavior_0.1/sparse_f:None_id:None/w:0.05_wp:0.25/6_Cont:False_window:0.05_f_window:0.2_df:0.005_blocksize:100_conv_True_shuffle:True_batch:224_sparse_(None_None)_blocksz446_pos_emb:False_temp_emb:True_drop:0.35_dt:True_2.0_52_max0.005_(8, 8, 8)_8_256.pt\"\n",
    "    RAND_PERM = False\n",
    "    MCONF = \"./models/tensorboard/visnav_medial/behavior_pred_exp/classification/ablations_1/behavior_before_stim_RESUMETrue_paststateTrue_method_behavior_True_['speed']_predictbehaviorTrue_roundedFalsevisualTrue_contrastiveFalse_['id', 'frames', 'behavior_mean']/_finetuning_behavior_0.1/sparse_f:None_id:None/w:0.05_wp:0.25/mconf.yaml\"\n",
    "    FREEZE_MODEL = False\n",
    "    TITLE = None\n",
    "    SEED = 25\n",
    "    BEHAVIOR = True\n",
    "    PREDICT_BEHAVIOR = True\n",
    "    BEHAVIOR_VARS = ['speed']\n",
    "    ROUND_VARS = False\n",
    "    PAST_STATE = True\n",
    "    VISUAL = True\n",
    "    CONTRASTIVE = True\n",
    "    CLIP_VARS = ['id', 'frames']\n",
    "    FUSE_STIM_BEHAVIOR = False\n",
    "    FINETUNE = False\n",
    "    PDATA = 0.1\n",
    "    MLP_ONLY = False\n",
    "    PARALLEL = False\n",
    "except:\n",
    "    print(\"Running in terminal\")\n",
    "    args = parse_args()\n",
    "    TRAIN = args.train\n",
    "    INFERENCE = args.infer\n",
    "    DATASET = args.dataset\n",
    "    DIST = args.dist\n",
    "    DOWNSTREAM = args.downstream\n",
    "    RESUME = args.resume\n",
    "    RAND_PERM = args.rand_perm\n",
    "    MCONF = args.mconf\n",
    "    FREEZE_MODEL = args.freeze_model\n",
    "    TITLE = args.title\n",
    "    SEED = args.seed\n",
    "    BEHAVIOR = args.behavior\n",
    "    PREDICT_BEHAVIOR = args.predict_behavior\n",
    "    BEHAVIOR_VARS = args.behavior_vars\n",
    "    ROUND_VARS = args.round_vars\n",
    "    PAST_STATE = args.past_state\n",
    "    VISUAL = args.visual\n",
    "    CONTRASTIVE = args.contrastive\n",
    "    CLIP_VARS = args.clip_vars\n",
    "    FUSE_STIM_BEHAVIOR = args.fuse_stim_behavior\n",
    "    FINETUNE = args.finetune\n",
    "    PDATA = args.pdata\n",
    "    MLP_ONLY = args.mlp_only\n",
    "    PARALLEL = True\n",
    "    \n",
    "set_seed(25)\n",
    "\n",
    "print(f\" // CONTRASTIVE: {CONTRASTIVE} //\")\n",
    "print(f\" // VISUAL: {VISUAL} //\")\n",
    "print(f\" // PAST_STATE: {PAST_STATE} //\")\n",
    "print(f\" // PREDICT_BEHAVIOR: {PREDICT_BEHAVIOR} //\")\n",
    "print(f\" // BEHAVIOR: {BEHAVIOR} //\")\n",
    "print(f\" // FUSE_STIM_BEHAVIOR: {FUSE_STIM_BEHAVIOR} //\")\n",
    "\n",
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroformer.prepare_data import DataLinks\n",
    "\n",
    "ds = \"LateralVRDataset\"\n",
    "# ds = \"MedialVRDataset\"\n",
    "ds = \"VisNav_VR_Expt\"\n",
    "data_dir = f\"data/VisNav_VR_Expt/\"\n",
    "DATA_POINTERS = getattr(DataLinks, ds)\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    print(\"Downloading data...\")\n",
    "    import gdown\n",
    "    url = DATA_POINTERS['url']\n",
    "    gdown.download_folder(id=url, quiet=False, use_cookies=False, output=DATA_POINTERS['DIRECTORY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config files\n",
    "import yaml\n",
    "\n",
    "# base_path = \"configs/visnav/predict_behavior\"\n",
    "base_path = \"./models/tensorboard/visnav_medial\" if MCONF is None else os.path.dirname(MCONF)\n",
    "\n",
    "with open(os.path.join(base_path, 'mconf.yaml'), 'r') as stream:\n",
    "    mconf = yaml.full_load(stream)\n",
    "\n",
    "with open(os.path.join(base_path, 'tconf.yaml'), 'r') as stream:\n",
    "    tconf = yaml.full_load(stream)\n",
    "\n",
    "with open(os.path.join(base_path, 'dconf.yaml'), 'r') as stream:\n",
    "    dconf = yaml.full_load(stream)\n",
    "\n",
    "# open yaml as omegacong\n",
    "mconf = OmegaConf.create(mconf)\n",
    "tconf = OmegaConf.create(tconf)\n",
    "dconf = OmegaConf.create(dconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ./data/VisNav_VR_Expt/LateralVRDataset\n"
     ]
    }
   ],
   "source": [
    "import mat73\n",
    "\n",
    "# data_path = DATA_POINTERS['RESPONSE_PATH']\n",
    "if DATASET in [\"first\", \"visnav\"]:\n",
    "    data_path = \"./data/VisNav_VR_Expt\"\n",
    "elif DATASET == \"medial\":\n",
    "    data_path = \"./data/VisNav_VR_Expt/MedialVRDataset/\"\n",
    "elif DATASET == \"lateral\":\n",
    "    data_path = \"./data/VisNav_VR_Expt/LateralVRDataset\"\n",
    "\n",
    "print(f\"Loading data from {data_path}\")\n",
    "# stimulus = np.load(os.path.join(data_path, \"stimulus.npy\"), allow_pickle=True)\n",
    "# response = np.load(os.path.join(data_path, \"response.npy\"), allow_pickle=True)\n",
    "# trial_data = np.load(os.path.join(data_path, \"trial_data.npy\"), allow_pickle=True)\n",
    "data = mat73.loadmat(os.path.join(data_path, \"experiment_data.mat\"))['neuroformer']\n",
    "\n",
    "# data_response_path = \"/data5/antonis/neuroformer/data/VisNav_VR_Expt/yiyi/experiment_data_selected.mat\"\n",
    "# data_response = scipy.io.loadmat(data_response_path)\n",
    "# neurons_sel1 = \"./data/VisNav_VR_Expt/yiyi/sel1.csv\"\n",
    "# neurons_sel1 = pd.read_csv(neurons_sel1)\n",
    "# neurons_sel1 = np.array(neurons_sel1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common attributes: {}\n"
     ]
    }
   ],
   "source": [
    "if INFERENCE or DOWNSTREAM or FINETUNE or TRAIN: \n",
    "    window = mconf.window\n",
    "    window_prev = mconf.window_prev\n",
    "    frame_window = mconf.frame_window\n",
    "    window_behavior = mconf.window_behavior if hasattr(mconf, 'window_behavior') else None\n",
    "    dt = mconf.dt\n",
    "    dt_frames = mconf.dt_frames if hasattr(mconf, 'dt_frames') else 0.05\n",
    "    dt_vars = mconf.dt_vars if hasattr(mconf, 'dt_vars') else 0.05\n",
    "    dt_speed = mconf.dt_speed if hasattr(mconf, 'dt_speed') else 0.2\n",
    "    intervals = None\n",
    "else:\n",
    "    window = 0.05\n",
    "    window_prev = 0.25\n",
    "    frame_window = window + window_prev\n",
    "    window_behavior = window\n",
    "    dt = 0.01\n",
    "    dt_frames = 0.05\n",
    "    dt_vars = 0.05\n",
    "    dt_speed = 0.2\n",
    "    intervals = None\n",
    "\n",
    "# set attrs that are not equal\n",
    "common_attrs = check_common_attrs(mconf, tconf, dconf)\n",
    "print(f\"Common attributes: {common_attrs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " // using behavior vars: ['speed'] //\n"
     ]
    }
   ],
   "source": [
    "## choose modalities ##\n",
    "\n",
    "# behavior\n",
    "behavior = BEHAVIOR\n",
    "# behavior_vars = ['t', 'eyerad', 'phi', 'speed', 'th']\n",
    "behavior_vars = ['speed'] if BEHAVIOR_VARS is None else BEHAVIOR_VARS\n",
    "n_behavior = len(behavior_vars)\n",
    "predict_behavior = PREDICT_BEHAVIOR\n",
    "# stimulus\n",
    "visual_stim = VISUAL\n",
    "\n",
    "print(f\" // using behavior vars: {BEHAVIOR_VARS} //\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['eyerad', 'phi', 'rewards', 'speed', 'spiketimes', 't', 'th', 'trialsummary', 'vid_sm'])\n"
     ]
    }
   ],
   "source": [
    "from neuroformer.SpikeVidUtils import trial_df, get_df_visnav, make_intervals, set_trials\n",
    "\n",
    "stimulus = data['vid_sm']\n",
    "response = data['spiketimes']['spks']\n",
    "trial_data = data['trialsummary']\n",
    "# response = data_response['spiketime_sel2']['spks']\n",
    "\n",
    "print(data.keys())\n",
    "\n",
    "df = get_df_visnav(response, trial_data, dt_vars)\n",
    "# df = df[df['ID'].isin(neurons_sel1)].reset_index(drop=True)\n",
    "\n",
    "if behavior or predict_behavior is True:\n",
    "    window_behavior = window\n",
    "    df_behavior = pd.DataFrame({k: data[k] for k in behavior_vars + ['t']})\n",
    "    # rename t to time\n",
    "    df_behavior = df_behavior.rename(columns={'t': 'Time'}) if df_behavior is not None else None\n",
    "    df_behavior = set_trials(df_behavior, trial_data) \n",
    "    df_behavior['Interval'] = make_intervals(df_behavior, window)\n",
    "    df_behavior['Interval_2'] = make_intervals(df_behavior, window_prev)\n",
    "\n",
    "    # prepare speed variables\n",
    "    if 'speed' in df_behavior.columns:\n",
    "        df_behavior['speed'] = df_behavior['speed'].apply(lambda x: round_n(x, dt_speed))\n",
    "        dt_range_speed = df_behavior['speed'].min(), df_behavior['speed'].max()\n",
    "        dt_range_speed = np.arange(dt_range_speed[0], dt_range_speed[1] + dt_speed, dt_speed)\n",
    "        n_behavior = len(dt_range_speed)\n",
    "        stoi_speed = { round_n(ch, dt_speed):i for i,ch in enumerate(dt_range_speed) }\n",
    "        itos_speed = { i:round_n(ch, dt_speed) for i,ch in enumerate(dt_range_speed) }\n",
    "    else:\n",
    "        n_behavior = None\n",
    "        stoi_speed = None\n",
    "        itos_speed = None\n",
    "        assert predict_behavior is False\n",
    "    \n",
    "    if ROUND_VARS:\n",
    "        print(f\" // ROUNDING behavior vars to {dt} //\")\n",
    "        dt_phi = 0.2\n",
    "        dt_th = 0.2\n",
    "        df_behavior['phi'] = df_behavior['phi'].apply(lambda x: round_n(x, dt_phi))\n",
    "        df_behavior['th'] = df_behavior['th'].apply(lambda x: round_n(x, dt_th))\n",
    "\n",
    "        # prepare phi variables\n",
    "        dt_range_phi = df_behavior['phi']\n",
    "        dt_range_phi = np.arange(dt_range_phi[0], dt_range_phi[1] + dt_phi, dt_phi)\n",
    "        stoi_phi = { round_n(ch, dt_phi):i for i,ch in enumerate(dt_range_phi) }\n",
    "        itos_phi = { i:round_n(ch, dt_phi) for i,ch in enumerate(dt_range_phi) }\n",
    "\n",
    "        # prepare th variables\n",
    "        dt_range_th =  df_behavior['th']\n",
    "        dt_range_th = np.arange(dt_range_th[0], dt_range_th[1] + dt_th, dt_th)\n",
    "        stoi_th = { round_n(ch, dt_th):i for i,ch in enumerate(dt_range_th) }\n",
    "        itos_th = { i:round_n(ch, dt_th) for i,ch in enumerate(dt_range_th) }\n",
    "\n",
    "    # assert (window_behavior) % dt_vars < 1e-5, \"window + window_prev must be divisible by dt_vars\"\n",
    "    # samples_per_behavior = int((window + window_prev) // dt_vars)\n",
    "    # behavior_block_size = int((window + window_prev) // dt_vars) * (len(df_behavior.columns) - 1)\n",
    "    samples_per_behavior = int(window_behavior // dt_vars)\n",
    "    behavior_block_size = samples_per_behavior * (len(df_behavior.columns) - 1)\n",
    "else:\n",
    "    behavior = False \n",
    "    df_behavior = None\n",
    "    behavior_vars = None\n",
    "    behavior_block_size = 0\n",
    "    samples_per_behavior = 0\n",
    "    stoi_speed = None\n",
    "    itos_speed = None\n",
    "    dt_range_speed = None\n",
    "    n_behavior = None\n",
    "    stoi_phi = None\n",
    "    itos_phi = None\n",
    "    dt_range_phi = None\n",
    "    stoi_th = None\n",
    "    itos_th = None\n",
    "    dt_range_th = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spikerates.shape: (2023, 30116)\n"
     ]
    }
   ],
   "source": [
    "dt = 0.05\n",
    "\n",
    "def bin_spikes(data, dt):\n",
    "    # Compute the maximum time across all spike times\n",
    "    max_time = max(neuron[0].max() for neuron in data if neuron[0].size != 0)\n",
    "\n",
    "    # Compute the number of intervals\n",
    "    N_intervals = int(np.ceil(max_time / dt))\n",
    "\n",
    "    # Create a 2D matrix of zeros\n",
    "    N_Neurons = len(data)\n",
    "    spike_matrix = np.zeros((N_Neurons, N_intervals))\n",
    "\n",
    "    # Iterate over the neurons and their spike times\n",
    "    for i, neuron in enumerate(data):\n",
    "        # Remove NaN values\n",
    "        spike_times = neuron[0][~np.isnan(neuron[0])]\n",
    "        # Iterate over the spike times\n",
    "        for spike_time in spike_times:\n",
    "            # Compute the interval index for the spike time\n",
    "            interval_index = int(spike_time // dt)\n",
    "\n",
    "            # Increment the spike count for the neuron and interval\n",
    "            spike_matrix[i, interval_index] += 1\n",
    "\n",
    "    return spike_matrix.astype(np.int8)\n",
    "\n",
    "spikerates = bin_spikes(response, dt)\n",
    "print(f\"spikerates.shape: {spikerates.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f78a78fe3a0>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(df_behavior)), df_behavior['speed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_vars = ['speed']\n",
    "df_behavior = pd.DataFrame({k: data[k] for k in behavior_vars + ['t']})\n",
    "df_behavior.rename(columns={'t': 'Time'}, inplace=True)\n",
    "\n",
    "def assign_behavior_to_bins(behavior, dt, behavior_col='speed'):\n",
    "    # Create an array of zeros\n",
    "    behavior_values = np.zeros(math.ceil(behavior['Time'].max() / dt))\n",
    "\n",
    "    # Iterate over the intervals\n",
    "    for i in range(len(behavior_values)):\n",
    "        # Compute the time point for the interval\n",
    "        time_point = i * dt\n",
    "\n",
    "        # Find the last behavior value that occurred before or at the time point\n",
    "        behavior_value = behavior[behavior['Time'] <= time_point][behavior_col].iloc[-1]\n",
    "\n",
    "        # Assign the behavior value to the interval\n",
    "        behavior_values[i] = behavior_value\n",
    "\n",
    "    return behavior_values.astype(np.float32)\n",
    "\n",
    "N_intervals = spikerates.shape[-1]\n",
    "dt_behavior = 0.05\n",
    "behavior_values = assign_behavior_to_bins(df_behavior, dt_behavior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(data_path, \"NF_1.5\")\n",
    "\n",
    "np.save(f\"{save_path}/spikerates_dt_{dt}.npy\", spikerates)\n",
    "# np.save(f\"{save_path}/behavior_speed_dt_{dt_behavior}.npy\", behavior_values)\n",
    "# np.save(f\"{save_path}/stimulus.npy\", stimulus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroformer.SpikeVidUtils import make_intervals\n",
    "\n",
    "df['Interval'] = make_intervals(df, window)\n",
    "df['real_interval'] = make_intervals(df, 0.05)\n",
    "df['Interval_2'] = make_intervals(df, window_prev)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "max_window = max(window, window_prev)\n",
    "dt_range = math.ceil(max_window / dt) + 1  # add first / last interval for SOS / EOS'\n",
    "n_dt = [round_n(dt * n, 0.005) for n in range(dt_range)] + ['EOS'] + ['PAD']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_group = 'Interval'\n",
    "groups = df.groupby([var_group, 'Trial']).size()\n",
    "n_unique = len(groups)\n",
    "top_p = 0.05\n",
    "top_k_groups = groups.nlargest(int(top_p * n_unique))\n",
    "mean_groups = groups.mean()\n",
    "\n",
    "print(f\"Unique groups: {n_unique}\")\n",
    "print(f\"Top {top_p} groups: {top_k_groups}\")\n",
    "print(f\"Mean groups: {mean_groups}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_group = 'Interval_2'\n",
    "groups = df.groupby([var_group, 'Trial']).size()\n",
    "n_unique = len(groups)\n",
    "top_p = 0.2\n",
    "top_k_groups = groups.nlargest(int(top_p * n_unique))\n",
    "mean_groups = groups.mean()\n",
    "\n",
    "print(f\"Unique groups: {n_unique}\")\n",
    "print(f\"Top {top_p} groups: {top_k_groups}\")\n",
    "print(f\"Mean groups: {mean_groups}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique combiation of trial intervals\n",
    "\n",
    "unique_combinations = df.groupby(['Interval', 'Trial']).size().reset_index().rename(columns={0:'count'})\n",
    "\n",
    "print(f\"Number of unique combinations: {len(unique_combinations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroformer.SpikeVidUtils import SpikeTimeVidData2\n",
    "\n",
    "## resnet3d feats\n",
    "n_frames = round(frame_window * 1/dt_frames)\n",
    "kernel_size = (n_frames, 5, 5)\n",
    "n_embd = 256\n",
    "n_embd_frames = 64\n",
    "frame_feats = stimulus\n",
    "frame_block_size = ((n_frames // kernel_size[0] * 30 * 100) // (n_embd_frames))\n",
    "frame_feats = torch.tensor(stimulus, dtype=torch.float32)\n",
    "conv_layer = True\n",
    "\n",
    "prev_id_block_size = 300\n",
    "id_block_size = 100   #\n",
    "block_size = frame_block_size + id_block_size + prev_id_block_size\n",
    "frame_memory = frame_window // dt_frames\n",
    "window = window\n",
    "\n",
    "neurons = sorted(list(set(df['ID'])))\n",
    "id_stoi = { ch:i for i,ch in enumerate(neurons) }\n",
    "id_itos = { i:ch for i,ch in enumerate(neurons) }\n",
    "\n",
    "neurons = sorted(list(set(df['ID'].unique())))\n",
    "trial_tokens = [f\"Trial {n}\" for n in df['Trial'].unique()]\n",
    "feat_encodings = neurons + ['SOS'] + ['EOS'] + ['PAD']  # + pixels \n",
    "stoi = { ch:i for i,ch in enumerate(feat_encodings) }\n",
    "itos = { i:ch for i,ch in enumerate(feat_encodings) }\n",
    "stoi_dt = { ch:i for i,ch in enumerate(n_dt) }\n",
    "itos_dt = { i:ch for i,ch in enumerate(n_dt) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "r_split = 0.8\n",
    "all_trials = sorted(df['Trial'].unique())\n",
    "train_trials = random.sample(all_trials, int(len(all_trials) * r_split))\n",
    "\n",
    "train_data = df[df['Trial'].isin(train_trials)]\n",
    "test_data = df[~df['Trial'].isin(train_trials)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pth = \"./data/cebra/visnav_medial\"\n",
    "\n",
    "df_cebra = df.copy()\n",
    "df_cebra_behavior = df_behavior.copy()\n",
    "df_cebra_train = train_data.copy()\n",
    "df_cebra_test = test_data.copy()\n",
    "\n",
    "# keep only common ids\n",
    "train_ids = df_cebra_train['ID'].unique()\n",
    "test_ids = df_cebra_test['ID'].unique()\n",
    "common_ids = set(train_ids).intersection(set(test_ids))\n",
    "df_cebra = df_cebra[df_cebra['ID'].isin(common_ids)]\n",
    "df_cebra_train = df_cebra_train[df_cebra_train['ID'].isin(common_ids)]\n",
    "df_cebra_test = df_cebra_test[df_cebra_test['ID'].isin(common_ids)]\n",
    "\n",
    "df_cebra_train = pd.merge(df_cebra_train, df_cebra_behavior, how='inner', on=['Interval', 'Trial'])\n",
    "df_cebra_test = pd.merge(df_cebra_test, df_cebra_behavior, how='inner', on=['Interval', 'Trial'])\n",
    "\n",
    "# make sure that for all pairs of intervals/trials in df, there are corresponding intervals/trials in df_behavior\n",
    "\n",
    "def cum_time(df):\n",
    "    for trial in df['Trial'].unique():\n",
    "        max_interval = df[df['Trial'] == trial]['Interval'].max()\n",
    "        # add max_time to all proceeding trials\n",
    "        df.loc[df['Trial'] > trial, 'Interval'] += max_interval\n",
    "    return df\n",
    "\n",
    "df_cebra = cum_time(df_cebra)\n",
    "df_cebra = df_cebra.drop_duplicates(subset='Interval', keep='last')\n",
    "df_cebra = pd.pivot_table(df_cebra, index='Interval', columns='ID', aggfunc='size', fill_value=0)\n",
    "\n",
    "df_cebra_train = cum_time(df_cebra_train)\n",
    "df_cebra_train = df_cebra_train.drop_duplicates(subset='Interval', keep='last')\n",
    "df_cebra_train_spikes = pd.pivot_table(df_cebra_train, index='Interval', columns='ID', aggfunc='size', fill_value=0)\n",
    "df_cebra_train_behavior = df_cebra_train['speed']\n",
    "\n",
    "df_cebra_test = cum_time(df_cebra_test)\n",
    "df_cebra_test = df_cebra_test.drop_duplicates(subset='Interval', keep='last')\n",
    "df_cebra_test_spikes = pd.pivot_table(df_cebra_test, index='Interval', columns='ID', aggfunc='size', fill_value=0)\n",
    "df_cebra_test_behavior = df_cebra_test['speed']\n",
    "\n",
    "# After creating the pivot tables:\n",
    "common_ids_after_merge = set(df_cebra_train_spikes.columns).intersection(set(df_cebra_test_spikes.columns))\n",
    "# Explicitly set the columns of both dataframes to be the same\n",
    "df_cebra_train_spikes = df_cebra_train_spikes.reindex(columns=common_ids_after_merge)\n",
    "df_cebra_test_spikes = df_cebra_test_spikes.reindex(columns=common_ids_after_merge)\n",
    "\n",
    "if not os.path.exists(save_pth):\n",
    "    os.makedirs(save_pth)\n",
    "df_cebra.to_csv(f\"{save_pth}/df_cebra.csv\", index=False)\n",
    "df_cebra_train.to_csv(f\"{save_pth}/df_cebra_train.csv\", index=False)\n",
    "df_cebra_train_spikes.to_csv(f\"{save_pth}/df_cebra_train_spikes.csv\", index=False)\n",
    "df_cebra_test_spikes.to_csv(f\"{save_pth}/df_cebra_test_spikes.csv\", index=False)\n",
    "df_cebra_test.to_csv(f\"{save_pth}/df_cebra_test.csv\", index=False)\n",
    "df_cebra_behavior.to_csv(f\"{save_pth}/df_cebra_behavior.csv\", index=False)\n",
    "df_cebra_train_behavior.to_csv(f\"{save_pth}/df_cebra_train_behavior.csv\", index=False)\n",
    "df_cebra_test_behavior.to_csv(f\"{save_pth}/df_cebra_test_behavior.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroformer.SpikeVidUtils import SpikeTimeVidData2\n",
    "\n",
    "\n",
    "train_dataset = SpikeTimeVidData2(train_data, None, block_size, id_block_size, frame_block_size, prev_id_block_size, \n",
    "                                  window, dt, frame_memory, stoi, itos, neurons, stoi_dt, itos_dt, frame_feats,\n",
    "                                  pred=False, window_prev=window_prev, frame_window=frame_window,\n",
    "                                  dt_frames=dt_frames, intervals=None, dataset='visnav',\n",
    "                                  behavior=df_behavior, behavior_vars=behavior_vars, dt_vars=dt_vars,\n",
    "                                  behavior_block_size=behavior_block_size, samples_per_behavior=samples_per_behavior,\n",
    "                                  window_behavior=window_behavior, predict_behavior=predict_behavior,\n",
    "                                  stoi_speed=stoi_speed, itos_speed=itos_speed, dt_speed=dt_speed) # stoi_phi=stoi_phi, itos_phi=itos_phi, \n",
    "                                  # dt_phi=dt_phi, stoi_th=stoi_th, itos_th=itos_th, dt_th=dt_th)\n",
    "\n",
    "test_dataset = train_dataset.copy(test_data)\n",
    "\n",
    "print(f'train: {len(train_dataset)}, test: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(train_dataset, batch_size=2, shuffle=False, num_workers=4, pin_memory=True)\n",
    "iterable = iter(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iterable)\n",
    "# print(x['behavior'].shape, x['behavior_dt'].shape)\n",
    "for k in x.keys():\n",
    "    print(k, x[k].shape)\n",
    "for k in y.keys():\n",
    "    print(f\"y: {k}, {y[k].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = (mconf.n_state_layers, mconf.n_state_history_layers, mconf.n_stimulus_layers)   \n",
    "max_epochs = 750\n",
    "batch_size = round((32 * 7))\n",
    "shuffle = True\n",
    "\n",
    "model_conf = GPTConfig(train_dataset.population_size, block_size,    # frame_block_size\n",
    "                        id_vocab_size=train_dataset.id_population_size,\n",
    "                        frame_block_size=frame_block_size,\n",
    "                        id_block_size=id_block_size,  # frame_block_size\n",
    "                        prev_id_block_size=prev_id_block_size,\n",
    "                        behavior_block_size=behavior_block_size,\n",
    "                        sparse_mask=False, p_sparse=None, \n",
    "                        sparse_topk_frame=None, sparse_topk_id=None, sparse_topk_prev_id=None,\n",
    "                        n_dt=len(n_dt),\n",
    "                        class_weights=None,\n",
    "                        pretrain=False,\n",
    "                        n_state_layers=mconf.n_state_layers, n_state_history_layers=mconf.n_state_history_layers,\n",
    "                        n_stimulus_layers=mconf.n_stimulus_layers, self_att_layers=mconf.self_att_layers,\n",
    "                        n_behavior_layers=mconf.n_behavior_layers, predict_behavior=predict_behavior, n_behavior=n_behavior,\n",
    "                        n_head=mconf.n_head, n_embd=mconf.n_embd, \n",
    "                        contrastive=mconf.contrastive, clip_emb=mconf.clip_emb, clip_temp=mconf.clip_temp, clip_loss=False,\n",
    "                        conv_layer=conv_layer, kernel_size=kernel_size,\n",
    "                        temp_emb=mconf.temp_emb, pos_emb=False, wave_emb=True,\n",
    "                        id_drop=0.35, im_drop=0.35, b_drop=0.45,\n",
    "                        window=window, window_prev=window_prev, frame_window=frame_window, dt=dt,\n",
    "                        neurons=neurons, stoi_dt=stoi_dt, itos_dt=itos_dt, n_embd_frames=n_embd_frames,\n",
    "                        ignore_index_id=stoi['PAD'], ignore_index_dt=stoi_dt['PAD'],\n",
    "                        fuse_stim_behavior=FUSE_STIM_BEHAVIOR, mlp_only=MLP_ONLY)  # 0.35\n",
    "\n",
    "# update_object(model_conf, mconf)\n",
    "model_conf.contrastive_vars = ['id', 'frames']\n",
    "\n",
    "if INFERENCE or MCONF is not None:\n",
    "    update_object(model_conf, mconf)\n",
    "\n",
    "if TRAIN or FINETUNE or INFERENCE:\n",
    "    if MCONF is not None:\n",
    "        print(f\"// -- updating model conf -- //\")\n",
    "        update_object(model_conf, mconf)\n",
    "\n",
    "    if BEHAVIOR and not PREDICT_BEHAVIOR:\n",
    "        model_conf.contrastive_vars += ['behavior_mean']\n",
    "        \n",
    "    if PREDICT_BEHAVIOR is True:\n",
    "        print(f\"// Predict behavior: n_behavior_layers = 0 //\")\n",
    "        model_conf.n_behavior_layers = 0\n",
    "        model_conf.predict_behavior = True\n",
    "\n",
    "    if PAST_STATE is False:\n",
    "        print(f\"// -- No past state, layers=0 -- //\")\n",
    "        model_conf.n_state_history_layers = 0\n",
    "\n",
    "    if CONTRASTIVE is True:\n",
    "        print(f\"// -- contrastive objective -- //\")\n",
    "        model_conf.contrastive = True\n",
    "    else:\n",
    "        print(f\"// -- NOOO contrastive objective -- //\")\n",
    "        model_conf.contrastive = False\n",
    "\n",
    "    if VISUAL is False:\n",
    "        print(f\"// -- No visual, layers=0 -- //\")\n",
    "        model_conf.n_stimulus_layers = 0\n",
    "\n",
    "    if CLIP_VARS is not None:\n",
    "        print(f\"// -- CLIP VARS: {CLIP_VARS} -- //\")\n",
    "        model_conf.contrastive_vars = None\n",
    "    \n",
    "\n",
    "\n",
    "model = GPT(model_conf)\n",
    "\n",
    "if RESUME:\n",
    "    print(f\"// -- Resuming model -- //\")\n",
    "    model.load_state_dict(torch.load(RESUME, map_location='cpu'), strict=False)\n",
    "\n",
    "n = 1\n",
    "title =  f'ablations_1/finetuning_{PDATA}_resume{RESUME != None}/behavior_before_stim_RESUME{RESUME != None}_paststate{PAST_STATE}_method_behavior_{behavior}_{behavior_vars}_predictbehavior{PREDICT_BEHAVIOR}_rounded{ROUND_VARS}visual{VISUAL}_contrastive{model_conf.contrastive}_{model_conf.contrastive_vars}'\n",
    "\n",
    "# count number of files at the same level as this one\n",
    "if not INFERENCE:\n",
    "    while os.path.exists(f'./models/tensorboard/visnav_medial/{title}'):\n",
    "        n += 1\n",
    "        title = f'cebra/RESUME{RESUME != None}_paststate{PAST_STATE}_method_behavior_{behavior}_{behavior_vars}_predictbehavior{PREDICT_BEHAVIOR}_visual{VISUAL}_contrastive{model_conf.contrastive}_{model_conf.contrastive_vars}'\n",
    "if FINETUNE:        \n",
    "    title = os.path.join(title, f'_{args.title}_{PDATA}')\n",
    "if TITLE is not None:\n",
    "    title = title + f'_{TITLE}'\n",
    "# model_path = f\"\"\"./models/tensorboard/visnav_medial/{title}/sparse_f:{mconf.sparse_topk_frame}_id:{mconf.sparse_topk_id}/w:{window}_wp:{window_prev}/{6}_Cont:{mconf.contrastive}_window:{window}_f_window:{frame_window}_df:{dt}_blocksize:{id_block_size}_conv_{conv_layer}_shuffle:{shuffle}_batch:{batch_size}_sparse_({mconf.sparse_topk_frame}_{mconf.sparse_topk_id})_blocksz{block_size}_pos_emb:{mconf.pos_emb}_temp_emb:{mconf.temp_emb}_drop:{mconf.id_drop}_dt:{shuffle}_2.0_{max(stoi_dt.values())}_max{dt}_{layers}_{mconf.n_head}_{mconf.n_embd}.pt\"\"\"\"\n",
    "model_path = f\"\"\"./models/tensorboard/visnav_{DATASET}/behavior_pred_exp/classification/{title}/sparse_f:{mconf.sparse_topk_frame}_id:{mconf.sparse_topk_id}/w:{window}_wp:{window_prev}/{6}_Cont:{mconf.contrastive}_window:{window}_f_window:{frame_window}_df:{dt}_blocksize:{id_block_size}_conv_{conv_layer}_shuffle:{shuffle}_batch:{batch_size}_sparse_({mconf.sparse_topk_frame}_{mconf.sparse_topk_id})_blocksz{block_size}_pos_emb:{mconf.pos_emb}_temp_emb:{mconf.temp_emb}_drop:{mconf.id_drop}_dt:{shuffle}_2.0_{max(stoi_dt.values())}_max{dt}_{layers}_{mconf.n_head}_{mconf.n_embd}.pt\"\"\"\n",
    "\n",
    "# %%\n",
    "model.cpu()\n",
    "preds, features, loss = model(x, y)\n",
    "for key in loss.keys():\n",
    "    print(key, loss[key])\n",
    "\n",
    "\n",
    "preds, features, loss = model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(F\"DIST: {DIST}\")\n",
    "tconf = TrainerConfig(max_epochs=max_epochs, batch_size=batch_size, learning_rate=2e-4, \n",
    "                    num_workers=4, lr_decay=True, patience=3, warmup_tokens=8e7, \n",
    "                    decay_weights=True, weight_decay=1.0, shuffle=shuffle,\n",
    "                    final_tokens=len(train_dataset)*(id_block_size) * (max_epochs),\n",
    "                    clip_norm=1.0, grad_norm_clip=1.0,\n",
    "                    dataset='higher_order', mode='predict',\n",
    "                    block_size=train_dataset.block_size,\n",
    "                    id_block_size=train_dataset.id_block_size,\n",
    "                    show_grads=False, plot_raster=False,\n",
    "                    ckpt_path=model_path, no_pbar=False, \n",
    "                    dist=DIST, save_every=0)\n",
    "\n",
    "if TRAIN:\n",
    "    # trainer = Trainer(model, train_dataset, test_dataset, tconf, model_conf)\n",
    "    # if DOWNSTREAM:\n",
    "    #     mconf.__setattr__('freeze_model', FREEZE_MODEL)\n",
    "    #     trainer.config.__setattr__('warmup_tokens', 100)\n",
    "    #     N_CLASSES = 2\n",
    "    #     classifier = ClassifierWrapper(model, mconf, N_CLASSES)\n",
    "    #     train_model = classifier\n",
    "    # else:\n",
    "    #     train_model = model\n",
    "    train_model = model\n",
    "    trainer = Trainer(train_model, train_dataset, test_dataset, tconf, model_conf)\n",
    "    trainer.train()\n",
    "elif FINETUNE:\n",
    "    assert PDATA is not None, \"Must provide path to data to finetune\"\n",
    "    if PDATA < 1:\n",
    "        batch_size = 32 * 7\n",
    "        setattr(tconf, 'batch_size', batch_size)\n",
    "        # max_epochs = 200\n",
    "    # assert RESUME is not None, \"Must provide path to model to finetune\"\n",
    "    loss_bprop = ['behavior']\n",
    "    r_split_ft = PDATA\n",
    "    n_finetune_trials = round(len(train_data['Trial'].unique()) * r_split_ft)\n",
    "    finetune_trials = train_data['Trial'].unique()[:n_finetune_trials]\n",
    "    finetune_data = train_data[train_data['Trial'].isin(finetune_trials)]\n",
    "    finetune_dataset = train_dataset.copy(finetune_data)\n",
    "    \n",
    "    setattr(tconf, 'loss_bprop', loss_bprop)\n",
    "    setattr(tconf, 'finetune', True)\n",
    "    setattr(tconf, 'max_epochs', 1000)\n",
    "    print(f\"// loss to backprop: {loss_bprop} //\")\n",
    "    print(f\"// -- Finetuning model -- //\")\n",
    "    if RESUME is not None:\n",
    "        print(f\"// -- Loading model from {RESUME} -- //\")\n",
    "        model.load_state_dict(torch.load(RESUME, map_location='cpu'), strict=False)\n",
    "    trainer = Trainer(model, finetune_dataset, test_dataset, tconf, model_conf)\n",
    "    trainer.train()\n",
    "\n",
    "else:\n",
    "    if RESUME is not None:\n",
    "        model_path = RESUME\n",
    "    else:\n",
    "        model_path = glob.glob(os.path.join(base_path, '**.pt'), recursive=True)[0]\n",
    "    model_path = RESUME\n",
    "    print(f\"Loading model from {model_path}\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location='cpu'), strict=True)\n",
    "\n",
    "# loader = DataLoader(train_dataset, batch_size=2, shuffle=False, num_workers=4, pin_memory=True)\n",
    "# iterable = iter(loader)\n",
    "# x, y = next(iterable)\n",
    "\n",
    "# load best model after training\n",
    "if TRAIN or FINETUNE:\n",
    "    # load best model\n",
    "    model.load_state_dict(torch.load(tconf.ckpt_path, map_location='cpu'), strict=True)\n",
    "dir_name = os.path.dirname(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICT_BEHAVIOR = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get correlation between predicted and true speed\n",
    "if PREDICT_BEHAVIOR:\n",
    "    from neuroformer.utils import predict_behavior\n",
    "    chosen_trials = test_data['Trial'].unique()\n",
    "    trial_data = test_data[test_data['Trial'].isin(chosen_trials)]\n",
    "    trial_dataset = train_dataset.copy(trial_data)\n",
    "    sample_behavior = False\n",
    "    top_p = 0.75 if sample_behavior else 0\n",
    "    behavior_preds = predict_behavior(model, trial_dataset, itos_speed, \n",
    "                                      sample=sample_behavior, top_p=top_p)\n",
    "\n",
    "    from scipy.stats import pearsonr\n",
    "    r, p = pearsonr(behavior_preds['behavior'], behavior_preds['true'])\n",
    "    print(f\"r: {r}, p: {p}\")\n",
    "    # behavior_preds.to_csv(os.path.join(dir_name, f'behavior_pred_sample_{sample_behavior}_{top_p}.csv'), index=False)\n",
    "    # plot hexplot of predicted vs true speed\n",
    "    # import seaborn as sns\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # sns.set_theme(style=\"white\", color_codes=True)\n",
    "    # g = sns.jointplot(x=\"behavior\", y=\"true\", data=behavior_preds, kind=\"hex\")\n",
    "    # g.ax_joint.plot([0, 1], [0, 1], 'k--')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract latents ##\n",
    "\n",
    "from neuroformer.utils import extract_latents\n",
    "\n",
    "# latent_trials = np.random.choice(train_data['Trial'].unique())\n",
    "latent_trials = train_data['Trial'].unique()\n",
    "latent_data = train_data[train_data['Trial'].isin(latent_trials)]\n",
    "latent_dataset = train_dataset.copy(latent_data)\n",
    "\n",
    "latents = extract_latents(model, latent_dataset, res=dt_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents['id'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from neuroformer.visualize import set_plot_white\n",
    "\n",
    "IS_3D = False\n",
    "\n",
    "def plot_data(data, is_3d=True):\n",
    "    set_plot_white()\n",
    "\n",
    "    # Creating a color map based on the keys of the data\n",
    "    cmap = cm.get_cmap('viridis')  # Can choose different colormap here\n",
    "    norm = plt.Normalize(min(data.keys()), max(data.keys()))  # Normalize speed values\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    if is_3d:\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.set_zlabel('Z')\n",
    "    else:\n",
    "        ax = fig.add_subplot(111)\n",
    "    \n",
    "    # Iterating through the dictionary\n",
    "    for speed in data.keys():\n",
    "        speed_data = torch.stack(data[speed], dim=0)\n",
    "        # Convert list of vectors to a numpy array\n",
    "        vectors = np.array(speed_data)\n",
    "        \n",
    "        if is_3d:\n",
    "            ax.scatter(vectors[:, 0], vectors[:, 1], vectors[:, 2], color=cmap(norm(speed)))\n",
    "        else:\n",
    "            ax.scatter(vectors[:, 0], vectors[:, 1], color=cmap(norm(speed)))\n",
    "\n",
    "    # Setting labels\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "\n",
    "    # show colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    plt.colorbar(sm)\n",
    "    plt.title('Latent Space, Alignment (Color = Speed)', fontsize=20)\n",
    "\n",
    "\n",
    "plot_data(latents['behavior_mean'], is_3d=IS_3D)\n",
    "save_pth = \"./plots/dim_reduction\"\n",
    "# plt.savefig(os.path.join(save_pth, '2D_speed_neurons.png'))\n",
    "# plt.savefig(os.path.join(save_pth, '2D_speed_neurons.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_to_spherical(cartesian_coords):\n",
    "    if cartesian_coords.shape[1] == 2:  # Handle 2D input\n",
    "        x, y = cartesian_coords.T\n",
    "        r = np.sqrt(x**2 + y**2)\n",
    "        theta = np.arctan2(y, x)\n",
    "        return np.stack((r, theta), axis=-1)  # Shape will be (N, 2)\n",
    "    elif cartesian_coords.shape[1] == 3:  # Handle 3D input\n",
    "        z, y, x = cartesian_coords.T\n",
    "        r = np.sqrt(x**2 + y**2 + z**2)\n",
    "        theta = np.arctan2(y, x)\n",
    "        phi = np.arccos(z / r)\n",
    "        return np.stack((r, theta, phi), axis=-1)  # Shape will be (N, 3)\n",
    "    else:\n",
    "        raise ValueError(\"Input shape is not supported. It should be (N, 2) or (N, 3).\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from neuroformer.visualize import set_plot_white\n",
    "\n",
    "set_plot_white()\n",
    "\n",
    "# Your dictionary. This is a simplified example\n",
    "data = latents['behavior_mean']\n",
    "\n",
    "# Creating a color map based on the keys of the data\n",
    "cmap = cm.get_cmap('viridis')  # Can choose different colormap here\n",
    "norm = plt.Normalize(min(data.keys()), max(data.keys()))  # Normalize speed values\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "if IS_3D:\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.set_zlabel('Z')\n",
    "else:\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "# Iterating through the dictionary\n",
    "for speed in data.keys():\n",
    "    speed_data = torch.stack(data[speed], dim=0)\n",
    "    speed_data = cartesian_to_spherical(speed_data)\n",
    "    # Convert list of vectors to a numpy array\n",
    "    vectors = np.array(speed_data)\n",
    "    if vectors.shape[1] == 2:\n",
    "        ax.scatter(vectors[:, 0], vectors[:, 1], color=cmap(norm(speed)))\n",
    "    elif vectors.shape[1] == 3:\n",
    "        ax.scatter(vectors[:, 0], vectors[:, 1], vectors[:, 2], color=cmap(norm(speed)))\n",
    "\n",
    "# Show colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "plt.colorbar(sm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroformer.utils import predict_raster_recursive_time_auto, process_predictions\n",
    "\n",
    "PARALLEL = PARALLEL\n",
    "df_pred_paths = list(pathlib.Path(base_path).glob('*df_pred_.csv*.csv'))\n",
    "df_pred = pd.read_csv(df_pred_paths[0]) if len(df_pred_paths) > 0 else None \n",
    "# df_pred = \"./models/tensorboard/visnav_lateral/behavior_pred_exp/classification/ablations_1/behavior_before_stim_RESUMEFalse_paststateTrue_method_behavior_True_['phi', 'th']_predictbehaviorFalse_roundedFalsevisualTrue_contrastiveFalse_['id', 'frames', 'behavior_mean', 'behavior_mean']/sparse_f:None_id:None/w:0.05_wp:0.25/df_true__top_p0_top_p_t0_9_temp1_0_temp_t1_0_.csv\"\n",
    "results_dict = dict()\n",
    "\n",
    "top_p = 0.76\n",
    "top_p_t = 0.8\n",
    "temp = 1.03\n",
    "temp_t = 1.1\n",
    "\n",
    "test_trials = test_data['Trial'].unique()\n",
    "# pick 8 trials at random from test\n",
    "trials = np.random.choice(test_trials, 8, replace=False)\n",
    "\n",
    "if df_pred is None:\n",
    "    from joblib import Parallel, delayed\n",
    "    # Define a function to process each trial\n",
    "    def process_trial(model, train_dataset, df, stoi, itos_dt, itos, window, window_prev, top_p, top_p_t, temp, temp_t, trial):\n",
    "        print(f\"-- Trial: {trial} --\")\n",
    "        df_trial = df[df['Trial'] == trial]\n",
    "        trial_dataset = train_dataset.copy(df_trial)\n",
    "        results_trial = predict_raster_recursive_time_auto(model, trial_dataset, window, window_prev, stoi, itos_dt, itos=itos, \n",
    "                                                        sample=True, top_p=top_p, top_p_t=top_p_t, temp=temp, temp_t=temp_t, \n",
    "                                                        frame_end=0, get_dt=True, gpu=False, pred_dt=True, plot_probs=True)\n",
    "        df_trial_pred, df_trial_true = process_predictions(results_trial, stoi, itos, window)\n",
    "        print(f\"pred: {df_trial_pred.shape}, true: {df_trial_true.shape}\" )\n",
    "        return df_trial_pred, df_trial_true\n",
    "\n",
    "    if PARALLEL:\n",
    "        # Process each trial in parallel\n",
    "        results = Parallel(n_jobs=-1)(delayed(process_trial)(model, train_dataset, df, stoi, itos_dt, \n",
    "                                                            itos, window, window_prev, top_p, top_p_t, \n",
    "                                                            temp, temp_t, trial) for trial in trials)\n",
    "    else:\n",
    "        # Process each trial sequentially\n",
    "        results = []\n",
    "        for trial in trials:\n",
    "            results.append(process_trial(model, train_dataset, df, stoi, itos_dt, \n",
    "                                            itos, window, window_prev, top_p, top_p_t, \n",
    "                                            temp, temp_t, trial))\n",
    "    # Combine the results from each trial\n",
    "    for n, (df_trial_pred, df_trial_true) in enumerate(results):   \n",
    "        print(f\"-- No. {n} Trial --\")\n",
    "        if df_pred is None:\n",
    "            df_pred = df_trial_pred\n",
    "            df_true = df_trial_true\n",
    "        else:\n",
    "            df_pred = pd.concat([df_pred, df_trial_pred])\n",
    "            df_true = pd.concat([df_true, df_trial_true])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis import get_rates_trial, calc_corr_psth\n",
    "\n",
    "df_1 = df[df['Trial'].isin(trials)]\n",
    "df_pred_full = df_pred\n",
    "\n",
    "window_pred = 2.5\n",
    "window_pred = window if window_pred is None else window_pred\n",
    "df_pred_full = set_intervals(df_pred_full, window, window_prev, window_pred)\n",
    "df_1 = set_intervals(df_1, window, window_prev, window_pred)\n",
    "\n",
    "from neuroformer.analysis import compute_scores, compute_scores_scikit\n",
    "df_true = df[df['Trial'].isin(trials)]\n",
    "scores = compute_scores(df_1, df_pred)\n",
    "scores_scikit = compute_scores_scikit(df_1, df_pred)\n",
    "print(scores)\n",
    "print(f\"len predL: {len(df_pred)}, len true: {len(df_true)}\")\n",
    "\n",
    "model_name = os.path.basename(model_path)\n",
    "df_pred.to_csv(os.path.join(dir_name, F'df_pred_.csv'))\n",
    "\n",
    "intervals = np.array(sorted(set(df_pred_full['Interval'].unique()) & set(df_pred_full['Interval'].unique())))\n",
    "labels = np.array([round(window_pred + window_pred*n, 2) for n in range(0, int(max(df_pred_full['Interval']) / window_pred))])\n",
    "ids = sorted(set(df['ID'].unique()) & set(df['ID'].unique()))\n",
    "\n",
    "rates_pred = get_rates_trial(df_pred_full, labels)\n",
    "rates_1 = get_rates_trial(df_1, labels)\n",
    "\n",
    "top_corr_pred = calc_corr_psth(rates_pred, rates_1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Evaluate results\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from neuroformer.visualize import *\n",
    "from neuroformer.analysis import get_accuracy\n",
    "\n",
    "\n",
    "len_pred, len_true = len(df_pred_full), len(df_1)\n",
    "print(f\"len_pred: {len_pred}, len_true: {len_true}\")\n",
    "\n",
    "accuracy = get_accuracy(df_pred, df_1)\n",
    "pred_scores = compute_scores(df_1, df_pred_full)\n",
    "\n",
    "print(f\"pred: {pred_scores}\")\n",
    "\n",
    "n_bins = 30\n",
    "set_plot_white()\n",
    "plt.figure(figsize=(10, 10), facecolor='white')\n",
    "plt.title(f'PSTH Correlations (V1 + AL) {title}', fontsize=25)\n",
    "plt.ylabel('Count (n)', fontsize=25)\n",
    "plt.xlabel('Pearson r', fontsize=25)\n",
    "# plt.hist(top_corr_real_2, label='real - real3', alpha=0.6)\n",
    "plt.hist(top_corr_pred, label='real - simulated', alpha=0.6, bins=30)\n",
    "plt.legend(fontsize=20)\n",
    "\n",
    "model_name = os.path.basename(model_path)\n",
    "\n",
    "top_p = 0\n",
    "save_title = f'_top_p{str(top_p)}_top_p_t{str(top_p_t)}_temp{str(temp)}_temp_t{str(temp_t)}'.replace('.', '_')\n",
    "plt.savefig(os.path.join(dir_name, F'psth_corr_{save_title}_.svg'))\n",
    "df_pred.to_csv(os.path.join(dir_name, F'df_pred_{save_title}_.csv'))\n",
    "df_true.to_csv(os.path.join(dir_name, F'df_true_{save_title}_.csv'))\n",
    "\n",
    "plot_distribution(df_1, df_pred, save_path=os.path.join(dir_name, F'psth_dist_.svg'))\n",
    "# save scores to json}}\n",
    "# check if files already exists\n",
    "scores_path = os.path.join(dir_name, F'scores_{save_title}_.json')\n",
    "\n",
    "# with open(os.path.join(scores_path), 'w') as fp:\n",
    "#     json.dump(pred_scores, fp)\n",
    "\n",
    "# save scikit scores to json\n",
    "scores_path_scikit = os.path.join(dir_name, F'scores_scikit_{save_title}.json')\n",
    "with open(scores_path_scikit, 'w') as fp:\n",
    "    json.dump(scores_scikit, fp)\n",
    "\n",
    "total_scores = dict()\n",
    "total_scores['pred'] = pred_scores\n",
    "\n",
    "print(f\"model: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterable = iter(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while x['trial'] < 4:\n",
    "\n",
    "x, y = next(iterable)\n",
    "\n",
    "T = len(x['id'])\n",
    "P = x['pad'] - 1\n",
    "T_prev = len(x['id_prev'])\n",
    "P_prev = x['pad_prev'] - 4\n",
    "\n",
    "iv = float(x['interval'])\n",
    "\n",
    "xid = x['id'][: T - P]\n",
    "xid = [itos[int(i)] for i in xid]\n",
    "\n",
    "xid_prev = x['id_prev'][: T_prev - P_prev]\n",
    "xid_prev = [itos[int(i)] for i in xid_prev]\n",
    "\n",
    "print(f\"iv: {iv}, ix+window: {iv + window} pid: {x['pid']} cid: {x['cid']}\")\n",
    "print(f\"x: {xid}\")\n",
    "print(f\"xid_prev: {xid_prev}\")\n",
    "\n",
    "if 'behavior' in y:\n",
    "    t_var = 'Interval' # 'Interval'\n",
    "    tdiff = 0\n",
    "    int_var = 'cid'\n",
    "    y_behavior = y['behavior']\n",
    "    y_behavior = [itos_speed[int(i)] for i in y_behavior]\n",
    "    print(f\"y_behavior: {y_behavior}\")\n",
    "    true_behavior = print(df_behavior[(df_behavior[t_var] > round(float(x[int_var][0]), 2) - tdiff) & (df_behavior[t_var] <= round(float(x[int_var][1]), 2)) & (df_behavior['Trial'] == int(x['trial']))])\n",
    "    print(f\"true_behavior: {true_behavior}\")\n",
    "\n",
    "tdiff = 0\n",
    "t_var = 'Time' # 'Interval'\n",
    "int_var = 'cid'\n",
    "# df[(df[t_var] >= iv - tdiff) & (df[t_var] <= iv + (window + tdiff)) & (df['Trial'] == int(x['trial']))]\n",
    "# df[(df[t_var] >= float(x[int_var][0]) - tdiff) & (df[t_var] <= float(x[int_var][1] + tdiff)) & (df['Trial'] == int(x['trial']))]\n",
    "df[(df[t_var] > float(x[int_var][0]) - tdiff) & (df[t_var] <= float(x['cid'][1] + tdiff)) & (df['Trial'] == int(x['trial']))]\n",
    "\n",
    "t_var = 'Time' # 'Interval'\n",
    "int_var = 'cid'\n",
    "df[(df[t_var] > round(float(x[int_var][0]), 2) - tdiff) & (df[t_var] <= round(float(x[int_var][1]), 2)) & (df['Trial'] == int(x['trial']))]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
